{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCAR(observed_values, missing_ratio, masks):\n",
    "    for col in range(observed_values.shape[1]):  # col #\n",
    "\n",
    "        obs_indices = np.where(observed_values[:, col])[0]\n",
    "        miss_indices = np.random.choice(\n",
    "        obs_indices, (int)(len(obs_indices) * missing_ratio), replace=False\n",
    "        )\n",
    "        masks[miss_indices, col] = False\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data_loaders import *\n",
    "\n",
    "from missing_process.missing_method import * \n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_loader(\"california\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_loaders import *\n",
    "import missing_process.missing_method as missing_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_middle_single_column(lower,upper,partial_missing,dataset):\n",
    "    if lower == 0:\n",
    "        lower_quantile = np.min(dataset, axis=0)\n",
    "    else:\n",
    "        lower_quantile = np.quantile(dataset,lower, axis=0)\n",
    "    if upper == 1:\n",
    "        upper_quantile = np.max(dataset, axis=0)\n",
    "    else:\n",
    "        upper_quantile = np.quantile(dataset,upper, axis=0)\n",
    "\n",
    "\n",
    "    ix_larger_than = dataset >= lower_quantile\n",
    "    ix_smaller_than = dataset <= upper_quantile\n",
    "\n",
    "    \n",
    "    combined_ix = np.equal(ix_larger_than, ix_smaller_than)\n",
    "    combined_ix = random_missing_single_column(combined_ix,partial_missing)\n",
    "    return combined_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_missing_single_column(array, fraction_to_change):\n",
    "\n",
    "    result = array.copy()\n",
    "    n_to_change = int(np.sum(result) * fraction_to_change)\n",
    "    ix_to_change = np.random.choice(np.flatnonzero(result), size=n_to_change, replace=False)\n",
    "\n",
    "    result[ix_to_change] = False\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_single(X,multiple_block,missing_dim = 1):\n",
    "    \n",
    "    Xnan = X.copy()\n",
    "\n",
    "    #---- Missing Dimention\n",
    "    \n",
    "    ix_list = []\n",
    "    for key in multiple_block.keys():\n",
    "        info = multiple_block[key]\n",
    "        combined_ix = generate_middle_single_column(info[\"lower\"],info[\"upper\"],info[\"partial_missing\"], X)\n",
    "        ix_list.append(combined_ix)\n",
    "    combined_ix = np.logical_or.reduce(ix_list)\n",
    "    \n",
    "    Xnan[:,][combined_ix] = np.nan\n",
    "\n",
    "    masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM_missing(observed_values,missing_para):\n",
    "\n",
    "    mask = np.ones(observed_values.shape)\n",
    "    mask_single = missing_single(observed_values[:,missing_para[\"column\"]],missing_para[\"missing\"])\n",
    "    mask[:,missing_para[\"column\"]]= mask_single\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5],[6,6,6],[7,7,7],[8,8,8],[9,9,9],[10,10,10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(dataname,path: str, aug_rate=1,missing_type = \"MCAR\",\n",
    "                  missing_para = \"\"):\n",
    " \n",
    "    data = dataset_loader(dataname)\n",
    "    # print(data)\n",
    "    # data.replace(\"?\", np.nan, inplace=True)\n",
    "    # Don't apply data argument (use n*dataset)\n",
    "    # data_aug = pd.concat([data] * aug_rate)\n",
    "\n",
    "\n",
    "    observed_values = data[\"data\"].astype(\"float32\")\n",
    "\n",
    "    #observed_values = np.array([[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5],[6,6,6],[7,7,7],[8,8,8],[9,9,9],[10,10,10]]).astype(\"float32\")\n",
    "\n",
    "    #np.random.shuffle(observed_values)\n",
    "\n",
    "    # Print the shuffled array\n",
    "\n",
    "    observed_masks = ~np.isnan(observed_values)\n",
    "    masks = observed_masks.copy()\n",
    "    \n",
    "    \"Need input origin dataset and parameters\"\n",
    "    if missing_type == \"MCAR\":\n",
    "        masks = MCAR(observed_values,missing_para,masks)\n",
    "\n",
    "    elif missing_type == \"quantile\":\n",
    "        Xnan, Xz = missing_method.missing_by_range(observed_values, missing_para)\n",
    "        masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
    "\n",
    "    elif missing_type == \"logistic\":\n",
    "        masks = missing_method.MNAR_mask_logistic(observed_values, missing_para)\n",
    "\n",
    "    elif missing_type == \"self_mask\":\n",
    "        masks = missing_method.MNAR_self_mask_logistic(observed_values, missing_para)\n",
    "\n",
    "    elif missing_type == \"BN\":\n",
    "        masks == BM_missing(observed_values, missing_para)\n",
    "\n",
    "\n",
    "    # gt_mask: 0 for missing elements and manully maksed elements\n",
    "    gt_masks = masks.reshape(observed_masks.shape)\n",
    "\n",
    "    observed_values = np.nan_to_num(observed_values)\n",
    "    observed_masks = observed_masks.astype(int)\n",
    "    gt_masks = gt_masks.astype(int)\n",
    "\n",
    "    return observed_values, observed_masks, gt_masks, data[\"data\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class tabular_dataset(Dataset):\n",
    "    # eval_length should be equal to attributes number.\n",
    "    def __init__(\n",
    "        self, dataname, use_index_list=None, \n",
    "        aug_rate=1, seed=0,\n",
    "        missing_type = \"MCAR\", missing_para = \"\",missing_name = \"MCAR\"\n",
    "        ):\n",
    "        #self.eval_length = eval_length\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        dataset_path = f\"datasets/{dataname}/data.csv\"\n",
    "        processed_data_path = (\n",
    "            f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}.pk\"\n",
    "        )\n",
    "        processed_data_path_norm = (\n",
    "            f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}_max-min_norm.pk\"\n",
    "        )\n",
    "\n",
    "        # print(processed_data_path)\n",
    "        # print(processed_data_path_norm)\n",
    "        # If no dataset created\n",
    "        if not os.path.isfile(processed_data_path):\n",
    "            self.observed_values, self.observed_masks, self.gt_masks, self.eval_length = process_func(\n",
    "                dataname, dataset_path, aug_rate=aug_rate,\n",
    "                missing_type = missing_type, missing_para = missing_para\n",
    "            )\n",
    "            with open(processed_data_path, \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    [self.observed_values, self.observed_masks, self.gt_masks, self.eval_length], f\n",
    "                )\n",
    "            print(\"--------Dataset created--------\")\n",
    "\n",
    "        elif os.path.isfile(processed_data_path_norm):\n",
    "            with open(processed_data_path_norm, \"rb\") as f:\n",
    "                self.observed_values, self.observed_masks, self.gt_masks, self.eval_length = pickle.load(\n",
    "                    f\n",
    "                )\n",
    "            print(\"--------Normalized dataset loaded--------\")\n",
    "        \n",
    "        if use_index_list is None:\n",
    "            self.use_index_list = np.arange(len(self.observed_values))\n",
    "        else:\n",
    "            self.use_index_list = use_index_list\n",
    "\n",
    "    def __getitem__(self, org_index):\n",
    "        index = self.use_index_list[org_index]\n",
    "        s = {\n",
    "            \"observed_data\": self.observed_values[index],\n",
    "            \"observed_mask\": self.observed_masks[index],\n",
    "            \"gt_mask\": self.gt_masks[index],\n",
    "            \"timepoints\": np.arange(self.eval_length),\n",
    "        }\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_index_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataname, seed=1, nfold=5, batch_size=16,\n",
    "                   missing_type = \"Quantile\", missing_para = 0.5, missing_name = \"Q1_complete\"):\n",
    "\n",
    "    dataset = tabular_dataset(dataname = dataname,seed=seed,\n",
    "                              missing_type = missing_type, missing_para = missing_para,\n",
    "                                missing_name = missing_name)\n",
    "    # print(\"Missing Type:\",missing_type)\n",
    "    # print(\"Missing Para:\",missing_para)\n",
    "    print(\"Missing Name:\",missing_name)\n",
    "    \n",
    "    \n",
    "    indlist = np.arange(len(dataset))\n",
    "\n",
    "    np.random.seed(seed + 1)\n",
    "    np.random.shuffle(indlist)\n",
    "\n",
    "    tmp_ratio = 1 / nfold\n",
    "    start = (int)((nfold - 1) * len(dataset) * tmp_ratio)\n",
    "    \n",
    "    end = (int)(nfold * len(dataset) * tmp_ratio)\n",
    "\n",
    "    test_index = indlist[start:end]\n",
    "    remain_index = np.delete(indlist, np.arange(start, end))\n",
    "\n",
    "    np.random.shuffle(remain_index)\n",
    "\n",
    "    # Modify here to change train,valid ratio\n",
    "    num_train = (int)(len(remain_index) * 1)\n",
    "    train_index = remain_index[:num_train]\n",
    "    valid_index = remain_index[num_train:]\n",
    "\n",
    "\n",
    "\n",
    "    # Here we perform max-min normalization.\n",
    "    print(\"Here we perform max-min normalization.\")\n",
    "    processed_data_path_norm = (\n",
    "        f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}_max-min_norm.pk\"\n",
    "    )\n",
    "    if not os.path.isfile(processed_data_path_norm):\n",
    "        print(\n",
    "            \"--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\"\n",
    "        )\n",
    "        # data transformation after train-test split.\n",
    "        col_num = dataset.observed_values.shape[1]\n",
    "        max_arr = np.zeros(col_num)\n",
    "        min_arr = np.zeros(col_num)\n",
    "        mean_arr = np.zeros(col_num)\n",
    "        for k in range(col_num):\n",
    "            # Using observed_mask to avoid counting missing values.\n",
    "            obs_ind = dataset.observed_masks[train_index, k].astype(bool)\n",
    "            temp = dataset.observed_values[train_index, k]\n",
    "            max_arr[k] = max(temp[obs_ind])\n",
    "            min_arr[k] = min(temp[obs_ind])\n",
    "        # print(f\"--------------Max-value for each column {max_arr}--------------\")\n",
    "        # print(f\"--------------Min-value for each column {min_arr}--------------\")\n",
    "\n",
    "        dataset.observed_values = (\n",
    "            (dataset.observed_values - 0 + 1) / (max_arr - 0 + 1)\n",
    "        ) * dataset.observed_masks\n",
    "\n",
    "        with open(processed_data_path_norm, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                [dataset.observed_values, dataset.observed_masks, dataset.gt_masks, dataset.eval_length], f\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_name = \"C1_double\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/h1fq6f_x6tj819lfbsgb561h0000gn/T/ipykernel_60734/1537670710.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Dataset created--------\n",
      "Missing Name: C0_lower\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C0_upper\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C0_double\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C1_lower\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C1_upper\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C1_double\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C2_lower\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C2_upper\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C2_double\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C3_lower\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C3_upper\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C3_double\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C4_lower\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C4_upper\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C4_double\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C5_lower\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C5_upper\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C5_double\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C6_lower\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C6_upper\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C6_double\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C7_lower\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C7_upper\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------Dataset created--------\n",
      "Missing Name: C7_double\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n"
     ]
    }
   ],
   "source": [
    "missing_type = \"BN\"\n",
    "for missing_name in missing_list:\n",
    "    missing_para = missing_list[missing_name]\n",
    "    prepare_dataset(\"california\",seed=1, nfold=5, batch_size=16,\n",
    "                   missing_type = missing_type, missing_para = missing_para, missing_name = missing_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list = {\n",
    "    \"C0_lower\":{\n",
    "        \"column\":0,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C0_upper\":{\n",
    "        \"column\":0,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C0_double\":{\n",
    "        \"column\":0,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C1_lower\":{\n",
    "        \"column\":1,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C1_upper\":{\n",
    "        \"column\":1,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C1_double\":{\n",
    "        \"column\":1,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C2_lower\":{\n",
    "        \"column\":2,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C2_upper\":{\n",
    "        \"column\":2,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C2_double\":{\n",
    "        \"column\":2,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C3_lower\":{\n",
    "        \"column\":3,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C3_upper\":{\n",
    "        \"column\":3,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C3_double\":{\n",
    "        \"column\":3,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C4_lower\":{\n",
    "        \"column\":4,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C4_upper\":{\n",
    "        \"column\":4,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C4_double\":{\n",
    "        \"column\":4,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C5_lower\":{\n",
    "        \"column\":5,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C5_upper\":{\n",
    "        \"column\":5,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C5_double\":{\n",
    "        \"column\":5,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C6_lower\":{\n",
    "        \"column\":6,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C6_upper\":{\n",
    "        \"column\":6,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C6_double\":{\n",
    "        \"column\":6,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C7_lower\":{\n",
    "        \"column\":7,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C7_upper\":{\n",
    "        \"column\":7,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C7_double\":{\n",
    "        \"column\":7,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"BM_missing.json\", \"w\") as file:\n",
    "    json.dump(missing_list, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
