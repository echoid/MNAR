{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from data_loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_loader(\"california\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from missing_process.block_rules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_loaders import *\n",
    "import missing_process.missing_method as missing_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_middle_single_column(lower,upper,partial_missing,dataset):\n",
    "    if lower == 0:\n",
    "        lower_quantile = np.min(dataset, axis=0)\n",
    "    else:\n",
    "        lower_quantile = np.quantile(dataset,lower, axis=0)\n",
    "    if upper == 1:\n",
    "        upper_quantile = np.max(dataset, axis=0)\n",
    "    else:\n",
    "        upper_quantile = np.quantile(dataset,upper, axis=0)\n",
    "\n",
    "\n",
    "    ix_larger_than = dataset >= lower_quantile\n",
    "    ix_smaller_than = dataset <= upper_quantile\n",
    "\n",
    "    \n",
    "    combined_ix = np.equal(ix_larger_than, ix_smaller_than)\n",
    "    combined_ix = random_missing_single_column(combined_ix,partial_missing)\n",
    "    return combined_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_missing_single_column(array, fraction_to_change):\n",
    "\n",
    "    result = array.copy()\n",
    "    n_to_change = int(np.sum(result) * fraction_to_change)\n",
    "    ix_to_change = np.random.choice(np.flatnonzero(result), size=n_to_change, replace=False)\n",
    "\n",
    "    result[ix_to_change] = False\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_single(X,multiple_block,missing_dim = 1):\n",
    "    \n",
    "    Xnan = X.copy()\n",
    "\n",
    "    #---- Missing Dimention\n",
    "    \n",
    "    ix_list = []\n",
    "    for key in multiple_block.keys():\n",
    "        info = multiple_block[key]\n",
    "        combined_ix = generate_middle_single_column(info[\"lower\"],info[\"upper\"],info[\"partial_missing\"], X)\n",
    "        ix_list.append(combined_ix)\n",
    "    combined_ix = np.logical_or.reduce(ix_list)\n",
    "    \n",
    "    Xnan[:,][combined_ix] = np.nan\n",
    "\n",
    "    masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM_missing(observed_values,missing_para):\n",
    "\n",
    "    mask = np.ones(observed_values.shape)\n",
    "    mask_single = missing_single(observed_values[:,missing_para[\"column\"]],missing_para[\"missing\"])\n",
    "    mask[:,missing_para[\"column\"]]= mask_single\n",
    "\n",
    "    print(\"MASK from BM_missing\",mask)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5],[6,6,6],[7,7,7],[8,8,8],[9,9,9],[10,10,10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(dataname,path: str, aug_rate=1,missing_type = \"MCAR\",\n",
    "                  missing_para = \"\"):\n",
    " \n",
    "    data = dataset_loader(dataname)\n",
    "    observed_values = data[\"data\"].astype(\"float32\")\n",
    "\n",
    "    #observed_values = np.array([[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5],[6,6,6],[7,7,7],[8,8,8],[9,9,9],[10,10,10]]).astype(\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "    observed_masks = ~np.isnan(observed_values)\n",
    "    masks = observed_masks.copy()\n",
    "    \n",
    "    \"Need input origin dataset and parameters\"\n",
    "    if missing_type == \"MCAR\":\n",
    "        #masks = MCAR(observed_values,missing_para,masks)\n",
    "        pass\n",
    "\n",
    "    elif missing_type == \"quantile\":\n",
    "        Xnan, Xz = missing_method.missing_by_range(observed_values, missing_para)\n",
    "        masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
    "\n",
    "    elif missing_type == \"logistic\":\n",
    "        masks = missing_method.MNAR_mask_logistic(observed_values, missing_para)\n",
    "\n",
    "    elif missing_type == \"diffuse\":\n",
    "        #masks = missing_method.MNAR_self_mask_logistic(observed_values, missing_para)\n",
    "\n",
    "        masks = diffuse_mnar_single(observed_values, missing_para[0],missing_para[1])\n",
    "\n",
    "    elif missing_type == \"BN\":\n",
    "        print(\"go BN\")\n",
    "        masks = BM_missing(observed_values, missing_para)\n",
    "\n",
    "\n",
    "    # gt_mask: 0 for missing elements and manully maksed elements\n",
    "\n",
    "    gt_masks = masks.reshape(observed_masks.shape)\n",
    "\n",
    "    observed_values = np.nan_to_num(observed_values)\n",
    "    observed_masks = observed_masks.astype(int)\n",
    "    gt_masks = gt_masks.astype(int)\n",
    "\n",
    "    return observed_values, observed_masks, gt_masks, data[\"data\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class tabular_dataset(Dataset):\n",
    "    # eval_length should be equal to attributes number.\n",
    "    def __init__(\n",
    "        self, dataname, use_index_list=None, \n",
    "        aug_rate=1, seed=0,\n",
    "        missing_type = \"MCAR\", missing_para = \"\",missing_name = \"MCAR\"\n",
    "        ):\n",
    "        #self.eval_length = eval_length\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        dataset_path = f\"datasets/{dataname}/data.csv\"\n",
    "        processed_data_path = (\n",
    "            f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}.pk\"\n",
    "        )\n",
    "        processed_data_path_norm = (\n",
    "            f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}_max-min_norm.pk\"\n",
    "        )\n",
    "\n",
    "        # print(processed_data_path)\n",
    "        # print(processed_data_path_norm)\n",
    "        # If no dataset created\n",
    "        if not os.path.isfile(processed_data_path):\n",
    "            self.observed_values, self.observed_masks, self.gt_masks, self.eval_length = generate_mask(\n",
    "                dataname, dataset_path, aug_rate=aug_rate,\n",
    "                missing_type = missing_type, missing_para = missing_para)\n",
    "            print(\"Self.gtMasks\",self.gt_masks)\n",
    "            with open(processed_data_path, \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    [self.observed_values, self.observed_masks, self.gt_masks, self.eval_length], f\n",
    "                )\n",
    "            print(\"--------Dataset created--------\")\n",
    "\n",
    "        elif os.path.isfile(processed_data_path_norm):\n",
    "            with open(processed_data_path_norm, \"rb\") as f:\n",
    "                self.observed_values, self.observed_masks, self.gt_masks, self.eval_length = pickle.load(\n",
    "                    f\n",
    "                )\n",
    "            print(\"--------Normalized dataset loaded--------\")\n",
    "        \n",
    "        if use_index_list is None:\n",
    "            self.use_index_list = np.arange(len(self.observed_values))\n",
    "        else:\n",
    "            self.use_index_list = use_index_list\n",
    "\n",
    "    def __getitem__(self, org_index):\n",
    "        index = self.use_index_list[org_index]\n",
    "        s = {\n",
    "            \"observed_data\": self.observed_values[index],\n",
    "            \"observed_mask\": self.observed_masks[index],\n",
    "            \"gt_mask\": self.gt_masks[index],\n",
    "            \"timepoints\": np.arange(self.eval_length),\n",
    "        }\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_index_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columnwise_min_max_scaling(dataset):\n",
    "    col_num = dataset.observed_values.shape[1]\n",
    "    max_arr = np.zeros(col_num)\n",
    "    min_arr = np.zeros(col_num)\n",
    "\n",
    "    for k in range(col_num):\n",
    "        # Using observed_mask to avoid counting missing values.\n",
    "        obs_ind = dataset.gt_masks[:, k].astype(bool)\n",
    "        temp = dataset.observed_values[:, k][obs_ind]\n",
    "        if len(temp) > 0:  # Check if there are non-zero mask values\n",
    "            max_arr[k] = max(temp)\n",
    "            min_arr[k] = min(temp)\n",
    "        else:\n",
    "            max_arr[k] = 1\n",
    "            min_arr[k] = 0\n",
    "\n",
    "    dataset.observed_values = ((dataset.observed_values - min_arr) / (max_arr - min_arr + 1e-6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataname, seed=1, nfold=5, batch_size=16,\n",
    "                   missing_type = \"Quantile\", missing_para = 0.5, missing_name = \"Q1_complete\"):\n",
    "\n",
    "    dataset = tabular_dataset(dataname = dataname,seed=seed,\n",
    "                              missing_type = missing_type, missing_para = missing_para,\n",
    "                                missing_name = missing_name)\n",
    "    # print(\"Missing Type:\",missing_type)\n",
    "    # print(\"Missing Para:\",missing_para)\n",
    "    print(\"Missing Name:\",missing_name)\n",
    "    \n",
    "    \n",
    "    indlist = np.arange(len(dataset))\n",
    "\n",
    "    np.random.seed(seed + 1)\n",
    "    np.random.shuffle(indlist)\n",
    "\n",
    "    tmp_ratio = 1 / nfold\n",
    "    start = (int)((nfold - 1) * len(dataset) * tmp_ratio)\n",
    "    \n",
    "    end = (int)(nfold * len(dataset) * tmp_ratio)\n",
    "\n",
    "    test_index = indlist[start:end]\n",
    "    remain_index = np.delete(indlist, np.arange(start, end))\n",
    "\n",
    "    np.random.shuffle(remain_index)\n",
    "\n",
    "    # Modify here to change train,valid ratio\n",
    "    num_train = (int)(len(remain_index) * 1)\n",
    "    train_index = remain_index[:num_train]\n",
    "    valid_index = remain_index[num_train:]\n",
    "\n",
    "\n",
    "\n",
    "    # Here we perform max-min normalization.\n",
    "    print(\"Here we perform max-min normalization.\")\n",
    "    processed_data_path_norm = (\n",
    "        f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}_max-min_norm.pk\"\n",
    "    )\n",
    "    if not os.path.isfile(processed_data_path_norm):\n",
    "        print(\n",
    "            \"--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\"\n",
    "        )\n",
    "# 使用上面的函数来进行数据处理\n",
    "        columnwise_min_max_scaling(dataset)\n",
    "\n",
    "        with open(processed_data_path_norm, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                [dataset.observed_values, dataset.observed_masks, dataset.gt_masks, dataset.eval_length], f\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar_single(data, up_percentile = 0.5, obs_percentile = 0.5):\n",
    "    \n",
    "    def scale_data(data):\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "        return scaled_data\n",
    "\n",
    "    data = scale_data(data)\n",
    "\n",
    "    mask = np.ones(data.shape)\n",
    "\n",
    "    n_cols = data.shape[1]\n",
    "    n_miss_cols = int(n_cols * 0.5)  # 选择50%的列作为缺失列\n",
    "    miss_cols = np.random.choice(n_cols, size=n_miss_cols, replace=False)  # 随机选择缺失列的索引\n",
    "\n",
    "    obs_cols = [col for col in range(data.shape[1]) if col not in miss_cols]\n",
    "    \n",
    "    for miss_col in miss_cols:\n",
    "        missvar_bounds = np.quantile(data[:, miss_col], up_percentile)\n",
    "        temp = data[:, miss_col] > missvar_bounds\n",
    "        \n",
    "        obsvar_bounds = np.quantile(data[temp][:, obs_cols], obs_percentile)\n",
    "        temp2 = data[:, miss_col] > obsvar_bounds\n",
    "\n",
    "        merged_temp = np.logical_or(temp, temp2).astype(int)\n",
    "        mask[:, miss_col] = merged_temp\n",
    "    print(\"Missing Rate\",1 - np.count_nonzero(mask) / mask.size)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_name = \"C1_double\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self.gtMasks [[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 1 0]\n",
      " [0 1 0 ... 0 1 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 1 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "--------Dataset created--------\n",
      "Missing Name: 0.25\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "Self.gtMasks [[0 1 0 ... 0 1 0]\n",
      " [0 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 0 1]\n",
      " ...\n",
      " [0 0 0 ... 1 0 1]\n",
      " [0 1 0 ... 1 1 1]\n",
      " [0 0 0 ... 1 1 1]]\n",
      "--------Dataset created--------\n",
      "Missing Name: 0.5\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "Self.gtMasks [[0 0 1 ... 1 1 1]\n",
      " [1 1 1 ... 0 1 1]\n",
      " [0 0 1 ... 1 1 1]\n",
      " ...\n",
      " [0 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 0 1 1]\n",
      " [1 0 0 ... 1 1 1]]\n",
      "--------Dataset created--------\n",
      "Missing Name: 0.75\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n"
     ]
    }
   ],
   "source": [
    "dataset = \"california\"\n",
    "dataset = \"concrete_compression\"\n",
    "#dataset = \"wine_quality_white\"\n",
    "#dataset = \"wine_quality_red\"\n",
    "#dataset = \"banknote\"\n",
    "\n",
    "#\"concrete_compression\"#,\"wine_quality_white\",\"banknote\",\n",
    "seed = 1\n",
    "nfold = 5\n",
    "\n",
    "\n",
    "missing_type = \"logistic\"\n",
    "#missing_type = \"diffuse\"\n",
    "#missing_type = \"quantile\"\n",
    "#missing_type = \"BM\"\n",
    "\n",
    "missing_rule = load_json_file(\"q_ratio.json\")\n",
    "#missing_rule = load_json_file(\"diffuse_ratio.json\")\n",
    "\n",
    "#missing_rule = load_json_file(\"single_quantile.json\")\n",
    "#missing_rule = load_json_file(\"double_quantile_1.json\")\n",
    "#missing_rule = load_json_file(\"double_quantile_2.json\")\n",
    "\n",
    "#missing_rule = load_json_file(\"BM_missing.json\")\n",
    "\n",
    "\n",
    "for missing_name in missing_rule:\n",
    "    missing_para = missing_rule[missing_name]\n",
    "    prepare_dataset(\n",
    "        dataname=dataset,\n",
    "        seed=seed,nfold=nfold,batch_size=128,\n",
    "        missing_type = missing_type,missing_para = missing_para,missing_name = missing_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q2_Q3_complete': {'1': {'lower': 0.25, 'upper': 0.5, 'partial_missing': 0.0},\n",
       "  '2': {'lower': 0.5, 'upper': 0.75, 'partial_missing': 0.0}},\n",
       " 'Q2_Q3_partial': {'1': {'lower': 0.25, 'upper': 0.5, 'partial_missing': 0.5},\n",
       "  '2': {'lower': 0.5, 'upper': 0.75, 'partial_missing': 0.5}},\n",
       " 'Q2_Q4_complete': {'1': {'lower': 0.25, 'upper': 0.5, 'partial_missing': 0.0},\n",
       "  '2': {'lower': 0.75, 'upper': 1, 'partial_missing': 0.0}},\n",
       " 'Q2_Q4_partial': {'1': {'lower': 0.25, 'upper': 0.5, 'partial_missing': 0.5},\n",
       "  '2': {'lower': 0.75, 'upper': 1, 'partial_missing': 0.5}},\n",
       " 'Q3_Q4_complete': {'1': {'lower': 0.5, 'upper': 0.75, 'partial_missing': 0.0},\n",
       "  '2': {'lower': 0.75, 'upper': 1, 'partial_missing': 0.0}},\n",
       " 'Q3_Q4_partial': {'1': {'lower': 0.5, 'upper': 0.75, 'partial_missing': 0.5},\n",
       "  '2': {'lower': 0.75, 'upper': 1, 'partial_missing': 0.5}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C0_lower\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C0_upper\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C0_double\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C1_lower\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C1_upper\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C1_double\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C2_lower\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C2_upper\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C2_double\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C3_lower\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C3_upper\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C3_double\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C4_lower\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C4_upper\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C4_double\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C5_lower\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C5_upper\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C5_double\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C6_lower\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C6_upper\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C6_double\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C7_lower\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C7_upper\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C7_double\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C8_lower\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C8_upper\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C8_double\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C9_lower\n",
      "Here we perform max-min normalization.\n",
      "--------Normalized dataset loaded--------\n",
      "Missing Name: C9_double\n",
      "Here we perform max-min normalization.\n",
      "go BN\n",
      "MASK from BM_missing [[1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "Self.gtMasks [[1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]]\n",
      "--------Dataset created--------\n",
      "Missing Name: C10_lower\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "go BN\n",
      "MASK from BM_missing [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]]\n",
      "Self.gtMasks [[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 0]]\n",
      "--------Dataset created--------\n",
      "Missing Name: C10_upper\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "go BN\n",
      "MASK from BM_missing [[1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]]\n",
      "Self.gtMasks [[1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 0]]\n",
      "--------Dataset created--------\n",
      "Missing Name: C10_double\n",
      "Here we perform max-min normalization.\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "go BN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/h1fq6f_x6tj819lfbsgb561h0000gn/T/ipykernel_41656/1537670710.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      "/var/folders/yl/h1fq6f_x6tj819lfbsgb561h0000gn/T/ipykernel_41656/1537670710.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      "/var/folders/yl/h1fq6f_x6tj819lfbsgb561h0000gn/T/ipykernel_41656/1537670710.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 1 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m missing_name \u001b[39min\u001b[39;00m missing_list:\n\u001b[1;32m      4\u001b[0m     missing_para \u001b[39m=\u001b[39m missing_list[missing_name]\n\u001b[0;32m----> 5\u001b[0m     prepare_dataset(dataset,seed\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, nfold\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m                    missing_type \u001b[39m=\u001b[39;49m missing_type, missing_para \u001b[39m=\u001b[39;49m missing_para, missing_name \u001b[39m=\u001b[39;49m missing_name)\n",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(dataname, seed, nfold, batch_size, missing_type, missing_para, missing_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_dataset\u001b[39m(dataname, seed\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, nfold\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[1;32m      2\u001b[0m                    missing_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mQuantile\u001b[39m\u001b[39m\"\u001b[39m, missing_para \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m, missing_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mQ1_complete\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     dataset \u001b[39m=\u001b[39m tabular_dataset(dataname \u001b[39m=\u001b[39;49m dataname,seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m      5\u001b[0m                               missing_type \u001b[39m=\u001b[39;49m missing_type, missing_para \u001b[39m=\u001b[39;49m missing_para,\n\u001b[1;32m      6\u001b[0m                                 missing_name \u001b[39m=\u001b[39;49m missing_name)\n\u001b[1;32m      7\u001b[0m     \u001b[39m# print(\"Missing Type:\",missing_type)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39m# print(\"Missing Para:\",missing_para)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMissing Name:\u001b[39m\u001b[39m\"\u001b[39m,missing_name)\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36mtabular_dataset.__init__\u001b[0;34m(self, dataname, use_index_list, aug_rate, seed, missing_type, missing_para, missing_name)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# print(processed_data_path)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# print(processed_data_path_norm)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# If no dataset created\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(processed_data_path):\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobserved_values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobserved_masks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgt_masks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_length \u001b[39m=\u001b[39m generate_mask(\n\u001b[1;32m     24\u001b[0m         dataname, dataset_path, aug_rate\u001b[39m=\u001b[39;49maug_rate,\n\u001b[1;32m     25\u001b[0m         missing_type \u001b[39m=\u001b[39;49m missing_type, missing_para \u001b[39m=\u001b[39;49m missing_para)\n\u001b[1;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSelf.gtMasks\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgt_masks)\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(processed_data_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m, in \u001b[0;36mgenerate_mask\u001b[0;34m(dataname, path, aug_rate, missing_type, missing_para)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39melif\u001b[39;00m missing_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBN\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     32\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgo BN\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     masks \u001b[39m=\u001b[39m BM_missing(observed_values, missing_para)\n\u001b[1;32m     36\u001b[0m \u001b[39m# gt_mask: 0 for missing elements and manully maksed elements\u001b[39;00m\n\u001b[1;32m     38\u001b[0m gt_masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mreshape(observed_masks\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36mBM_missing\u001b[0;34m(observed_values, missing_para)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mBM_missing\u001b[39m(observed_values,missing_para):\n\u001b[1;32m      3\u001b[0m     mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(observed_values\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m     mask_single \u001b[39m=\u001b[39m missing_single(observed_values[:,missing_para[\u001b[39m\"\u001b[39;49m\u001b[39mcolumn\u001b[39;49m\u001b[39m\"\u001b[39;49m]],missing_para[\u001b[39m\"\u001b[39m\u001b[39mmissing\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m     mask[:,missing_para[\u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m=\u001b[39m mask_single\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMASK from BM_missing\u001b[39m\u001b[39m\"\u001b[39m,mask)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11 is out of bounds for axis 1 with size 11"
     ]
    }
   ],
   "source": [
    "missing_type = \"BN\"\n",
    "dataset = \"wine_quality_white\"\n",
    "for missing_name in missing_list:\n",
    "    missing_para = missing_list[missing_name]\n",
    "    prepare_dataset(dataset,seed=1, nfold=5, batch_size=16,\n",
    "                   missing_type = missing_type, missing_para = missing_para, missing_name = missing_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list = {\n",
    "    \"C0_lower\":{\n",
    "        \"column\":0,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C0_upper\":{\n",
    "        \"column\":0,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C0_double\":{\n",
    "        \"column\":0,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C1_lower\":{\n",
    "        \"column\":1,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C1_upper\":{\n",
    "        \"column\":1,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C1_double\":{\n",
    "        \"column\":1,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C2_lower\":{\n",
    "        \"column\":2,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C2_upper\":{\n",
    "        \"column\":2,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C2_double\":{\n",
    "        \"column\":2,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C3_lower\":{\n",
    "        \"column\":3,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C3_upper\":{\n",
    "        \"column\":3,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C3_double\":{\n",
    "        \"column\":3,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C4_lower\":{\n",
    "        \"column\":4,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C4_upper\":{\n",
    "        \"column\":4,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C4_double\":{\n",
    "        \"column\":4,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C5_lower\":{\n",
    "        \"column\":5,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C5_upper\":{\n",
    "        \"column\":5,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C5_double\":{\n",
    "        \"column\":5,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C6_lower\":{\n",
    "        \"column\":6,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C6_upper\":{\n",
    "        \"column\":6,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C6_double\":{\n",
    "        \"column\":6,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \"C7_lower\":{\n",
    "        \"column\":7,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C7_upper\":{\n",
    "        \"column\":7,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C7_double\":{\n",
    "        \"column\":7,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \n",
    "    \"C8_lower\":{\n",
    "        \"column\":8,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C8_upper\":{\n",
    "        \"column\":8,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C8_double\":{\n",
    "        \"column\":8,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \n",
    "    \"C9_lower\":{\n",
    "        \"column\":9,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C8_upper\":{\n",
    "        \"column\":9,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C9_double\":{\n",
    "        \"column\":8,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \n",
    "\n",
    "    \"C10_lower\":{\n",
    "        \"column\":10,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C10_upper\":{\n",
    "        \"column\":10,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C10_double\":{\n",
    "        \"column\":10,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "    \n",
    "    \"C11_lower\":{\n",
    "        \"column\":11,\n",
    "        \"missing\":{1:{\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0}}\n",
    "    },\n",
    "    \"C11_upper\":{\n",
    "        \"column\":11,\n",
    "        \"missing\":{1:{\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}\n",
    "    },\n",
    "    \"C11_double\":{\n",
    "        \"column\":11,\n",
    "        \"missing\": {\"1\": {\"lower\": 0.0, \"upper\": 0.35, \"partial_missing\": 0.0}, \n",
    "                    \"2\": {\"lower\": 0.65, \"upper\": 1, \"partial_missing\": 0.0}}},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"BM_missing.json\", \"w\") as file:\n",
    "    json.dump(missing_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7348913   0.67358271 -0.0466758   0.1950446 ]\n",
      " [ 0.65579731 -0.06367053  0.17834973  0.5178313 ]\n",
      " [ 0.60817107  0.76605918 -0.12639074 -0.24375432]\n",
      " ...\n",
      " [-0.23022065 -0.80191988  0.98284117 -0.0755649 ]\n",
      " [-0.20851314 -0.45560047  0.71584793  0.23204505]\n",
      " [-0.08964529  0.071439    0.21737031  0.74188154]] [[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " ...\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]] [[1 1 0 1]\n",
      " [1 0 1 1]\n",
      " [1 1 0 0]\n",
      " ...\n",
      " [0 0 1 0]\n",
      " [0 0 1 1]\n",
      " [0 1 1 1]] 4\n"
     ]
    }
   ],
   "source": [
    "processed_data_path_norm = \"datasets/banknote/quantile-Q1_complete_seed-1_max-min_norm.pk\"\n",
    "    \n",
    "\n",
    "with open(processed_data_path_norm, \"rb\") as f:\n",
    "        observed_values, observed_masks, gt_masks, eval_length = pickle.load(\n",
    "                f\n",
    "        )\n",
    "\n",
    "print(observed_values, observed_masks, gt_masks, eval_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
