{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\ANACONDA\\envs\\py3.10\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('./')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams['font.size'] = 15.0\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['savefig.format'] = 'pdf'\n",
    "plt.rcParams['lines.linewidth'] = 2.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Here we use the white-wine dataset from the UCI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "data1 = np.array(pd.read_csv(url, low_memory=False, sep=';'))\n",
    "# ---- drop the classification attribute\n",
    "data = np.round(data1[:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  0.,  0., ...,  0.,  9.,  6.],\n",
       "       [ 6.,  0.,  0., ...,  0., 10.,  6.],\n",
       "       [ 8.,  0.,  0., ...,  0., 10.,  6.],\n",
       "       ...,\n",
       "       [ 6.,  0.,  0., ...,  0.,  9.,  6.],\n",
       "       [ 6.,  0.,  0., ...,  0., 13.,  7.],\n",
       "       [ 6.,  0.,  0., ...,  0., 12.,  6.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "# All numerical and the last column is the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, D = data.shape\n",
    "n_latent = D - 1\n",
    "n_hidden = 128\n",
    "n_samples = 20\n",
    "max_iter = 30000\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/h1fq6f_x6tj819lfbsgb561h0000gn/T/ipykernel_72448/293900972.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  data = data / np.std(data, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# ---- standardize data\n",
    "data = data - np.mean(data, axis=0)\n",
    "data = data / np.std(data, axis=0)\n",
    "\n",
    "# ---- random permutation \n",
    "p = np.random.permutation(N)\n",
    "data = data[p, :]\n",
    "\n",
    "# ---- we use the full dataset for training here, but you can make a train-val split\n",
    "Xtrain = data.copy()\n",
    "Xval = Xtrain.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce missing \n",
    "Here we denote\n",
    "- Xnan: data matrix with np.nan as the missing entries\n",
    "- Xz: data matrix with 0 as the missing entries\n",
    "- S: missing mask \n",
    "\n",
    "The missing process depends on the missing data itself:\n",
    "- in half the features, set the feature value to missing when it is higher than the feature mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- introduce missing process\n",
    "Xnan = Xtrain.copy()\n",
    "Xz = Xtrain.copy()\n",
    "\n",
    "# Selecte half of them, if the number is less than mean then we seen them as missing\n",
    "# (Using np.nan or 0 to represent)\n",
    "mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "ix_larger_than_mean = Xnan[:, :int(D / 2)] > mean\n",
    "\n",
    "Xnan[:, :int(D / 2)][ix_larger_than_mean] = np.nan\n",
    "Xz[:, :int(D / 2)][ix_larger_than_mean] = 0\n",
    "\n",
    "# Mask to indicate if we have missing value\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "The model we are building has a Gaussian prior and a Gaussian observation model,\n",
    "\n",
    "$$ p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} | \\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "$$ p(\\mathbf{x} | \\mathbf{z}) = \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_{\\theta}(\\mathbf{z}), \\sigma^2\\mathbf{I})$$\n",
    "\n",
    "$$ p(\\mathbf{x}) = \\int p(\\mathbf{x} | \\mathbf{z})p(\\mathbf{z}) d\\mathbf{z}$$\n",
    "\n",
    "where $\\mathbf{\\mu}_{\\theta}(\\mathbf{z}): \\mathbb{R}^d \\rightarrow \\mathbb{R}^p $ in general is a deep neural net, but in this case is a linear mapping, $\\mathbf{\\mu} = \\mathbf{Wz + b}$.\n",
    "\n",
    "The variational posterior is also Gaussian\n",
    "\n",
    "$$q_{\\gamma}(\\mathbf{z} | \\mathbf{x}) = \\mathcal{N}(\\mathbf{z} | \\mu_{\\gamma}(\\mathbf{x}), \\sigma_{\\gamma}(\\mathbf{x})^2 \\mathbf{I})$$\n",
    "\n",
    "If the missing process is *missing at random*, it is ignorable and the ELBO becomes, as described in [the MIWAE paper](https://arxiv.org/abs/1812.02633)\n",
    "\n",
    "$$ E_{\\mathbf{z}_1...\\mathbf{z}_K} \\left[ \\log \\frac{1}{K}\\sum_{k=1}^K \\frac{p_{\\theta}(\\mathbf{x^o} | \\mathbf{z}_k)p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z}_k | \\mathbf{x^o})} \\right] $$\n",
    "\n",
    "When the missing process is MNAR it is non-ignorable and we need to include the missing model. In this example we include the missing model as a logistic regression in each feature dimension\n",
    "\n",
    "$$ p_{\\phi}(\\mathbf{s} | \\mathbf{x^o, x^m}) = \\text{Bern}(\\mathbf{s} | \\pi_{\\phi}(\\mathbf{x^o, x^m}))$$\n",
    "\n",
    "$$ \\pi_{\\phi, j}(x_j) = \\frac{1}{1 + e^{-\\text{logits}_j}} $$\n",
    "\n",
    "$$ \\text{logits}_j = W_j (x_j - b_j) $$\n",
    "\n",
    "The ELBO in the MNAR case becomes\n",
    "\n",
    "$$ E_{(\\mathbf{z}_1, \\mathbf{x}_1^m)...(\\mathbf{z}_K, \\mathbf{x}_K^m)} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\phi}(\\mathbf{s} | \\mathbf{x}^o, \\mathbf{x}_k^m) p_{\\theta}(\\mathbf{x}^o | \\mathbf{z}_k) p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z} | \\mathbf{x}^o)} \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "Let's first define the inputs of the model\n",
    "- x_pl: data input\n",
    "- s_pl: mask input\n",
    "- n_pl: number of importance samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n",
      "0/30000 updates, 0.16 s, nan train_loss, nan val_loss\n",
      "0 / 4898\n",
      "100 / 4898\n",
      "200 / 4898\n",
      "300 / 4898\n",
      "400 / 4898\n",
      "500 / 4898\n",
      "600 / 4898\n",
      "700 / 4898\n",
      "800 / 4898\n",
      "900 / 4898\n",
      "1000 / 4898\n",
      "1100 / 4898\n",
      "1200 / 4898\n",
      "1300 / 4898\n",
      "1400 / 4898\n",
      "1500 / 4898\n",
      "1600 / 4898\n",
      "1700 / 4898\n",
      "1800 / 4898\n",
      "1900 / 4898\n",
      "2000 / 4898\n",
      "2100 / 4898\n",
      "2200 / 4898\n",
      "2300 / 4898\n",
      "2400 / 4898\n",
      "2500 / 4898\n",
      "2600 / 4898\n",
      "2700 / 4898\n",
      "2800 / 4898\n",
      "2900 / 4898\n",
      "3000 / 4898\n",
      "3100 / 4898\n",
      "3200 / 4898\n",
      "3300 / 4898\n",
      "3400 / 4898\n",
      "3500 / 4898\n",
      "3600 / 4898\n",
      "3700 / 4898\n",
      "3800 / 4898\n",
      "3900 / 4898\n",
      "4000 / 4898\n",
      "4100 / 4898\n",
      "4200 / 4898\n",
      "4300 / 4898\n",
      "4400 / 4898\n",
      "4500 / 4898\n",
      "4600 / 4898\n",
      "4700 / 4898\n",
      "4800 / 4898\n",
      "imputation RMSE:  nan\n",
      "0 / 4898\n",
      "100 / 4898\n",
      "200 / 4898\n",
      "300 / 4898\n",
      "400 / 4898\n",
      "500 / 4898\n",
      "600 / 4898\n",
      "700 / 4898\n",
      "800 / 4898\n",
      "900 / 4898\n",
      "1000 / 4898\n",
      "1100 / 4898\n",
      "1200 / 4898\n",
      "1300 / 4898\n",
      "1400 / 4898\n",
      "1500 / 4898\n",
      "1600 / 4898\n",
      "1700 / 4898\n",
      "1800 / 4898\n",
      "1900 / 4898\n",
      "2000 / 4898\n",
      "2100 / 4898\n",
      "2200 / 4898\n",
      "2300 / 4898\n",
      "2400 / 4898\n",
      "2500 / 4898\n",
      "2600 / 4898\n",
      "2700 / 4898\n",
      "2800 / 4898\n",
      "2900 / 4898\n",
      "3000 / 4898\n",
      "3100 / 4898\n",
      "3200 / 4898\n",
      "3300 / 4898\n",
      "3400 / 4898\n",
      "3500 / 4898\n",
      "3600 / 4898\n",
      "3700 / 4898\n",
      "3800 / 4898\n",
      "3900 / 4898\n",
      "4000 / 4898\n",
      "4100 / 4898\n",
      "4200 / 4898\n",
      "4300 / 4898\n",
      "4400 / 4898\n",
      "4500 / 4898\n",
      "4600 / 4898\n",
      "4700 / 4898\n",
      "4800 / 4898\n",
      "imputation RMSE:  nan\n",
      "0 / 4898\n",
      "100 / 4898\n",
      "200 / 4898\n",
      "300 / 4898\n",
      "400 / 4898\n",
      "500 / 4898\n",
      "600 / 4898\n",
      "700 / 4898\n",
      "800 / 4898\n",
      "900 / 4898\n",
      "1000 / 4898\n",
      "1100 / 4898\n",
      "1200 / 4898\n",
      "1300 / 4898\n",
      "1400 / 4898\n",
      "1500 / 4898\n",
      "1600 / 4898\n",
      "1700 / 4898\n",
      "1800 / 4898\n",
      "1900 / 4898\n",
      "2000 / 4898\n",
      "2100 / 4898\n",
      "2200 / 4898\n",
      "2300 / 4898\n",
      "2400 / 4898\n",
      "2500 / 4898\n",
      "2600 / 4898\n",
      "2700 / 4898\n",
      "2800 / 4898\n",
      "2900 / 4898\n",
      "3000 / 4898\n",
      "3100 / 4898\n",
      "3200 / 4898\n",
      "3300 / 4898\n",
      "3400 / 4898\n",
      "3500 / 4898\n",
      "3600 / 4898\n",
      "3700 / 4898\n",
      "3800 / 4898\n",
      "3900 / 4898\n",
      "4000 / 4898\n",
      "4100 / 4898\n",
      "4200 / 4898\n",
      "4300 / 4898\n",
      "4400 / 4898\n",
      "4500 / 4898\n",
      "4600 / 4898\n",
      "4700 / 4898\n",
      "4800 / 4898\n",
      "imputation RMSE:  nan\n",
      "0 / 4898\n",
      "100 / 4898\n",
      "200 / 4898\n",
      "300 / 4898\n",
      "400 / 4898\n",
      "500 / 4898\n",
      "600 / 4898\n",
      "700 / 4898\n",
      "800 / 4898\n",
      "900 / 4898\n",
      "1000 / 4898\n",
      "1100 / 4898\n",
      "1200 / 4898\n",
      "1300 / 4898\n",
      "1400 / 4898\n",
      "1500 / 4898\n",
      "1600 / 4898\n",
      "1700 / 4898\n",
      "1800 / 4898\n",
      "1900 / 4898\n",
      "2000 / 4898\n",
      "2100 / 4898\n",
      "2200 / 4898\n",
      "2300 / 4898\n",
      "2400 / 4898\n",
      "2500 / 4898\n",
      "2600 / 4898\n",
      "2700 / 4898\n",
      "2800 / 4898\n",
      "2900 / 4898\n",
      "3000 / 4898\n",
      "3100 / 4898\n",
      "3200 / 4898\n",
      "3300 / 4898\n",
      "3400 / 4898\n",
      "3500 / 4898\n",
      "3600 / 4898\n",
      "3700 / 4898\n",
      "3800 / 4898\n",
      "3900 / 4898\n",
      "4000 / 4898\n",
      "4100 / 4898\n",
      "4200 / 4898\n",
      "4300 / 4898\n",
      "4400 / 4898\n",
      "4500 / 4898\n",
      "4600 / 4898\n",
      "4700 / 4898\n",
      "4800 / 4898\n",
      "imputation RMSE:  nan\n",
      "0 / 4898\n",
      "100 / 4898\n",
      "200 / 4898\n",
      "300 / 4898\n",
      "400 / 4898\n",
      "500 / 4898\n",
      "600 / 4898\n",
      "700 / 4898\n",
      "800 / 4898\n",
      "900 / 4898\n",
      "1000 / 4898\n",
      "1100 / 4898\n",
      "1200 / 4898\n",
      "1300 / 4898\n",
      "1400 / 4898\n",
      "1500 / 4898\n",
      "1600 / 4898\n",
      "1700 / 4898\n",
      "1800 / 4898\n",
      "1900 / 4898\n",
      "2000 / 4898\n",
      "2100 / 4898\n",
      "2200 / 4898\n",
      "2300 / 4898\n",
      "2400 / 4898\n",
      "2500 / 4898\n",
      "2600 / 4898\n",
      "2700 / 4898\n",
      "2800 / 4898\n",
      "2900 / 4898\n",
      "3000 / 4898\n",
      "3100 / 4898\n",
      "3200 / 4898\n",
      "3300 / 4898\n",
      "3400 / 4898\n",
      "3500 / 4898\n",
      "3600 / 4898\n",
      "3700 / 4898\n",
      "3800 / 4898\n",
      "3900 / 4898\n",
      "4000 / 4898\n",
      "4100 / 4898\n",
      "4200 / 4898\n",
      "4300 / 4898\n",
      "4400 / 4898\n",
      "4500 / 4898\n",
      "4600 / 4898\n",
      "4700 / 4898\n",
      "4800 / 4898\n",
      "imputation RMSE:  nan\n",
      "0 / 4898\n",
      "100 / 4898\n",
      "200 / 4898\n",
      "300 / 4898\n",
      "400 / 4898\n",
      "500 / 4898\n",
      "600 / 4898\n",
      "700 / 4898\n",
      "800 / 4898\n",
      "900 / 4898\n",
      "1000 / 4898\n",
      "1100 / 4898\n",
      "1200 / 4898\n",
      "1300 / 4898\n",
      "1400 / 4898\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(N,D,S,Xz,Xnan, \u001b[39mtype\u001b[39;49m \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mnon-MIWAE\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[23], line 162\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(N, D, S, Xz, Xnan, type)\u001b[0m\n\u001b[1;32m    159\u001b[0m S \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39m~\u001b[39mnp\u001b[39m.\u001b[39misnan(Xnan), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnon-MIWAE\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     rmse, imputations \u001b[39m=\u001b[39m not_imputationRMSE(sess, Xtrain, Xnan, \u001b[39m10000\u001b[39;49m,mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x,x_pl,s_pl,n_pl)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[1;32m    165\u001b[0m     rmse, imputations \u001b[39m=\u001b[39m imputationRMSE(sess, Xtrain, Xnan, \u001b[39m10000\u001b[39m, mu, log_p_x_given_z, log_p_z, log_q_z_given_x,x_pl,s_pl,n_pl)\n",
      "Cell \u001b[0;32mIn[25], line 73\u001b[0m, in \u001b[0;36mnot_imputationRMSE\u001b[0;34m(sess, Xorg, Xnan, L, mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x, x_pl, s_pl, n_pl)\u001b[0m\n\u001b[1;32m     70\u001b[0m xz \u001b[39m=\u001b[39m Xz[i, :][\u001b[39mNone\u001b[39;00m, :]\n\u001b[1;32m     71\u001b[0m s \u001b[39m=\u001b[39m S[i, :][\u001b[39mNone\u001b[39;00m, :]\n\u001b[0;32m---> 73\u001b[0m _mu, wl, xm, xmix \u001b[39m=\u001b[39m imp(xz, s, L,mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x,x_pl,s_pl,n_pl)\n\u001b[1;32m     75\u001b[0m XM[i, :] \u001b[39m=\u001b[39m xm\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[25], line 55\u001b[0m, in \u001b[0;36mnot_imputationRMSE.<locals>.imp\u001b[0;34m(xz, s, L, mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x, x_pl, s_pl, n_pl)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimp\u001b[39m(xz, s, L,mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x,x_pl,s_pl,n_pl):                                      \u001b[39m# new item\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     _mu, _log_p_x_given_z, _log_p_z, _log_q_z_given_x, _log_p_s_given_x  \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     56\u001b[0m         [mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x],\n\u001b[1;32m     57\u001b[0m         {x_pl: xz, s_pl: s, n_pl: L})\n\u001b[1;32m     58\u001b[0m                                      \u001b[39m# new item\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     wl \u001b[39m=\u001b[39m softmax(_log_p_x_given_z \u001b[39m+\u001b[39m _log_p_s_given_x \u001b[39m+\u001b[39m _log_p_z \u001b[39m-\u001b[39m _log_q_z_given_x)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[39m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m final_fetches \u001b[39mor\u001b[39;00m final_targets \u001b[39mor\u001b[39;00m (handle \u001b[39mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_run(handle, final_targets, final_fetches,\n\u001b[1;32m   1192\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m   1372\u001b[0m                        run_metadata)\n\u001b[1;32m   1373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_call\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs):\n\u001b[1;32m   1377\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1379\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1380\u001b[0m     message \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(e\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1359\u001b[0m   \u001b[39m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1362\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_tf_sessionrun\u001b[39m(\u001b[39mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[39mreturn\u001b[39;00m tf_session\u001b[39m.\u001b[39;49mTF_SessionRun_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session, options, feed_dict,\n\u001b[1;32m   1455\u001b[0m                                           fetch_list, target_list,\n\u001b[1;32m   1456\u001b[0m                                           run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model(N,D,S,Xz,Xnan, type = \"non-MIWAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(N,D,S,Xz,Xnan, type = \"non-MIWAE\"):\n",
    "    print(\"Creating graph...\")\n",
    "    tf.reset_default_graph()\n",
    "    # ---- input\n",
    "    with tf.variable_scope('input'):\n",
    "        x_pl = tf.placeholder(tf.float32, [None, D], 'x_pl')\n",
    "        s_pl = tf.placeholder(tf.float32, [None, D], 's_pl')\n",
    "        n_pl = tf.placeholder(tf.int32, shape=(), name='n_pl')\n",
    "    # ---- parameters\n",
    "    with tf.variable_scope('data_process'):\n",
    "        logstd = tf.get_variable('logstd', shape=[])\n",
    "\n",
    "\n",
    "    # encoder\n",
    "    x = keras.layers.Dense(units=n_hidden, activation=tf.nn.tanh, name='l_enc1')(x_pl)\n",
    "    x = keras.layers.Dense(units=n_hidden, activation=tf.nn.tanh, name='l_enc2')(x)\n",
    "\n",
    "    q_mu = keras.layers.Dense(units=n_latent, activation=None, name='q_mu')(x)\n",
    "\n",
    "    q_logstd = keras.layers.Dense(units=n_latent, activation=lambda x: tf.clip_by_value(x, -10, 10),\n",
    "                            name='q_logstd')(x)\n",
    "\n",
    "\n",
    "    q_z = tfp.distributions.Normal(loc=q_mu, scale=tf.exp(q_logstd))\n",
    "\n",
    "    # ---- sample the latent value\n",
    "    l_z = q_z.sample(n_pl)                    # shape [n_samples, batch_size, dl]\n",
    "    l_z = tf.transpose(l_z, perm=[1, 0, 2])   # shape [batch_size, n_samples, dl]\n",
    "\n",
    "    # decoder\n",
    "    mu = keras.layers.Dense(units=D, activation=None, name='mu')(l_z)\n",
    "\n",
    "    # likelihood\n",
    "\n",
    "    p_x_given_z = tfp.distributions.Normal(loc=mu, scale=tf.exp(logstd))\n",
    "\n",
    "    l_out_mixed = mu * tf.expand_dims(1 - s_pl, axis=1) + tf.expand_dims(x_pl * s_pl, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    W = tf.get_variable('W', shape=[1, 1, D])\n",
    "    W = -tf.nn.softplus(W)\n",
    "    b = tf.get_variable('b', shape=[1, 1, D])\n",
    "\n",
    "    logits = W * (l_out_mixed - b)\n",
    "\n",
    "    p_s_given_x = tfp.distributions.Bernoulli(logits=logits)\n",
    "\n",
    "\n",
    "\n",
    "    # ---- evaluate the observed data in p(x|z)\n",
    "    log_p_x_given_z = tf.reduce_sum(tf.expand_dims(s_pl, axis=1) * \n",
    "                                    p_x_given_z.log_prob(tf.expand_dims(x_pl, axis=1)), axis=-1)  # sum over d-dimension\n",
    "\n",
    "    # --- evaluate the z-samples in q(z|x)\n",
    "    q_z2 = tfp.distributions.Normal(loc=tf.expand_dims(q_z.loc, axis=1), scale=tf.expand_dims(q_z.scale, axis=1))\n",
    "    log_q_z_given_x = tf.reduce_sum(q_z2.log_prob(l_z), axis=-1)\n",
    "\n",
    "    # ---- evaluate the z-samples in the prior p(z)\n",
    "    prior = tfp.distributions.Normal(loc=0.0, scale=1.0)\n",
    "    log_p_z = tf.reduce_sum(prior.log_prob(l_z), axis=-1)\n",
    "\n",
    "    # ---- evaluate the mask in p(s|x)\n",
    "    log_p_s_given_x = tf.reduce_sum(p_s_given_x.log_prob(tf.expand_dims(s_pl, axis=1)), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lpxz = log_p_x_given_z\n",
    "    lpz = log_p_z\n",
    "    lqzx = log_q_z_given_x\n",
    "    # the additional term for MNAR\n",
    "    lpsx = log_p_s_given_x\n",
    "\n",
    "    # ---- MIWAE\n",
    "    # ---- importance weights\n",
    "    l_w = lpxz + lpz - lqzx\n",
    "\n",
    "    # ---- sum over samples\n",
    "    log_sum_w = tf.reduce_logsumexp(l_w, axis=1)\n",
    "\n",
    "    # ---- average over samples\n",
    "    log_avg_weight = log_sum_w - tf.log(tf.cast(n_pl, tf.float32))\n",
    "\n",
    "    # ---- average over minibatch to get the average llh\n",
    "    MIWAE = tf.reduce_mean(log_avg_weight, axis=-1)\n",
    "\n",
    "\n",
    "    # ---- not-MIWAE\n",
    "    # ---- importance weights\n",
    "    l_w = lpxz + lpsx + lpz - lqzx\n",
    "\n",
    "    # ---- sum over samples\n",
    "    log_sum_w = tf.reduce_logsumexp(l_w, axis=1)\n",
    "\n",
    "    # ---- average over samples\n",
    "    log_avg_weight = log_sum_w - tf.log(tf.cast(n_pl, tf.float32))\n",
    "\n",
    "    # ---- average over minibatch to get the average llh\n",
    "    notMIWAE = tf.reduce_mean(log_avg_weight, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    # ---- training stuff\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    global_step = tf.Variable(initial_value=0, trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    if type == \"notMIWAE\":\n",
    "\n",
    "        loss = -notMIWAE\n",
    "    else:\n",
    "        loss = -MIWAE\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step, var_list=tvars)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "    batch_pointer = 0\n",
    "\n",
    "    start = time.time()\n",
    "    best = float(\"inf\")\n",
    "\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_batch = Xz[batch_pointer: batch_pointer + batch_size, :]\n",
    "        s_batch = S[batch_pointer: batch_pointer + batch_size, :]\n",
    "\n",
    "        _, _loss, _step = sess.run([train_op, loss, global_step], {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples})\n",
    "\n",
    "        batch_pointer += batch_size\n",
    "        \n",
    "        if batch_pointer > N - batch_size:\n",
    "            batch_pointer = 0\n",
    "\n",
    "            p = np.random.permutation(N)\n",
    "            Xz = Xz[p, :]\n",
    "            S = S[p, :]\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            took = time.time() - start\n",
    "            start = time.time()\n",
    "            \n",
    "            # --- change the following batch if you want a true validation set\n",
    "            x_batch = Xz  \n",
    "            s_batch = S\n",
    "            \n",
    "            val_loss, _step = sess.run([loss, global_step], {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples})\n",
    "\n",
    "            print(\"{0}/{1} updates, {2:.2f} s, {3:.2f} train_loss, {4:.2f} val_loss\".format(i, max_iter, took, _loss, val_loss))\n",
    "\n",
    "\n",
    "    # ---- S has been permuted during training, so just reinstantiate it\n",
    "    S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "    \n",
    "    if type ==\"non-MIWAE\":\n",
    "        rmse, imputations = not_imputationRMSE(sess, Xtrain, Xnan, 10000,mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x,x_pl,s_pl,n_pl)\n",
    "\n",
    "    else: \n",
    "        rmse, imputations = imputationRMSE(sess, Xtrain, Xnan, 10000, mu, log_p_x_given_z, log_p_z, log_q_z_given_x,x_pl,s_pl,n_pl)\n",
    "\n",
    "    print(\"imputation RMSE: \", rmse)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating graph...\")\n",
    "tf.reset_default_graph()\n",
    "# ---- input\n",
    "with tf.variable_scope('input'):\n",
    "    x_pl = tf.placeholder(tf.float32, [None, D], 'x_pl')\n",
    "    s_pl = tf.placeholder(tf.float32, [None, D], 's_pl')\n",
    "    n_pl = tf.placeholder(tf.int32, shape=(), name='n_pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the noise variance is learned as a shared parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- parameters\n",
    "with tf.variable_scope('data_process'):\n",
    "    logstd = tf.get_variable('logstd', shape=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder / inference network consists of two hidden layers with 128 units and tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = keras.layers.Dense(units=n_hidden, activation=tf.nn.tanh, name='l_enc1')(x_pl)\n",
    "x = keras.layers.Dense(units=n_hidden, activation=tf.nn.tanh, name='l_enc2')(x)\n",
    "\n",
    "q_mu = keras.layers.Dense(units=n_latent, activation=None, name='q_mu')(x)\n",
    "\n",
    "q_logstd = keras.layers.Dense(units=n_latent, activation=lambda x: tf.clip_by_value(x, -10, 10),\n",
    "                           name='q_logstd')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda x: tf.clip_by_value(x, -10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_z = tfp.distributions.Normal(loc=q_mu, scale=tf.exp(q_logstd))\n",
    "\n",
    "# ---- sample the latent value\n",
    "l_z = q_z.sample(n_pl)                    # shape [n_samples, batch_size, dl]\n",
    "l_z = tf.transpose(l_z, perm=[1, 0, 2])   # shape [batch_size, n_samples, dl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = keras.layers.Dense(units=D, activation=None, name='mu')(l_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation model / likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_x_given_z = tfp.distributions.Normal(loc=mu, scale=tf.exp(logstd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing model\n",
    "- first mix observed data and samples of missing data\n",
    "- feed through missing model\n",
    "- find likelihood of missing model parameters\n",
    "\n",
    "We have to expand the dimensions of x_pl and s_pl, since mu has size [batch, n_samples, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out_mixed = mu * tf.expand_dims(1 - s_pl, axis=1) + tf.expand_dims(x_pl * s_pl, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.get_variable('W', shape=[1, 1, D])\n",
    "W = -tf.nn.softplus(W)\n",
    "b = tf.get_variable('b', shape=[1, 1, D])\n",
    "\n",
    "logits = W * (l_out_mixed - b)\n",
    "\n",
    "p_s_given_x = tfp.distributions.Bernoulli(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- evaluate the observed data in p(x|z)\n",
    "log_p_x_given_z = tf.reduce_sum(tf.expand_dims(s_pl, axis=1) * \n",
    "                                p_x_given_z.log_prob(tf.expand_dims(x_pl, axis=1)), axis=-1)  # sum over d-dimension\n",
    "\n",
    "# --- evaluate the z-samples in q(z|x)\n",
    "q_z2 = tfp.distributions.Normal(loc=tf.expand_dims(q_z.loc, axis=1), scale=tf.expand_dims(q_z.scale, axis=1))\n",
    "log_q_z_given_x = tf.reduce_sum(q_z2.log_prob(l_z), axis=-1)\n",
    "\n",
    "# ---- evaluate the z-samples in the prior p(z)\n",
    "prior = tfp.distributions.Normal(loc=0.0, scale=1.0)\n",
    "log_p_z = tf.reduce_sum(prior.log_prob(l_z), axis=-1)\n",
    "\n",
    "# ---- evaluate the mask in p(s|x)\n",
    "log_p_s_given_x = tf.reduce_sum(p_s_given_x.log_prob(tf.expand_dims(s_pl, axis=1)), axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses for the MIWAE and not-MIWAE respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lpxz = log_p_x_given_z\n",
    "lpz = log_p_z\n",
    "lqzx = log_q_z_given_x\n",
    "# the additional term for MNAR\n",
    "lpsx = log_p_s_given_x\n",
    "\n",
    "# ---- MIWAE\n",
    "# ---- importance weights\n",
    "l_w = lpxz + lpz - lqzx\n",
    "\n",
    "# ---- sum over samples\n",
    "log_sum_w = tf.reduce_logsumexp(l_w, axis=1)\n",
    "\n",
    "# ---- average over samples\n",
    "log_avg_weight = log_sum_w - tf.log(tf.cast(n_pl, tf.float32))\n",
    "\n",
    "# ---- average over minibatch to get the average llh\n",
    "MIWAE = tf.reduce_mean(log_avg_weight, axis=-1)\n",
    "\n",
    "\n",
    "# ---- not-MIWAE\n",
    "# ---- importance weights\n",
    "l_w = lpxz + lpsx + lpz - lqzx\n",
    "\n",
    "# ---- sum over samples\n",
    "log_sum_w = tf.reduce_logsumexp(l_w, axis=1)\n",
    "\n",
    "# ---- average over samples\n",
    "log_avg_weight = log_sum_w - tf.log(tf.cast(n_pl, tf.float32))\n",
    "\n",
    "# ---- average over minibatch to get the average llh\n",
    "notMIWAE = tf.reduce_mean(log_avg_weight, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- training stuff\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "global_step = tf.Variable(initial_value=0, trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose wether you want to train the MIWAE or the notMIWAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 11:40:07.093137: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-03-29 11:40:07.108525: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "loss = -notMIWAE\n",
    "# loss = -MIWAE\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "train_op = optimizer.minimize(loss, global_step=global_step, var_list=tvars)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30000 updates, 0.24 s, nan train_loss, nan val_loss\n",
      "100/30000 updates, 0.16 s, nan train_loss, nan val_loss\n",
      "200/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "300/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "400/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "500/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "700/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "800/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "900/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "1000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "1100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "1200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "1300/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "1400/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "1500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "1600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "1700/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "1800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "1900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "2000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "2100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "2200/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "2300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "2400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "2500/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "2600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "2700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "2800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "2900/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "3000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "3100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "3200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "3300/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "3400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "3500/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "3600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "3700/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "3800/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "3900/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "4000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "4100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "4200/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "4300/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "4400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "4500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "4600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "4700/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "4800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "4900/30000 updates, 0.17 s, nan train_loss, nan val_loss\n",
      "5000/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "5100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "5200/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "5300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "5400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "5500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "5600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "5700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "5800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "5900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "6000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "6100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "6200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "6300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "6400/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "6500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "6600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "6700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "6800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "6900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "7000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "7100/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "7200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "7300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "7400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "7500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "7600/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "7700/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "7800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "7900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "8000/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "8100/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "8200/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "8300/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "8400/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "8500/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "8600/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "8700/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "8800/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "8900/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "9000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "9100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "9200/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "9300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "9400/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "9500/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "9600/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "9700/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "9800/30000 updates, 0.15 s, nan train_loss, nan val_loss\n",
      "9900/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "10000/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "10100/30000 updates, 0.15 s, nan train_loss, nan val_loss\n",
      "10200/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "10300/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "10400/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "10500/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "10600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "10700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "10800/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "10900/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "11000/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "11100/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "11200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "11300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "11400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "11500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "11600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "11700/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "11800/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "11900/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "12000/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "12100/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "12200/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "12300/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "12400/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "12500/30000 updates, 0.15 s, nan train_loss, nan val_loss\n",
      "12600/30000 updates, 0.15 s, nan train_loss, nan val_loss\n",
      "12700/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "12800/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "12900/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "13000/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "13100/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "13200/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "13300/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "13400/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "13500/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "13600/30000 updates, 0.19 s, nan train_loss, nan val_loss\n",
      "13700/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "13800/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "13900/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "14000/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "14100/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "14200/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "14300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "14400/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "14500/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "14600/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "14700/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "14800/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "14900/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "15000/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "15100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "15200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "15300/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "15400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "15500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "15600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "15700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "15800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "15900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "16000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "16100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "16200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "16300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "16400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "16500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "16600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "16700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "16800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "16900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "17000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "17100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "17200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "17300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "17400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "17500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "17600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "17700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "17800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "17900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "18000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "18100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "18200/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "18300/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "18400/30000 updates, 0.42 s, nan train_loss, nan val_loss\n",
      "18500/30000 updates, 0.17 s, nan train_loss, nan val_loss\n",
      "18600/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "18700/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "18800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "18900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "19000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "19100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "19200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "19300/30000 updates, 0.18 s, nan train_loss, nan val_loss\n",
      "19400/30000 updates, 0.21 s, nan train_loss, nan val_loss\n",
      "19500/30000 updates, 0.21 s, nan train_loss, nan val_loss\n",
      "19600/30000 updates, 0.16 s, nan train_loss, nan val_loss\n",
      "19700/30000 updates, 0.16 s, nan train_loss, nan val_loss\n",
      "19800/30000 updates, 0.17 s, nan train_loss, nan val_loss\n",
      "19900/30000 updates, 0.17 s, nan train_loss, nan val_loss\n",
      "20000/30000 updates, 0.17 s, nan train_loss, nan val_loss\n",
      "20100/30000 updates, 0.15 s, nan train_loss, nan val_loss\n",
      "20200/30000 updates, 0.16 s, nan train_loss, nan val_loss\n",
      "20300/30000 updates, 0.16 s, nan train_loss, nan val_loss\n",
      "20400/30000 updates, 0.16 s, nan train_loss, nan val_loss\n",
      "20500/30000 updates, 0.15 s, nan train_loss, nan val_loss\n",
      "20600/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "20700/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "20800/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "20900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "21000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "21100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "21200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "21300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "21400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "21500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "21600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "21700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "21800/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "21900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "22000/30000 updates, 0.10 s, nan train_loss, nan val_loss\n",
      "22100/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "22200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "22300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "22400/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "22500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "22600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "22700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "22800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "22900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "23000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "23100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "23200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "23300/30000 updates, 52.79 s, nan train_loss, nan val_loss\n",
      "23400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "23500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "23600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "23700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "23800/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "23900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "24000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "24100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "24200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "24300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "24400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "24500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "24600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "24700/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "24800/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "24900/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "25000/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "25100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "25200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "25300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "25400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "25500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "25600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "25700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "25800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "25900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "26000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "26100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "26200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "26300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "26400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "26500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "26600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "26700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "26800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "26900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "27000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "27100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "27200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "27300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "27400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "27500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "27600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "27700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "27800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "27900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "28000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "28100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "28200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "28300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "28400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "28500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "28600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "28700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "28800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "28900/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "29000/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "29100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "29200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "29300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "29400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "29500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "29600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "29700/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "29800/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "29900/30000 updates, 0.10 s, nan train_loss, nan val_loss\n"
     ]
    }
   ],
   "source": [
    "batch_pointer = 0\n",
    "\n",
    "start = time.time()\n",
    "best = float(\"inf\")\n",
    "\n",
    "\n",
    "for i in range(max_iter):\n",
    "    x_batch = Xz[batch_pointer: batch_pointer + batch_size, :]\n",
    "    s_batch = S[batch_pointer: batch_pointer + batch_size, :]\n",
    "\n",
    "    _, _loss, _step = sess.run([train_op, loss, global_step], {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples})\n",
    "\n",
    "    batch_pointer += batch_size\n",
    "    \n",
    "    if batch_pointer > N - batch_size:\n",
    "        batch_pointer = 0\n",
    "\n",
    "        p = np.random.permutation(N)\n",
    "        Xz = Xz[p, :]\n",
    "        S = S[p, :]\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        took = time.time() - start\n",
    "        start = time.time()\n",
    "        \n",
    "        # --- change the following batch if you want a true validation set\n",
    "        x_batch = Xz  \n",
    "        s_batch = S\n",
    "        \n",
    "        val_loss, _step = sess.run([loss, global_step], {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples})\n",
    "\n",
    "        print(\"{0}/{1} updates, {2:.2f} s, {3:.2f} train_loss, {4:.2f} val_loss\".format(i, max_iter, took, _loss, val_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single imputation RMSE\n",
    "The *self-normalized importance sampling* approach for the MIWAE is described in this [paper](https://arxiv.org/pdf/1812.02633.pdf). This needs to be modified slightly in the MNAR case to account for the missing model, as described in the not-MIWAE paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imputationRMSE(sess, Xorg, Xnan, L, mu, log_p_x_given_z, log_p_z, log_q_z_given_x,x_pl,s_pl,n_pl):\n",
    "\n",
    "    N = len(Xorg)\n",
    "    \n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan)] = 0\n",
    "    S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1)[:, None])\n",
    "        return e_x / e_x.sum(axis=1)[:, None]\n",
    "\n",
    "    def imp(xz, s, L,mu, log_p_x_given_z, log_p_z, log_q_z_given_x,x_pl,s_pl,n_pl):\n",
    "        _mu, _log_p_x_given_z, _log_p_z, _log_q_z_given_x = sess.run(\n",
    "            [mu, log_p_x_given_z, log_p_z, log_q_z_given_x],\n",
    "            {x_pl: xz, s_pl: s, n_pl: L})\n",
    "\n",
    "        wl = softmax(_log_p_x_given_z + _log_p_z - _log_q_z_given_x)\n",
    "\n",
    "        xm = np.sum((_mu.T * wl.T).T, axis=1)\n",
    "        xmix = xz + xm * (1 - s)\n",
    "\n",
    "        return _mu, wl, xm, xmix\n",
    "\n",
    "    XM = np.zeros_like(Xorg)\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        xz = Xz[i, :][None, :]\n",
    "        s = S[i, :][None, :]\n",
    "\n",
    "        _mu, wl, xm, xmix = imp(xz, s, L,mu, log_p_x_given_z, log_p_z, log_q_z_given_x,x_pl,s_pl,n_pl)\n",
    "\n",
    "        XM[i, :] = xm\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('{0} / {1}'.format(i, N))\n",
    "\n",
    "    return np.sqrt(np.sum((Xorg - XM) ** 2 * (1 - S)) / np.sum(1 - S)), XM\n",
    "\n",
    "\n",
    "def not_imputationRMSE(sess, Xorg, Xnan, L,mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x,x_pl,s_pl,n_pl):\n",
    "\n",
    "    N = len(Xorg)\n",
    "    \n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan)] = 0\n",
    "    S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1)[:, None])\n",
    "        return e_x / e_x.sum(axis=1)[:, None]\n",
    "\n",
    "    def imp(xz, s, L,mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x,x_pl,s_pl,n_pl):                                      # new item\n",
    "        _mu, _log_p_x_given_z, _log_p_z, _log_q_z_given_x, _log_p_s_given_x  = sess.run(\n",
    "            [mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x],\n",
    "            {x_pl: xz, s_pl: s, n_pl: L})\n",
    "                                         # new item\n",
    "        wl = softmax(_log_p_x_given_z + _log_p_s_given_x + _log_p_z - _log_q_z_given_x)\n",
    "\n",
    "        xm = np.sum((_mu.T * wl.T).T, axis=1)\n",
    "        xmix = xz + xm * (1 - s)\n",
    "\n",
    "        return _mu, wl, xm, xmix\n",
    "\n",
    "    XM = np.zeros_like(Xorg)\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        xz = Xz[i, :][None, :]\n",
    "        s = S[i, :][None, :]\n",
    "\n",
    "        _mu, wl, xm, xmix = imp(xz, s, L,mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x,x_pl,s_pl,n_pl)\n",
    "\n",
    "        XM[i, :] = xm\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('{0} / {1}'.format(i, N))\n",
    "\n",
    "    return np.sqrt(np.sum((Xorg - XM) ** 2 * (1 - S)) / np.sum(1 - S)), XM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the single imputation RMSE using 10k importance samples\n",
    "If you used the MIWAE loss use the imputationRMSE \n",
    "\n",
    "If you used the notMIWAE loss use the not_imputationRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4898\n",
      "100 / 4898\n",
      "200 / 4898\n",
      "300 / 4898\n",
      "400 / 4898\n",
      "500 / 4898\n",
      "600 / 4898\n",
      "700 / 4898\n",
      "800 / 4898\n",
      "900 / 4898\n",
      "1000 / 4898\n",
      "1100 / 4898\n",
      "1200 / 4898\n",
      "1300 / 4898\n",
      "1400 / 4898\n",
      "1500 / 4898\n",
      "1600 / 4898\n",
      "1700 / 4898\n",
      "1800 / 4898\n",
      "1900 / 4898\n",
      "2000 / 4898\n",
      "2100 / 4898\n",
      "2200 / 4898\n",
      "2300 / 4898\n",
      "2400 / 4898\n",
      "2500 / 4898\n",
      "2600 / 4898\n",
      "2700 / 4898\n",
      "2800 / 4898\n",
      "2900 / 4898\n",
      "3000 / 4898\n",
      "3100 / 4898\n",
      "3200 / 4898\n",
      "3300 / 4898\n",
      "3400 / 4898\n",
      "3500 / 4898\n",
      "3600 / 4898\n",
      "3700 / 4898\n",
      "3800 / 4898\n",
      "3900 / 4898\n",
      "4000 / 4898\n",
      "4100 / 4898\n",
      "4200 / 4898\n",
      "4300 / 4898\n",
      "4400 / 4898\n",
      "4500 / 4898\n",
      "4600 / 4898\n",
      "4700 / 4898\n",
      "4800 / 4898\n",
      "imputation RMSE:  nan\n"
     ]
    }
   ],
   "source": [
    "# ---- S has been permuted during training, so just reinstantiate it\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "\n",
    "rmse, imputations = not_imputationRMSE(sess, Xtrain, Xnan, 10000)\n",
    "# rmse, imputations = imputationRMSE(sess, Xtrain, Xnan, 10000)\n",
    "\n",
    "print(\"imputation RMSE: \", rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat the same steps for MIWAE\n",
    "### Choose wether you want to train the MIWAE or the notMIWAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 11:42:33.790233: W tensorflow/c/c_api.cc:291] Operation '{name:'b/Adam_1/Assign' id:1061 op device:{requested: '', assigned: ''} def:{{{node b/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=[\"loc:@b\"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](b/Adam_1, b/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "loss = -MIWAE\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "train_op = optimizer.minimize(loss, global_step=global_step, var_list=tvars)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30000 updates, 0.17 s, nan train_loss, nan val_loss\n",
      "100/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "300/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "400/30000 updates, 0.31 s, nan train_loss, nan val_loss\n",
      "500/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "600/30000 updates, 0.15 s, nan train_loss, nan val_loss\n",
      "700/30000 updates, 0.16 s, nan train_loss, nan val_loss\n",
      "800/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "900/30000 updates, 0.10 s, nan train_loss, nan val_loss\n",
      "1000/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "1100/30000 updates, 0.29 s, nan train_loss, nan val_loss\n",
      "1200/30000 updates, 0.15 s, nan train_loss, nan val_loss\n",
      "1300/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "1400/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "1500/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "1600/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "1700/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "1800/30000 updates, 0.14 s, nan train_loss, nan val_loss\n",
      "1900/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "2000/30000 updates, 0.12 s, nan train_loss, nan val_loss\n",
      "2100/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "2200/30000 updates, 0.11 s, nan train_loss, nan val_loss\n",
      "2300/30000 updates, 0.10 s, nan train_loss, nan val_loss\n",
      "2400/30000 updates, 0.10 s, nan train_loss, nan val_loss\n",
      "2500/30000 updates, 0.10 s, nan train_loss, nan val_loss\n",
      "2600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "2700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "2800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "2900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "3000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "3100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "3200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "3300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "3400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "3500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "3600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "3700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "3800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "3900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "4000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "4100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "4200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "4300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "4400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "4500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "4600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "4700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "4800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "4900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "5000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "5100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "5200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "5300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "5400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "5500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "5600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "5700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "5800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "5900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "6000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "6100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "6200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "6300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "6400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "6500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "6600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "6700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "6800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "6900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "7000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "7100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "7200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "7300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "7400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "7500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "7600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "7700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "7800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "7900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "8000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "8100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "8200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "8300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "8400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "8500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "8600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "8700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "8800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "8900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "9000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "9100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "9200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "9300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "9400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "9500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "9600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "9700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "9800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "9900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "10000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "10100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "10200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "10300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "10400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "10500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "10600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "10700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "10800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "10900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "11000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "11100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "11200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "11300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "11400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "11500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "11600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "11700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "11800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "11900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "12000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "12100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "12200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "12300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "12400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "12500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "12600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "12700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "12800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "12900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "13000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "13100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "13200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "13300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "13400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "13500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "13600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "13700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "13800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "13900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "14000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "14100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "14200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "14300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "14400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "14500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "14600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "14700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "14800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "14900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "15000/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "15100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "15200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "15300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "15400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "15500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "15600/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "15700/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "15800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "15900/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "16000/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "16100/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "16200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "16300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "16400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "16500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "16600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "16700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "16800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "16900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "17000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "17100/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "17200/30000 updates, 0.16 s, nan train_loss, nan val_loss\n",
      "17300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "17400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "17500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "17600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "17700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "17800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "17900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "18000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "18100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "18200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "18300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "18400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "18500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "18600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "18700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "18800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "18900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "19000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "19100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "19200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "19300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "19400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "19500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "19600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "19700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "19800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "19900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "20000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "20100/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "20200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "20300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "20400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "20500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "20600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "20700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "20800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "20900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "21000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "21100/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "21200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "21300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "21400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "21500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "21600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "21700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "21800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "21900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "22000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "22100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "22200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "22300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "22400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "22500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "22600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "22700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "22800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "22900/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "23000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "23100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "23200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "23300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "23400/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "23500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "23600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "23700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "23800/30000 updates, 0.13 s, nan train_loss, nan val_loss\n",
      "23900/30000 updates, 0.10 s, nan train_loss, nan val_loss\n",
      "24000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "24100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "24200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "24300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "24400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "24500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "24600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "24700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "24800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "24900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "25000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "25100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "25200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "25300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "25400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "25500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "25600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "25700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "25800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "25900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "26000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "26100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "26200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "26300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "26400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "26500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "26600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "26700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "26800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "26900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "27000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "27100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "27200/30000 updates, 0.08 s, nan train_loss, nan val_loss\n",
      "27300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "27400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "27500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "27600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "27700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "27800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "27900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "28000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "28100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "28200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "28300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "28400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "28500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "28600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "28700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "28800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "28900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "29000/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "29100/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "29200/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "29300/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "29400/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "29500/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "29600/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "29700/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "29800/30000 updates, 0.09 s, nan train_loss, nan val_loss\n",
      "29900/30000 updates, 0.09 s, nan train_loss, nan val_loss\n"
     ]
    }
   ],
   "source": [
    "batch_pointer = 0\n",
    "\n",
    "start = time.time()\n",
    "best = float(\"inf\")\n",
    "\n",
    "\n",
    "for i in range(max_iter):\n",
    "    x_batch = Xz[batch_pointer: batch_pointer + batch_size, :]\n",
    "    s_batch = S[batch_pointer: batch_pointer + batch_size, :]\n",
    "\n",
    "    _, _loss, _step = sess.run([train_op, loss, global_step], {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples})\n",
    "\n",
    "    batch_pointer += batch_size\n",
    "    \n",
    "    if batch_pointer > N - batch_size:\n",
    "        batch_pointer = 0\n",
    "\n",
    "        p = np.random.permutation(N)\n",
    "        Xz = Xz[p, :]\n",
    "        S = S[p, :]\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        took = time.time() - start\n",
    "        start = time.time()\n",
    "        \n",
    "        # --- change the following batch if you want a true validation set\n",
    "        x_batch = Xz  \n",
    "        s_batch = S\n",
    "        \n",
    "        val_loss, _step = sess.run([loss, global_step], {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples})\n",
    "\n",
    "        print(\"{0}/{1} updates, {2:.2f} s, {3:.2f} train_loss, {4:.2f} val_loss\".format(i, max_iter, took, _loss, val_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the single imputation RMSE using 10k importance samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4898\n",
      "100 / 4898\n",
      "200 / 4898\n",
      "300 / 4898\n",
      "400 / 4898\n",
      "500 / 4898\n",
      "600 / 4898\n",
      "700 / 4898\n",
      "800 / 4898\n",
      "900 / 4898\n",
      "1000 / 4898\n",
      "1100 / 4898\n",
      "1200 / 4898\n",
      "1300 / 4898\n",
      "1400 / 4898\n",
      "1500 / 4898\n",
      "1600 / 4898\n",
      "1700 / 4898\n",
      "1800 / 4898\n",
      "1900 / 4898\n",
      "2000 / 4898\n",
      "2100 / 4898\n",
      "2200 / 4898\n",
      "2300 / 4898\n",
      "2400 / 4898\n",
      "2500 / 4898\n",
      "2600 / 4898\n",
      "2700 / 4898\n",
      "2800 / 4898\n",
      "2900 / 4898\n",
      "3000 / 4898\n",
      "3100 / 4898\n",
      "3200 / 4898\n",
      "3300 / 4898\n",
      "3400 / 4898\n",
      "3500 / 4898\n",
      "3600 / 4898\n",
      "3700 / 4898\n",
      "3800 / 4898\n",
      "3900 / 4898\n",
      "4000 / 4898\n",
      "4100 / 4898\n",
      "4200 / 4898\n",
      "4300 / 4898\n",
      "4400 / 4898\n",
      "4500 / 4898\n",
      "4600 / 4898\n",
      "4700 / 4898\n",
      "4800 / 4898\n",
      "imputation RMSE:  nan\n"
     ]
    }
   ],
   "source": [
    "# ---- S has been permuted during training, so just reinstantiate it\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "\n",
    "#rmse, imputations = not_imputationRMSE(sess, Xtrain, Xnan, 10000)\n",
    "rmse, imputations = imputationRMSE(sess, Xtrain, Xnan, 10000)\n",
    "\n",
    "print(\"imputation RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to missForest and MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "estimator = RandomForestRegressor(n_estimators=100)\n",
    "imp = IterativeImputer(estimator=estimator)\n",
    "imp.fit(Xnan)\n",
    "Xrec = imp.transform(Xnan)\n",
    "rmse_mf = np.sqrt(np.sum((Xtrain - Xrec) ** 2 * (1 - S)) / np.sum(1 - S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missForst imputation RMSE:  1.6266977288476672\n"
     ]
    }
   ],
   "source": [
    "print(\"missForst imputation RMSE: \", rmse_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = IterativeImputer(max_iter=100)\n",
    "imp.fit(Xnan)\n",
    "Xrec = imp.transform(Xnan)\n",
    "RMSE_iter = np.sqrt(np.sum((Xtrain - Xrec) ** 2 * (1 - S)) / np.sum(1 - S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICE, imputation RMSE 1.410276372331692\n"
     ]
    }
   ],
   "source": [
    "print(\"MICE, imputation RMSE\", RMSE_iter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAR missing data imputation performance:\n",
    "\n",
    "* Non - MIWAE RMSE: 1.04\n",
    "* MIWAE RMSE: 1.38\n",
    "* missForest RMSE: 1.62\n",
    "* MICE RMSE: 1.41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the learned missing model\n",
    "There is a separate missing process in each feature dimesion, inspect each of them, plot as function of feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_W, _b = sess.run([W, b])\n",
    "x = np.linspace(-2, 2, 100)\n",
    "\n",
    "def sigmoid(logits):\n",
    "    return 1 / (1 + np.exp(-logits))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for d in range(D // 2):\n",
    "    logits = _W[0][0][d] * (x - _b[0][0][d])\n",
    "    psx = sigmoid(logits)\n",
    "    plt.plot(x, psx)\n",
    "    \n",
    "plt.xlabel('feature value')\n",
    "plt.ylabel('$p(s|x)$')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should illustrate that the probability of the feature value being observed when it is below the feature mean should be close to 1, while the probability of being observed above the feature mean should be close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
