2023-04-30 16:19:17.218801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-30 16:19:20.443526: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/python/3.9/anaconda/lib
2023-04-30 16:19:20.443566: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-04-30 16:19:56.672942: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/python/3.9/anaconda/lib
2023-04-30 16:19:56.673534: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/python/3.9/anaconda/lib
2023-04-30 16:19:56.673552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
WARNING:tensorflow:From /home/s223039521/.local/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
/home/s223039521/MNAR/MNAR/run.py:88: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  S = np.array(~np.isnan(Xnan), dtype=np.float)
2023-04-30 16:21:29.629627: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-30 16:21:30.740061: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-04-30 16:21:30.740119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: boromir
2023-04-30 16:21:30.740126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: boromir
2023-04-30 16:21:30.740343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program
2023-04-30 16:21:30.740385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.161.3
2023-04-30 16:21:31.163217: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
OTlogistic
OTlogistic
Creating graph...
Saving session...
Saving session...
0/100000 updates, 0.54 s, 3.24 train_loss, 2.94 val_loss
Saving session...
100/100000 updates, 0.57 s, 2.62 train_loss, 2.55 val_loss
200/100000 updates, 0.43 s, 2.65 train_loss, 2.58 val_loss
Saving session...
300/100000 updates, 0.36 s, 2.09 train_loss, 2.54 val_loss
Saving session...
400/100000 updates, 0.42 s, 2.27 train_loss, 2.52 val_loss
500/100000 updates, 0.42 s, 2.57 train_loss, 2.55 val_loss
Saving session...
600/100000 updates, 0.36 s, 2.16 train_loss, 2.50 val_loss
700/100000 updates, 0.41 s, 2.36 train_loss, 2.51 val_loss
800/100000 updates, 0.35 s, 2.43 train_loss, 2.54 val_loss
900/100000 updates, 0.36 s, 2.39 train_loss, 2.52 val_loss
1000/100000 updates, 0.35 s, 2.34 train_loss, 2.51 val_loss
1100/100000 updates, 0.35 s, 2.47 train_loss, 2.52 val_loss
Saving session...
1200/100000 updates, 0.35 s, 2.62 train_loss, 2.47 val_loss
1300/100000 updates, 0.42 s, 2.19 train_loss, 2.49 val_loss
1400/100000 updates, 0.36 s, 2.42 train_loss, 2.48 val_loss
1500/100000 updates, 0.35 s, 2.56 train_loss, 2.49 val_loss
Saving session...
1600/100000 updates, 0.35 s, 2.80 train_loss, 2.46 val_loss
1700/100000 updates, 0.42 s, 2.65 train_loss, 2.47 val_loss
Saving session...
1800/100000 updates, 0.35 s, 2.30 train_loss, 2.43 val_loss
1900/100000 updates, 0.42 s, 2.16 train_loss, 2.48 val_loss
2000/100000 updates, 0.36 s, 2.18 train_loss, 2.45 val_loss
2100/100000 updates, 0.35 s, 2.33 train_loss, 2.47 val_loss
2200/100000 updates, 0.35 s, 2.34 train_loss, 2.45 val_loss
2300/100000 updates, 0.36 s, 2.25 train_loss, 2.44 val_loss
2400/100000 updates, 0.35 s, 1.88 train_loss, 2.45 val_loss
2500/100000 updates, 0.36 s, 1.92 train_loss, 2.45 val_loss
2600/100000 updates, 0.35 s, 2.63 train_loss, 2.44 val_loss
2700/100000 updates, 0.36 s, 2.35 train_loss, 2.44 val_loss
2800/100000 updates, 0.35 s, 2.57 train_loss, 2.45 val_loss
Saving session...
2900/100000 updates, 0.36 s, 2.16 train_loss, 2.41 val_loss
3000/100000 updates, 0.42 s, 2.46 train_loss, 2.43 val_loss
3100/100000 updates, 0.36 s, 2.34 train_loss, 2.42 val_loss
Saving session...
3200/100000 updates, 0.35 s, 2.67 train_loss, 2.41 val_loss
3300/100000 updates, 0.42 s, 1.65 train_loss, 2.43 val_loss
Saving session...
3400/100000 updates, 0.35 s, 2.32 train_loss, 2.39 val_loss
3500/100000 updates, 0.42 s, 2.00 train_loss, 2.42 val_loss
3600/100000 updates, 0.36 s, 2.44 train_loss, 2.41 val_loss
Saving session...
3700/100000 updates, 0.35 s, 2.35 train_loss, 2.39 val_loss
3800/100000 updates, 0.42 s, 2.17 train_loss, 2.41 val_loss
3900/100000 updates, 0.36 s, 2.39 train_loss, 2.42 val_loss
4000/100000 updates, 0.36 s, 2.05 train_loss, 2.42 val_loss
4100/100000 updates, 0.35 s, 3.11 train_loss, 2.40 val_loss
Saving session...
4200/100000 updates, 0.35 s, 2.39 train_loss, 2.39 val_loss
Saving session...
4300/100000 updates, 0.42 s, 2.23 train_loss, 2.38 val_loss
4400/100000 updates, 0.41 s, 3.08 train_loss, 2.40 val_loss
4500/100000 updates, 0.35 s, 2.14 train_loss, 2.41 val_loss
4600/100000 updates, 0.35 s, 2.79 train_loss, 2.41 val_loss
4700/100000 updates, 0.36 s, 2.25 train_loss, 2.40 val_loss
4800/100000 updates, 0.36 s, 1.99 train_loss, 2.38 val_loss
4900/100000 updates, 0.35 s, 2.69 train_loss, 2.41 val_loss
5000/100000 updates, 0.35 s, 2.23 train_loss, 2.39 val_loss
5100/100000 updates, 0.36 s, 2.52 train_loss, 2.41 val_loss
5200/100000 updates, 0.35 s, 2.51 train_loss, 2.39 val_loss
5300/100000 updates, 0.35 s, 2.64 train_loss, 2.42 val_loss
5400/100000 updates, 0.36 s, 2.36 train_loss, 2.39 val_loss
5500/100000 updates, 0.36 s, 2.51 train_loss, 2.38 val_loss
Saving session...
5600/100000 updates, 0.35 s, 2.25 train_loss, 2.38 val_loss
5700/100000 updates, 0.42 s, 2.52 train_loss, 2.39 val_loss
5800/100000 updates, 0.35 s, 2.18 train_loss, 2.39 val_loss
Saving session...
5900/100000 updates, 0.36 s, 2.03 train_loss, 2.38 val_loss
6000/100000 updates, 0.42 s, 2.94 train_loss, 2.45 val_loss
6100/100000 updates, 0.36 s, 1.80 train_loss, 2.39 val_loss
6200/100000 updates, 0.35 s, 1.93 train_loss, 2.40 val_loss
6300/100000 updates, 0.35 s, 2.30 train_loss, 2.39 val_loss
6400/100000 updates, 0.36 s, 2.72 train_loss, 2.39 val_loss
Saving session...
6500/100000 updates, 0.36 s, 2.39 train_loss, 2.37 val_loss
Saving session...
6600/100000 updates, 0.41 s, 2.32 train_loss, 2.36 val_loss
6700/100000 updates, 0.41 s, 2.09 train_loss, 2.41 val_loss
6800/100000 updates, 0.36 s, 2.14 train_loss, 2.39 val_loss
6900/100000 updates, 0.36 s, 2.45 train_loss, 2.39 val_loss
7000/100000 updates, 0.35 s, 2.18 train_loss, 2.38 val_loss
7100/100000 updates, 0.36 s, 2.80 train_loss, 2.39 val_loss
7200/100000 updates, 0.36 s, 2.21 train_loss, 2.39 val_loss
7300/100000 updates, 0.35 s, 2.22 train_loss, 2.42 val_loss
7400/100000 updates, 0.35 s, 2.41 train_loss, 2.37 val_loss
7500/100000 updates, 0.35 s, 2.33 train_loss, 2.42 val_loss
7600/100000 updates, 0.35 s, 2.33 train_loss, 2.38 val_loss
7700/100000 updates, 0.34 s, 2.16 train_loss, 2.38 val_loss
7800/100000 updates, 0.35 s, 2.07 train_loss, 2.41 val_loss
7900/100000 updates, 0.36 s, 2.44 train_loss, 2.39 val_loss
Saving session...
8000/100000 updates, 0.35 s, 2.61 train_loss, 2.36 val_loss
8100/100000 updates, 0.42 s, 2.88 train_loss, 2.40 val_loss
8200/100000 updates, 0.36 s, 2.31 train_loss, 2.42 val_loss
8300/100000 updates, 0.39 s, 2.21 train_loss, 2.37 val_loss
8400/100000 updates, 0.53 s, 2.26 train_loss, 2.37 val_loss
8500/100000 updates, 0.38 s, 2.62 train_loss, 2.41 val_loss
8600/100000 updates, 0.36 s, 2.05 train_loss, 2.39 val_loss
8700/100000 updates, 0.36 s, 2.44 train_loss, 2.36 val_loss
8800/100000 updates, 0.39 s, 1.93 train_loss, 2.38 val_loss
8900/100000 updates, 0.41 s, 2.11 train_loss, 2.36 val_loss
9000/100000 updates, 0.38 s, 2.54 train_loss, 2.41 val_loss
9100/100000 updates, 0.36 s, 2.47 train_loss, 2.38 val_loss
9200/100000 updates, 0.37 s, 1.85 train_loss, 2.38 val_loss
9300/100000 updates, 0.36 s, 2.45 train_loss, 2.38 val_loss
9400/100000 updates, 0.36 s, 2.18 train_loss, 2.38 val_loss
9500/100000 updates, 0.36 s, 2.11 train_loss, 2.36 val_loss
9600/100000 updates, 0.36 s, 2.23 train_loss, 2.37 val_loss
9700/100000 updates, 0.36 s, 2.41 train_loss, 2.36 val_loss
9800/100000 updates, 0.38 s, 1.54 train_loss, 2.36 val_loss
9900/100000 updates, 0.40 s, 2.95 train_loss, 2.36 val_loss
Saving session...
10000/100000 updates, 0.36 s, 1.90 train_loss, 2.34 val_loss
10100/100000 updates, 0.43 s, 1.62 train_loss, 2.38 val_loss
10200/100000 updates, 0.39 s, 2.30 train_loss, 2.36 val_loss
10300/100000 updates, 0.36 s, 2.17 train_loss, 2.36 val_loss
10400/100000 updates, 0.38 s, 2.52 train_loss, 2.37 val_loss
10500/100000 updates, 0.36 s, 2.82 train_loss, 2.38 val_loss
10600/100000 updates, 0.37 s, 2.42 train_loss, 2.36 val_loss
10700/100000 updates, 0.40 s, 1.93 train_loss, 2.38 val_loss
10800/100000 updates, 0.36 s, 2.49 train_loss, 2.38 val_loss
Saving session...
10900/100000 updates, 0.36 s, 2.52 train_loss, 2.33 val_loss
11000/100000 updates, 0.43 s, 2.59 train_loss, 2.35 val_loss
11100/100000 updates, 0.38 s, 2.66 train_loss, 2.34 val_loss
11200/100000 updates, 0.36 s, 2.54 train_loss, 2.36 val_loss
11300/100000 updates, 0.36 s, 2.46 train_loss, 2.35 val_loss
11400/100000 updates, 0.46 s, 2.31 train_loss, 2.36 val_loss
11500/100000 updates, 0.37 s, 2.24 train_loss, 2.37 val_loss
11600/100000 updates, 0.36 s, 2.67 train_loss, 2.36 val_loss
11700/100000 updates, 0.36 s, 2.40 train_loss, 2.35 val_loss
11800/100000 updates, 0.38 s, 2.48 train_loss, 2.37 val_loss
11900/100000 updates, 0.36 s, 1.48 train_loss, 2.35 val_loss
12000/100000 updates, 0.36 s, 2.65 train_loss, 2.35 val_loss
12100/100000 updates, 0.36 s, 2.22 train_loss, 2.36 val_loss
12200/100000 updates, 0.38 s, 2.24 train_loss, 2.36 val_loss
Saving session...
12300/100000 updates, 0.35 s, 2.37 train_loss, 2.33 val_loss
12400/100000 updates, 0.42 s, 2.10 train_loss, 2.34 val_loss
12500/100000 updates, 0.36 s, 2.28 train_loss, 2.34 val_loss
12600/100000 updates, 0.36 s, 2.34 train_loss, 2.35 val_loss
12700/100000 updates, 0.47 s, 2.13 train_loss, 2.35 val_loss
12800/100000 updates, 0.37 s, 2.57 train_loss, 2.35 val_loss
12900/100000 updates, 0.38 s, 2.08 train_loss, 2.36 val_loss
13000/100000 updates, 0.36 s, 2.61 train_loss, 2.35 val_loss
13100/100000 updates, 0.36 s, 2.55 train_loss, 2.34 val_loss
Saving session...
13200/100000 updates, 0.39 s, 1.54 train_loss, 2.33 val_loss
13300/100000 updates, 0.43 s, 2.35 train_loss, 2.35 val_loss
13400/100000 updates, 0.40 s, 2.32 train_loss, 2.42 val_loss
13500/100000 updates, 0.37 s, 2.46 train_loss, 2.36 val_loss
13600/100000 updates, 0.38 s, 2.08 train_loss, 2.35 val_loss
13700/100000 updates, 0.36 s, 2.38 train_loss, 2.36 val_loss
13800/100000 updates, 0.39 s, 2.58 train_loss, 2.35 val_loss
13900/100000 updates, 0.38 s, 2.19 train_loss, 2.33 val_loss
14000/100000 updates, 0.36 s, 2.28 train_loss, 2.34 val_loss
14100/100000 updates, 0.36 s, 2.61 train_loss, 2.34 val_loss
14200/100000 updates, 0.41 s, 2.90 train_loss, 2.39 val_loss
14300/100000 updates, 0.55 s, 2.08 train_loss, 2.35 val_loss
14400/100000 updates, 0.39 s, 2.37 train_loss, 2.35 val_loss
14500/100000 updates, 0.39 s, 2.23 train_loss, 2.37 val_loss
14600/100000 updates, 0.39 s, 2.04 train_loss, 2.34 val_loss
14700/100000 updates, 0.36 s, 2.85 train_loss, 2.35 val_loss
14800/100000 updates, 0.37 s, 2.13 train_loss, 2.36 val_loss
14900/100000 updates, 0.46 s, 2.12 train_loss, 2.35 val_loss
slurmstepd-boromir: error: *** JOB 566154 ON boromir CANCELLED AT 2023-04-30T16:22:28 ***
15000/100000 updates, 0.36 s, 2.47 train_loss, 2.33 val_loss
