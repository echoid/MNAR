{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from scipy import optimize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from data_loaders import *\n",
    "import missing_process.missing_method as missing_method\n",
    "from missing_process.block_rules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCAR(observed_values, missing_ratio, masks):\n",
    "    for col in range(observed_values.shape[1]):  # col #\n",
    "\n",
    "        obs_indices = np.where(observed_values[:, col])[0]\n",
    "        miss_indices = np.random.choice(\n",
    "        obs_indices, (int)(len(obs_indices) * missing_ratio), replace=False\n",
    "        )\n",
    "        masks[miss_indices, col] = False\n",
    "\n",
    "    return masks\n",
    "\n",
    "def process_func(dataname, path: str, aug_rate=1,missing_type = \"MCAR\",\n",
    "                  missing_para = \"\"):\n",
    " \n",
    "    data = dataset_loader(dataname)\n",
    "    # print(data)\n",
    "    # data.replace(\"?\", np.nan, inplace=True)\n",
    "    # Don't apply data argument (use n*dataset)\n",
    "    # data_aug = pd.concat([data] * aug_rate)\n",
    "\n",
    "    observed_values = data[\"data\"].astype(\"float32\")\n",
    "\n",
    "    observed_masks = ~np.isnan(observed_values)\n",
    "    masks = observed_masks.copy()\n",
    "\n",
    "    \"Need input origin dataset and parameters\"\n",
    "    if missing_type == \"MCAR\":\n",
    "        masks = MCAR(observed_values,missing_para,masks)\n",
    "\n",
    "    elif missing_type == \"quantile\":\n",
    "        Xnan, Xz = missing_method.missing_by_range(observed_values, missing_para)\n",
    "        masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
    "\n",
    "    elif missing_type == \"logistic\":\n",
    "        masks = missing_method.MNAR_mask_logistic(observed_values, missing_para)\n",
    "\n",
    "    elif missing_type == \"diffuse\":\n",
    "        #print(\"Go Diffuse\")\n",
    "        #masks = missing_method.MNAR_self_mask_logistic(observed_values, missing_para)\n",
    "\n",
    "        masks = diffuse_mnar_single(observed_values, missing_para[0],missing_para[1])\n",
    "\n",
    "\n",
    "    # gt_mask: 0 for missing elements and manully maksed elements\n",
    "    gt_masks = masks.reshape(observed_masks.shape)\n",
    "\n",
    "    observed_values = np.nan_to_num(observed_values)\n",
    "    observed_masks = observed_masks.astype(int)\n",
    "    gt_masks = gt_masks.astype(int)\n",
    "\n",
    "    return observed_values, observed_masks, gt_masks, data[\"data\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tabular_dataset(Dataset):\n",
    "    # eval_length should be equal to attributes number.\n",
    "    def __init__(\n",
    "        self, dataname, use_index_list=None, \n",
    "        aug_rate=1, seed=0,\n",
    "        missing_type = \"MCAR\", missing_para = \"\",missing_name = \"MCAR\"\n",
    "        ):\n",
    "        #self.eval_length = eval_length\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        dataset_path = f\"datasets/{dataname}/data.csv\"\n",
    "        processed_data_path = (\n",
    "            f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}.pk\"\n",
    "        )\n",
    "        processed_data_path_norm = (\n",
    "            f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}_max-min_norm.pk\"\n",
    "        )\n",
    "        # If no dataset created\n",
    "        if not os.path.isfile(processed_data_path):\n",
    "            #print(\"--------NO Dataset--------\")\n",
    "            self.observed_values, self.observed_masks, self.gt_masks, self.eval_length = process_func(\n",
    "                dataname, dataset_path, aug_rate=aug_rate,\n",
    "                missing_type = missing_type, missing_para = missing_para\n",
    "            )\n",
    "            #print(\"self.eval_length\",self.eval_length)\n",
    "            # with open(processed_data_path, \"wb\") as f:\n",
    "            #     pickle.dump(\n",
    "            #         [self.observed_values, self.observed_masks, self.gt_masks, self.eval_length], f\n",
    "            #     )\n",
    "            #    print(\"--------Dataset created--------\")\n",
    "\n",
    "        elif os.path.isfile(processed_data_path):\n",
    "        #elif os.path.isfile(processed_data_path_norm):\n",
    "            with open(processed_data_path_norm, \"rb\") as f:\n",
    "                self.observed_values, self.observed_masks, self.gt_masks, self.eval_length = pickle.load(\n",
    "                    f\n",
    "                )\n",
    "                #print(\"--------Dataset loaded--------\")\n",
    "\n",
    "            \n",
    "            #print(\"--------Normalized dataset loaded--------\")\n",
    "\n",
    "\n",
    "            # 计算0的占比\n",
    "            # zero_percentage = (self.gt_masks == 0).mean() * 100\n",
    "\n",
    "            # print(f\"0的占比: {zero_percentage}%\")\n",
    "        \n",
    "        if use_index_list is None:\n",
    "            self.use_index_list = np.arange(len(self.observed_values))\n",
    "        else:\n",
    "            self.use_index_list = use_index_list\n",
    "\n",
    "    def __getitem__(self, org_index):\n",
    "        index = self.use_index_list[org_index]\n",
    "        s = {\n",
    "            \"observed_data\": self.observed_values[index],\n",
    "            \"observed_mask\": self.observed_masks[index],\n",
    "            \"gt_mask\": self.gt_masks[index],\n",
    "            \"timepoints\": np.arange(self.eval_length),\n",
    "        }\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataname, seed=1, nfold=5, batch_size=16,\n",
    "                   missing_type = \"MCAR\", missing_para = \"\", missing_name = \"MCAR\"):\n",
    "\n",
    "    dataset = tabular_dataset(dataname = dataname,seed=seed,\n",
    "                              missing_type = missing_type, missing_para = missing_para,\n",
    "                                missing_name = missing_name)\n",
    "    #print(f\"Dataset size:{len(dataset)} entries\")\n",
    "    \n",
    "    \n",
    "    indlist = np.arange(len(dataset))\n",
    "\n",
    "    np.random.seed(seed + 1)\n",
    "    np.random.shuffle(indlist)\n",
    "\n",
    "    tmp_ratio = 1 / nfold\n",
    "    start = (int)((nfold - 1) * len(dataset) * tmp_ratio)\n",
    "    \n",
    "    end = (int)(nfold * len(dataset) * tmp_ratio)\n",
    "\n",
    "    test_index = indlist[start:end]\n",
    "    remain_index = np.delete(indlist, np.arange(start, end))\n",
    "\n",
    "    np.random.shuffle(remain_index)\n",
    "\n",
    "    # Modify here to change train,valid ratio\n",
    "    num_train = (int)(len(remain_index) * 0.9)\n",
    "    train_index = remain_index[:num_train]\n",
    "    valid_index = remain_index[num_train:]\n",
    "\n",
    "\n",
    "\n",
    "    # Here we perform max-min normalization.\n",
    "    processed_data_path_norm = (\n",
    "        f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}_max-min_norm.pk\"\n",
    "    )\n",
    "    \n",
    "    if not os.path.isfile(processed_data_path_norm):\n",
    "        #print(\n",
    "        #    \"--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\"\n",
    "        #)\n",
    "        # data transformation after train-test split.\n",
    "        col_num = dataset.observed_values.shape[1]\n",
    "        max_arr = np.zeros(col_num)\n",
    "        min_arr = np.zeros(col_num)\n",
    "        mean_arr = np.zeros(col_num)\n",
    "        for k in range(col_num):\n",
    "            k =  2\n",
    "            # Using observed_mask to avoid counting missing values.\n",
    "            obs_ind = dataset.gt_masks[train_index, k].astype(bool)\n",
    "            temp = dataset.observed_values[train_index, k]\n",
    "            #print(temp[obs_ind])\n",
    "            try:\n",
    "                max_arr[k] = max(temp[obs_ind])\n",
    "                min_arr[k] = min(temp[obs_ind])\n",
    "            except:\n",
    "                max_arr[k] = max(temp)\n",
    "                min_arr[k] = min(temp)\n",
    "        # print(f\"--------------Max-value for each column {max_arr}--------------\")\n",
    "        # print(f\"--------------Min-value for each column {min_arr}--------------\")\n",
    "\n",
    "\n",
    "        dataset.observed_values = (\n",
    "            (dataset.observed_values - min_arr) / (max_arr - min_arr + 0.000001)\n",
    "        ) #* dataset.gt_masks\n",
    "\n",
    "        #print(\"Save dataset\")\n",
    "        with open(processed_data_path_norm, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                [dataset.observed_values, dataset.observed_masks, dataset.gt_masks, dataset.eval_length], f\n",
    "            )\n",
    "\n",
    "#     # Create datasets and corresponding data loaders objects.\n",
    "#     train_dataset = tabular_dataset(dataname = dataname,\n",
    "#         use_index_list=train_index, seed=seed,\n",
    "#         missing_type = missing_type, missing_para = missing_para, missing_name = missing_name\n",
    "#     )\n",
    "#     #train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=1)\n",
    "#     valid_dataset = tabular_dataset(dataname = dataname,\n",
    "#         use_index_list=valid_index, seed=seed,\n",
    "#         missing_type = missing_type, missing_para = missing_para, missing_name = missing_name\n",
    "#     )\n",
    "#     #valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=0)\n",
    "\n",
    "#     test_dataset = tabular_dataset(dataname = dataname,\n",
    "#         use_index_list=test_index, seed=seed,\n",
    "#         missing_type = missing_type, missing_para = missing_para, missing_name = missing_name\n",
    "#     )\n",
    "#    #test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=0)\n",
    "\n",
    "#     print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "#     print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "#     print(f\"Testing dataset size: {len(test_dataset)}\")\n",
    "\n",
    "    #return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cc : self_mask\n",
    "\n",
    "# Red: Self_mask\n",
    "\n",
    "# white: Self_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar_single(data, up_percentile = 0.5, obs_percentile = 0.5):\n",
    "    \n",
    "    def scale_data(data):\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "        return scaled_data\n",
    "\n",
    "    data = scale_data(data)\n",
    "\n",
    "    mask = np.ones(data.shape)\n",
    "\n",
    "    n_cols = data.shape[1]\n",
    "    n_miss_cols = int(n_cols)  # 选择50%的列作为缺失列\n",
    "    miss_cols = np.random.choice(n_cols, size=n_miss_cols, replace=False)  # 随机选择缺失列的索引\n",
    "\n",
    "    obs_cols = [col for col in range(data.shape[1]) if col not in miss_cols]\n",
    "    \n",
    "    for miss_col in miss_cols:\n",
    "        missvar_bounds = np.quantile(data[:, miss_col], up_percentile)\n",
    "        temp = data[:, miss_col] >= missvar_bounds\n",
    "\n",
    "        obsvar_bounds = np.quantile(data[temp][:, -miss_cols], obs_percentile)\n",
    "        temp2 = data[:, miss_col] > obsvar_bounds\n",
    "\n",
    "        merged_temp = np.logical_or(temp, temp2).astype(int)\n",
    "        mask[:, miss_col] = merged_temp\n",
    "    print(\"Missing Rate\",1 - np.count_nonzero(mask) / mask.size)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"banknote\"#,,\n",
    "dataset = \"concrete_compression\"\n",
    "dataset = \"wine_quality_white\"\n",
    "dataset = \"wine_quality_red\"\n",
    "dataset = \"california\"\n",
    "dataset = \"climate_model_crashes\"\n",
    "\n",
    "seed = 1\n",
    "nfold = 5\n",
    "missingtype = \"logistic\"\n",
    "#missingtype = \"self_mask\"\n",
    "missingtype = \"diffuse\"\n",
    "missingtype = \"quantile\"\n",
    "\n",
    "missing_rule = load_json_file(\"diffuse_ratio.json\")\n",
    "missing_rule = load_json_file(\"q_ratio.json\")\n",
    "missing_rule = load_json_file(\"single_quantile.json\")\n",
    "missing_rule = load_json_file(\"double_quantile_1.json\")\n",
    "missing_rule = load_json_file(\"double_quantile_2.json\")\n",
    "\n",
    "for rule_name in missing_rule:\n",
    "    rule = missing_rule[rule_name]\n",
    "    print(\"Current Rule\",rule )\n",
    "    # Create folder\n",
    "    # Every loader contains \"observed_data\", \"observed_mask\", \"gt_mask\", \"timepoints\"\n",
    "    get_dataloader(\n",
    "        dataname=dataset,\n",
    "        seed=seed,\n",
    "        nfold=nfold,\n",
    "        batch_size=128,\n",
    "        missing_type = missingtype,\n",
    "        missing_para = rule,\n",
    "        missing_name = rule_name\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = [\"banknote\",\"concrete_compression\",\n",
    "            \"wine_quality_white\",\"wine_quality_red\",\n",
    "            \"california\",\"climate_model_crashes\",\n",
    "            \"connectionist_bench_sonar\",\"qsar_biodegradation\",\n",
    "            \"yeast\",\"airfoil_self_noise\"\n",
    "            ]\n",
    "\n",
    "missingtypelist = [\"logistic\",\"diffuse\",\"quantile\"]\n",
    "\n",
    "\n",
    "# missing_rule = load_json_file(\"diffuse_ratio.json\")\n",
    "# missing_rule = load_json_file(\"q_ratio.json\")\n",
    "# missing_rule = load_json_file(\"single_quantile.json\")\n",
    "# missing_rule = load_json_file(\"double_quantile_1.json\")\n",
    "# missing_rule = load_json_file(\"double_quantile_2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\Epochoid\\AppData\\Local\\Temp\\ipykernel_3784\\2387325653.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      " 40%|████      | 4/10 [00:00<00:00, 37.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "california logistic 0.1\n",
      "california logistic 0.3\n",
      "california logistic 0.5\n",
      "california logistic 0.6\n",
      "california logistic 0.7\n",
      "california logistic 0.8\n",
      "california logistic 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Epochoid\\AppData\\Local\\Temp\\ipykernel_3784\\2387325653.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      "C:\\Users\\Epochoid\\AppData\\Local\\Temp\\ipykernel_3784\\2387325653.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      "C:\\Users\\Epochoid\\AppData\\Local\\Temp\\ipykernel_3784\\2387325653.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      "C:\\Users\\Epochoid\\AppData\\Local\\Temp\\ipykernel_3784\\2387325653.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      " 80%|████████  | 8/10 [00:00<00:00, 11.96it/s]C:\\Users\\Epochoid\\AppData\\Local\\Temp\\ipykernel_3784\\2387325653.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      "C:\\Users\\Epochoid\\AppData\\Local\\Temp\\ipykernel_3784\\2387325653.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      "C:\\Users\\Epochoid\\AppData\\Local\\Temp\\ipykernel_3784\\2387325653.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      "C:\\Users\\Epochoid\\AppData\\Local\\Temp\\ipykernel_3784\\2387325653.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      "C:\\Users\\Epochoid\\AppData\\Local\\Temp\\ipykernel_3784\\2387325653.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Rate 0.6815701929474385\n",
      "Missing Rate 0.7471723220226214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "nfold = 5\n",
    "\n",
    "for dataset in tqdm(datalist):\n",
    "    \n",
    "    for missingtype in missingtypelist:\n",
    "        if missingtype == \"logistic\":\n",
    "            missing_rule = load_json_file(\"missing_rate.json\")\n",
    "        elif missingtype == \"diffuse\":\n",
    "            missing_rule = load_json_file(\"diffuse_ratio.json\")\n",
    "        elif missingtype == \"quantile\":\n",
    "            missing_rule = load_json_file(\"complete.json\")\n",
    "        \n",
    "\n",
    "        for rule_name in missing_rule:\n",
    "            \n",
    "            rule = missing_rule[rule_name]\n",
    "\n",
    "            # Create folder\n",
    "            # Every loader contains \"observed_data\", \"observed_mask\", \"gt_mask\", \"timepoints\"\n",
    "            try:\n",
    "                get_dataloader(\n",
    "                    dataname=dataset,\n",
    "                    seed=seed,\n",
    "                    nfold=nfold,\n",
    "                    batch_size=128,\n",
    "                    missing_type = missingtype,\n",
    "                    missing_para = rule,\n",
    "                    missing_name = rule_name\n",
    "                )\n",
    "            except:\n",
    "                print(dataset,missingtype,rule_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7272727489471436\n",
      "2\n",
      "0.4279279112815857\n",
      "1\n",
      "0.2078743278980255\n",
      "4\n",
      "0.14898990094661713\n",
      "0\n",
      "1.0\n",
      "3\n",
      "Missing Rate 0.6815701929474385\n",
      "[False False False ... False False False]\n",
      "[False False False ... False False False]\n",
      "[False False False ... False False False]\n",
      "[False False False ... False False False]\n",
      "[False False False ... False False False]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"airfoil_self_noise\"\n",
    "missingtype = \"diffuse\"\n",
    "rule_name = \"0.7\"\n",
    "\n",
    "\n",
    "if missingtype == \"logistic\":\n",
    "    missing_rule = load_json_file(\"missing_rate.json\")\n",
    "elif missingtype == \"diffuse\":\n",
    "    missing_rule = load_json_file(\"diffuse_ratio.json\")\n",
    "elif missingtype == \"quantile\":\n",
    "    missing_rule = load_json_file(\"complete.json\")\n",
    "\n",
    "rule = missing_rule[rule_name] \n",
    "\n",
    "get_dataloader(\n",
    "    dataname=dataset,\n",
    "    seed=seed,\n",
    "    nfold=nfold,\n",
    "    batch_size=128,\n",
    "    missing_type = missingtype,\n",
    "    missing_para = rule,\n",
    "    missing_name = rule_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义需要替换的字符串和对应的替换字符串\n",
    "replacements = {\n",
    "    \"logistic-0.25+\": \"logistic-0.75\",\n",
    "    \"logistic-0.75+\": \"logistic-0.25\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_name = os.listdir(\"datasets\")\n",
    "for data in mod_name:\n",
    "    path = \"datasets/{}\".format(data)\n",
    "    files = os.listdir(path)\n",
    "    # 循环遍历文件列表\n",
    "    for filename in files:\n",
    "        # 遍历替换字典中的键值对\n",
    "        for old_str, new_str in replacements.items():\n",
    "            # 检查文件名中是否包含需要替换的字符串\n",
    "            if old_str in filename:\n",
    "                # 使用字符串的replace方法进行替换\n",
    "                new_filename = filename.replace(old_str, new_str)\n",
    "                # 使用os.rename()函数来重命名文件\n",
    "                os.rename(\"{}/{}\".format(path,filename), \"{}/{}\".format(path,new_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "n_var = 3\n",
    "min_corr = 0.1\n",
    "max_corr = 0.3\n",
    "\n",
    "mu = np.zeros(n_var)  # mean vector\n",
    "np.random.seed(1)\n",
    "\n",
    "corr = np.random.uniform(min_corr, max_corr, size=int(n_var * (n_var - 1) / 2))  # correlation vector (n_var, 2)\n",
    "cov = np.zeros((n_var, n_var))  # covariance matrix\n",
    "\n",
    "diag = np.eye(n_var)\n",
    "\n",
    "\n",
    "cov[np.triu_indices(n_var, k=1)] = corr  # fill upper triangular part with correlations\n",
    "cov = cov + cov.T + diag\n",
    "\n",
    "\n",
    "n = 300  # sample size\n",
    "np.random.seed(2)  # set seed so results are replicable\n",
    "dat = multivariate_normal.rvs(mean=mu, cov=cov, size=n)  # data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def missing(data, prob_miss, seed=100):\n",
    "    np.random.seed(seed)\n",
    "    m = np.zeros(data.shape[0])\n",
    "    for i in range(data.shape[0]):\n",
    "        m[i] = np.random.binomial(n=1, size=1, p=prob_miss[i])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def miss_data(miss_ind, dat, miss_col, n_var):\n",
    "    miss_dat = pd.DataFrame(dat)\n",
    "    miss_dat['miss.ind'] = miss_ind\n",
    "    miss_dat[miss_col] = np.where(miss_ind == 1, np.nan, miss_dat[miss_col])\n",
    "    \n",
    "    colnames = ['obs. var' + str(i) for i in range(1, n_var)] + ['miss.ind', 'miss.val']\n",
    "    miss_dat.columns = colnames\n",
    "    miss_dat.columns.values[miss_col] = 'miss.var'\n",
    "    \n",
    "    return miss_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar(target_miss, up_percentile, obs_percentile, dat, miss_col, n_var):\n",
    "    missvar_bounds = np.quantile(dat[:, miss_col], up_percentile)\n",
    "    temp = dat[:, miss_col] > missvar_bounds\n",
    "\n",
    "    obsvar_bounds = np.quantile(dat[temp, :-1][:, 0], obs_percentile)\n",
    "\n",
    "    miss_ind = np.zeros(len(dat))\n",
    "    miss_ind[temp] = dat[temp, :-1][:, 0] > obsvar_bounds\n",
    "\n",
    "    miss_dat = np.column_stack((dat, miss_ind, dat[:, miss_col]))\n",
    "    miss_dat[miss_ind == 1, miss_col] = np.nan\n",
    "\n",
    "    colnames = ['obs. var' + str(i) for i in range(1, n_var)] + ['miss. ind', 'miss. val']\n",
    "    miss_dat = pd.DataFrame(miss_dat)\n",
    "    #miss_dat.columns.values[miss_col] = 'miss.var'\n",
    "\n",
    "    return miss_dat,dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar(target_miss, up_percentile, obs_percentile, dat, miss_col, n_var):\n",
    "    missvar_bounds = np.quantile(dat[:, miss_col], up_percentile)\n",
    "    temp = dat[:, miss_col] > missvar_bounds\n",
    "\n",
    "    obsvar_bounds = np.quantile(dat[temp, :-1][:, 0], obs_percentile)\n",
    "\n",
    "    mask = np.zeros(len(dat))\n",
    "    mask[temp] = dat[temp, :-1][:, 0] > obsvar_bounds\n",
    "\n",
    "\n",
    "    return mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuse_mnar(target_miss, up_percentile, obs_percentile, dat, miss_col, n_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_miss = 0.2\n",
    "up_percentile = 0.6  # 必须小于1 - target.miss\n",
    "obs_percentile = 1 - target_miss/(1-up_percentile)\n",
    "\n",
    "miss_col =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar_single(data, up_percentile=0.5, obs_percentile=0.5):\n",
    "    \n",
    "    def scale_data(data):\n",
    "      min_vals = np.min(data, axis=0)\n",
    "      max_vals = np.max(data, axis=0)\n",
    "      scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "      return scaled_data\n",
    "\n",
    "    data = scale_data(data)\n",
    "\n",
    "    mask = np.ones(data.shape)\n",
    "\n",
    "    n_cols = data.shape[1]\n",
    "    n_miss_cols = int(n_cols * 0.5)  # 选择50%的列作为缺失列\n",
    "    miss_cols = np.random.choice(n_cols, size=n_miss_cols, replace=False)  # 随机选择缺失列的索引\n",
    "\n",
    "    obs_cols = [col for col in range(data.shape[1]) if col not in miss_cols]\n",
    "    for miss_col in miss_cols:\n",
    "      print(miss_col)\n",
    "      missvar_bounds = np.quantile(data[:, miss_col], up_percentile)\n",
    "      print(data[:, miss_col])\n",
    "      print(missvar_bounds)\n",
    "      temp = data[:, miss_col] > missvar_bounds\n",
    "      print(temp)\n",
    "\n",
    "\n",
    "      obsvar_bounds = np.quantile(data[temp, :][:,obs_cols], obs_percentile)\n",
    "      print(obsvar_bounds)\n",
    "      \n",
    "        # 初始化与原始数据维度相同的mask，所有元素均为1\n",
    "      #mask[temp, miss_col] = (data[temp, :][:,obs_cols][:, 0] <= obsvar_bounds).astype(int)\n",
    "      temp2 = data[:, miss_col]> obsvar_bounds\n",
    "      #temp2 = data[:, miss_col] > missvar_bounds  # 根据缺失值情况将对应位置的值设为0\n",
    "      print(temp2)\n",
    "      print()\n",
    "      # print(mask)\n",
    "      # print()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar_single(data, up_percentile = 0.5, obs_percentile = 0.5):\n",
    "    \n",
    "    def scale_data(data):\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "        return scaled_data\n",
    "\n",
    "    data = scale_data(data)\n",
    "\n",
    "    mask = np.ones(data.shape)\n",
    "\n",
    "    n_cols = data.shape[1]\n",
    "    n_miss_cols = int(n_cols * 0.5)  # 选择50%的列作为缺失列\n",
    "    miss_cols = np.random.choice(n_cols, size=n_miss_cols, replace=False)  # 随机选择缺失列的索引\n",
    "\n",
    "    obs_cols = [col for col in range(data.shape[1]) if col not in miss_cols]\n",
    "    \n",
    "    for miss_col in miss_cols:\n",
    "        missvar_bounds = np.quantile(data[:, miss_col], up_percentile)\n",
    "        temp = data[:, miss_col] > missvar_bounds\n",
    "        \n",
    "        obsvar_bounds = np.quantile(data[temp][:, obs_cols], obs_percentile)\n",
    "        temp2 = data[:, miss_col] > obsvar_bounds\n",
    "\n",
    "        merged_temp = np.logical_or(temp, temp2).astype(int)\n",
    "        mask[:, miss_col] = merged_temp\n",
    "    print(\"Missing Rate\",1 - np.count_nonzero(mask) / mask.size)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuse_mnar_single(dat,0.75,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar_single(data, target_miss_rate, up_percentile, obs_percentile):\n",
    "    \n",
    "    def scale_data(data):\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "        return scaled_data\n",
    "    \n",
    "    def compute_miss_rate(mask):\n",
    "        return 1 - np.count_nonzero(mask) / mask.size\n",
    "    \n",
    "    data = scale_data(data)\n",
    "\n",
    "    mask = np.ones(data.shape)\n",
    "\n",
    "    n_cols = data.shape[1]\n",
    "    n_miss_cols = int(n_cols * 0.5)  # 选择50%的列作为缺失列\n",
    "    miss_cols = np.random.choice(n_cols, size=n_miss_cols, replace=False)  # 随机选择缺失列的索引\n",
    "\n",
    "    obs_cols = [col for col in range(data.shape[1]) if col not in miss_cols]\n",
    "    \n",
    "    while True:\n",
    "        for miss_col in miss_cols:\n",
    "            missvar_bounds = np.quantile(data[:, miss_col], up_percentile)\n",
    "            temp = data[:, miss_col] > missvar_bounds\n",
    "            obsvar_bounds = np.quantile(data[temp][:, obs_cols], obs_percentile)\n",
    "            temp2 = data[:, miss_col] > obsvar_bounds\n",
    "\n",
    "            merged_temp = np.logical_or(temp, temp2).astype(int)\n",
    "            mask[:, miss_col] = merged_temp\n",
    "        \n",
    "        miss_rate = compute_miss_rate(mask)\n",
    "\n",
    "        print(\"Missing Rate\",miss_rate)\n",
    "        \n",
    "        if miss_rate <= target_miss_rate:\n",
    "            #up_percentile += 0.05\n",
    "            obs_percentile += 0.05\n",
    "            print(up_percentile, obs_percentile)\n",
    "        else:\n",
    "            #print(up_percentile,obs_percentile)\n",
    "            return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[10040,10001,10002],[1,2,1],[10,20,50],[1,1,2]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = diffuse_mnar_single(dat,0.2,0.5,0.5)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
