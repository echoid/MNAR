{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "import datetime\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "from scipy import optimize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from data_loaders import *\n",
    "import missing_process.missing_method as missing_method\n",
    "from missing_process.block_rules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCAR(observed_values, missing_ratio, masks):\n",
    "    for col in range(observed_values.shape[1]):  # col #\n",
    "\n",
    "        obs_indices = np.where(observed_values[:, col])[0]\n",
    "        miss_indices = np.random.choice(\n",
    "        obs_indices, (int)(len(obs_indices) * missing_ratio), replace=False\n",
    "        )\n",
    "        masks[miss_indices, col] = False\n",
    "\n",
    "    return masks\n",
    "\n",
    "def process_func(dataname, path: str, aug_rate=1,missing_type = \"MCAR\",\n",
    "                  missing_para = \"\"):\n",
    " \n",
    "    data = dataset_loader(dataname)\n",
    "    # print(data)\n",
    "    # data.replace(\"?\", np.nan, inplace=True)\n",
    "    # Don't apply data argument (use n*dataset)\n",
    "    # data_aug = pd.concat([data] * aug_rate)\n",
    "\n",
    "    observed_values = data[\"data\"].astype(\"float32\")\n",
    "\n",
    "    observed_masks = ~np.isnan(observed_values)\n",
    "    masks = observed_masks.copy()\n",
    "\n",
    "    \"Need input origin dataset and parameters\"\n",
    "    if missing_type == \"MCAR\":\n",
    "        masks = MCAR(observed_values,missing_para,masks)\n",
    "\n",
    "    elif missing_type == \"quantile\":\n",
    "        Xnan, Xz = missing_method.missing_by_range(observed_values, missing_para)\n",
    "        masks = np.array(~np.isnan(Xnan), dtype=np.float)\n",
    "\n",
    "    elif missing_type == \"logistic\":\n",
    "        masks = missing_method.MNAR_mask_logistic(observed_values, missing_para)\n",
    "\n",
    "    elif missing_type == \"diffuse\":\n",
    "        #masks = missing_method.MNAR_self_mask_logistic(observed_values, missing_para)\n",
    "\n",
    "        masks = diffuse_mnar_single(observed_values, missing_para[0],missing_para[1])\n",
    "\n",
    "\n",
    "    # gt_mask: 0 for missing elements and manully maksed elements\n",
    "    gt_masks = masks.reshape(observed_masks.shape)\n",
    "\n",
    "    observed_values = np.nan_to_num(observed_values)\n",
    "    observed_masks = observed_masks.astype(int)\n",
    "    gt_masks = gt_masks.astype(int)\n",
    "\n",
    "    return observed_values, observed_masks, gt_masks, data[\"data\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tabular_dataset(Dataset):\n",
    "    # eval_length should be equal to attributes number.\n",
    "    def __init__(\n",
    "        self, dataname, use_index_list=None, \n",
    "        aug_rate=1, seed=0,\n",
    "        missing_type = \"MCAR\", missing_para = \"\",missing_name = \"MCAR\"\n",
    "        ):\n",
    "        #self.eval_length = eval_length\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        dataset_path = f\"datasets/{dataname}/data.csv\"\n",
    "        processed_data_path = (\n",
    "            f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}.pk\"\n",
    "        )\n",
    "        processed_data_path_norm = (\n",
    "            f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}_max-min_norm.pk\"\n",
    "        )\n",
    "        # If no dataset created\n",
    "        if not os.path.isfile(processed_data_path):\n",
    "            self.observed_values, self.observed_masks, self.gt_masks, self.eval_length = process_func(\n",
    "                dataname, dataset_path, aug_rate=aug_rate,\n",
    "                missing_type = missing_type, missing_para = missing_para\n",
    "            )\n",
    "            print(\"self.eval_length\",self.eval_length)\n",
    "            with open(processed_data_path, \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    [self.observed_values, self.observed_masks, self.gt_masks, self.eval_length], f\n",
    "                )\n",
    "            print(\"--------Dataset created--------\")\n",
    "\n",
    "        elif os.path.isfile(processed_data_path_norm):\n",
    "            with open(processed_data_path_norm, \"rb\") as f:\n",
    "                self.observed_values, self.observed_masks, self.gt_masks, self.eval_length = pickle.load(\n",
    "                    f\n",
    "                )\n",
    "            print(\"--------Normalized dataset loaded--------\")\n",
    "        \n",
    "        if use_index_list is None:\n",
    "            self.use_index_list = np.arange(len(self.observed_values))\n",
    "        else:\n",
    "            self.use_index_list = use_index_list\n",
    "\n",
    "    def __getitem__(self, org_index):\n",
    "        index = self.use_index_list[org_index]\n",
    "        s = {\n",
    "            \"observed_data\": self.observed_values[index],\n",
    "            \"observed_mask\": self.observed_masks[index],\n",
    "            \"gt_mask\": self.gt_masks[index],\n",
    "            \"timepoints\": np.arange(self.eval_length),\n",
    "        }\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataname, seed=1, nfold=5, batch_size=16,\n",
    "                   missing_type = \"MCAR\", missing_para = \"\", missing_name = \"MCAR\"):\n",
    "\n",
    "    dataset = tabular_dataset(dataname = dataname,seed=seed,\n",
    "                              missing_type = missing_type, missing_para = missing_para,\n",
    "                                missing_name = missing_name)\n",
    "    print(f\"Dataset size:{len(dataset)} entries\")\n",
    "    \n",
    "    \n",
    "    indlist = np.arange(len(dataset))\n",
    "\n",
    "    np.random.seed(seed + 1)\n",
    "    np.random.shuffle(indlist)\n",
    "\n",
    "    tmp_ratio = 1 / nfold\n",
    "    start = (int)((nfold - 1) * len(dataset) * tmp_ratio)\n",
    "    \n",
    "    end = (int)(nfold * len(dataset) * tmp_ratio)\n",
    "\n",
    "    test_index = indlist[start:end]\n",
    "    remain_index = np.delete(indlist, np.arange(start, end))\n",
    "\n",
    "    np.random.shuffle(remain_index)\n",
    "\n",
    "    # Modify here to change train,valid ratio\n",
    "    num_train = (int)(len(remain_index) * 0.9)\n",
    "    train_index = remain_index[:num_train]\n",
    "    valid_index = remain_index[num_train:]\n",
    "\n",
    "\n",
    "\n",
    "    # Here we perform max-min normalization.\n",
    "    processed_data_path_norm = (\n",
    "        f\"datasets/{dataname}/{missing_type}-{missing_name}_seed-{seed}_max-min_norm.pk\"\n",
    "    )\n",
    "    if not os.path.isfile(processed_data_path_norm):\n",
    "        print(\n",
    "            \"--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\"\n",
    "        )\n",
    "        # data transformation after train-test split.\n",
    "        col_num = dataset.observed_values.shape[1]\n",
    "        max_arr = np.zeros(col_num)\n",
    "        min_arr = np.zeros(col_num)\n",
    "        mean_arr = np.zeros(col_num)\n",
    "        for k in range(col_num):\n",
    "            # Using observed_mask to avoid counting missing values.\n",
    "            obs_ind = dataset.observed_masks[train_index, k].astype(bool)\n",
    "            temp = dataset.observed_values[train_index, k]\n",
    "            max_arr[k] = max(temp[obs_ind])\n",
    "            min_arr[k] = min(temp[obs_ind])\n",
    "        print(f\"--------------Max-value for each column {max_arr}--------------\")\n",
    "        print(f\"--------------Min-value for each column {min_arr}--------------\")\n",
    "\n",
    "        dataset.observed_values = (\n",
    "            (dataset.observed_values - 0 + 1) / (max_arr - 0 + 1)\n",
    "        ) * dataset.observed_masks\n",
    "\n",
    "        with open(processed_data_path_norm, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                [dataset.observed_values, dataset.observed_masks, dataset.gt_masks, dataset.eval_length], f\n",
    "            )\n",
    "\n",
    "    # Create datasets and corresponding data loaders objects.\n",
    "    train_dataset = tabular_dataset(dataname = dataname,\n",
    "        use_index_list=train_index, seed=seed,\n",
    "        missing_type = missing_type, missing_para = missing_para, missing_name = missing_name\n",
    "    )\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=1)\n",
    "    valid_dataset = tabular_dataset(dataname = dataname,\n",
    "        use_index_list=valid_index, seed=seed,\n",
    "        missing_type = missing_type, missing_para = missing_para, missing_name = missing_name\n",
    "    )\n",
    "    #valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=0)\n",
    "\n",
    "    test_dataset = tabular_dataset(dataname = dataname,\n",
    "        use_index_list=test_index, seed=seed,\n",
    "        missing_type = missing_type, missing_para = missing_para, missing_name = missing_name\n",
    "    )\n",
    "   #test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=0)\n",
    "\n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "    print(f\"Testing dataset size: {len(test_dataset)}\")\n",
    "\n",
    "    #return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cc : self_mask\n",
    "\n",
    "# Red: Self_mask\n",
    "\n",
    "# white: Self_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar_single(data, up_percentile = 0.5, obs_percentile = 0.5):\n",
    "    \n",
    "    def scale_data(data):\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "        return scaled_data\n",
    "\n",
    "    data = scale_data(data)\n",
    "\n",
    "    mask = np.ones(data.shape)\n",
    "\n",
    "    n_cols = data.shape[1]\n",
    "    n_miss_cols = int(n_cols * 0.5)  # 选择50%的列作为缺失列\n",
    "    miss_cols = np.random.choice(n_cols, size=n_miss_cols, replace=False)  # 随机选择缺失列的索引\n",
    "\n",
    "    obs_cols = [col for col in range(data.shape[1]) if col not in miss_cols]\n",
    "    \n",
    "    for miss_col in miss_cols:\n",
    "        missvar_bounds = np.quantile(data[:, miss_col], up_percentile)\n",
    "        temp = data[:, miss_col] > missvar_bounds\n",
    "        \n",
    "        obsvar_bounds = np.quantile(data[temp][:, obs_cols], obs_percentile)\n",
    "        temp2 = data[:, miss_col] > obsvar_bounds\n",
    "\n",
    "        merged_temp = np.logical_or(temp, temp2).astype(int)\n",
    "        mask[:, miss_col] = merged_temp\n",
    "    print(\"Missing Rate\",1 - np.count_nonzero(mask) / mask.size)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Rule [0.25, 0.25]\n",
      "Goto diffuse\n",
      "Missing Rate 0.08789204959883301\n",
      "self.eval_length 4\n",
      "--------Dataset created--------\n",
      "Dataset size:1371 entries\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------------Max-value for each column [ 6.82480001 12.95160007 17.92740059  2.44950008]--------------\n",
      "--------------Min-value for each column [ -7.04209995 -13.7730999   -5.28609991  -8.54819965]--------------\n",
      "--------Normalized dataset loaded--------\n",
      "--------Normalized dataset loaded--------\n",
      "--------Normalized dataset loaded--------\n",
      "Training dataset size: 986\n",
      "Validation dataset size: 110\n",
      "Testing dataset size: 275\n",
      "Current Rule [0.5, 0.5]\n",
      "Goto diffuse\n",
      "Missing Rate 0.18180160466812545\n",
      "self.eval_length 4\n",
      "--------Dataset created--------\n",
      "Dataset size:1371 entries\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------------Max-value for each column [ 6.82480001 12.95160007 17.92740059  2.44950008]--------------\n",
      "--------------Min-value for each column [ -7.04209995 -13.7730999   -5.28609991  -8.54819965]--------------\n",
      "--------Normalized dataset loaded--------\n",
      "--------Normalized dataset loaded--------\n",
      "--------Normalized dataset loaded--------\n",
      "Training dataset size: 986\n",
      "Validation dataset size: 110\n",
      "Testing dataset size: 275\n",
      "Current Rule [0.75, 0.75]\n",
      "Goto diffuse\n",
      "Missing Rate 0.2811816192560175\n",
      "self.eval_length 4\n",
      "--------Dataset created--------\n",
      "Dataset size:1371 entries\n",
      "--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n",
      "--------------Max-value for each column [ 6.82480001 12.95160007 17.92740059  2.44950008]--------------\n",
      "--------------Min-value for each column [ -7.04209995 -13.7730999   -5.28609991  -8.54819965]--------------\n",
      "--------Normalized dataset loaded--------\n",
      "--------Normalized dataset loaded--------\n",
      "--------Normalized dataset loaded--------\n",
      "Training dataset size: 986\n",
      "Validation dataset size: 110\n",
      "Testing dataset size: 275\n"
     ]
    }
   ],
   "source": [
    "dataset = \"banknote\"#\"concrete_compression\",\"wine_quality_white\",\"banknote\"\n",
    "seed = 1\n",
    "nfold = 5\n",
    "missingtype = \"diffuse\"\n",
    "#missingtype = \"self_mask\"\n",
    "\n",
    "missing_rule = load_json_file(\"diffuse_ratio.json\")\n",
    "\n",
    "\n",
    "for rule_name in missing_rule:\n",
    "    rule = missing_rule[rule_name]\n",
    "    print(\"Current Rule\",rule )\n",
    "    # Create folder\n",
    "    # Every loader contains \"observed_data\", \"observed_mask\", \"gt_mask\", \"timepoints\"\n",
    "    get_dataloader(\n",
    "        dataname=dataset,\n",
    "        seed=seed,\n",
    "        nfold=nfold,\n",
    "        batch_size=128,\n",
    "        missing_type = missingtype,\n",
    "        missing_para = rule,\n",
    "        missing_name = rule_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "n_var = 3\n",
    "min_corr = 0.1\n",
    "max_corr = 0.3\n",
    "\n",
    "mu = np.zeros(n_var)  # mean vector\n",
    "np.random.seed(1)\n",
    "\n",
    "corr = np.random.uniform(min_corr, max_corr, size=int(n_var * (n_var - 1) / 2))  # correlation vector (n_var, 2)\n",
    "cov = np.zeros((n_var, n_var))  # covariance matrix\n",
    "\n",
    "diag = np.eye(n_var)\n",
    "\n",
    "\n",
    "cov[np.triu_indices(n_var, k=1)] = corr  # fill upper triangular part with correlations\n",
    "cov = cov + cov.T + diag\n",
    "\n",
    "\n",
    "n = 300  # sample size\n",
    "np.random.seed(2)  # set seed so results are replicable\n",
    "dat = multivariate_normal.rvs(mean=mu, cov=cov, size=n)  # data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def missing(data, prob_miss, seed=100):\n",
    "    np.random.seed(seed)\n",
    "    m = np.zeros(data.shape[0])\n",
    "    for i in range(data.shape[0]):\n",
    "        m[i] = np.random.binomial(n=1, size=1, p=prob_miss[i])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def miss_data(miss_ind, dat, miss_col, n_var):\n",
    "    miss_dat = pd.DataFrame(dat)\n",
    "    miss_dat['miss.ind'] = miss_ind\n",
    "    miss_dat[miss_col] = np.where(miss_ind == 1, np.nan, miss_dat[miss_col])\n",
    "    \n",
    "    colnames = ['obs. var' + str(i) for i in range(1, n_var)] + ['miss.ind', 'miss.val']\n",
    "    miss_dat.columns = colnames\n",
    "    miss_dat.columns.values[miss_col] = 'miss.var'\n",
    "    \n",
    "    return miss_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar(target_miss, up_percentile, obs_percentile, dat, miss_col, n_var):\n",
    "    missvar_bounds = np.quantile(dat[:, miss_col], up_percentile)\n",
    "    temp = dat[:, miss_col] > missvar_bounds\n",
    "\n",
    "    obsvar_bounds = np.quantile(dat[temp, :-1][:, 0], obs_percentile)\n",
    "\n",
    "    miss_ind = np.zeros(len(dat))\n",
    "    miss_ind[temp] = dat[temp, :-1][:, 0] > obsvar_bounds\n",
    "    print(miss_ind)\n",
    "\n",
    "    miss_dat = np.column_stack((dat, miss_ind, dat[:, miss_col]))\n",
    "    miss_dat[miss_ind == 1, miss_col] = np.nan\n",
    "\n",
    "    colnames = ['obs. var' + str(i) for i in range(1, n_var)] + ['miss. ind', 'miss. val']\n",
    "    miss_dat = pd.DataFrame(miss_dat)\n",
    "    #miss_dat.columns.values[miss_col] = 'miss.var'\n",
    "\n",
    "    return miss_dat,dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar(target_miss, up_percentile, obs_percentile, dat, miss_col, n_var):\n",
    "    missvar_bounds = np.quantile(dat[:, miss_col], up_percentile)\n",
    "    temp = dat[:, miss_col] > missvar_bounds\n",
    "\n",
    "    obsvar_bounds = np.quantile(dat[temp, :-1][:, 0], obs_percentile)\n",
    "\n",
    "    mask = np.zeros(len(dat))\n",
    "    mask[temp] = dat[temp, :-1][:, 0] > obsvar_bounds\n",
    "\n",
    "\n",
    "    return mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffuse_mnar(target_miss, up_percentile, obs_percentile, dat, miss_col, n_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_miss = 0.2\n",
    "up_percentile = 0.6  # 必须小于1 - target.miss\n",
    "obs_percentile = 1 - target_miss/(1-up_percentile)\n",
    "\n",
    "miss_col =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar_single(data, up_percentile=0.5, obs_percentile=0.5):\n",
    "    \n",
    "    def scale_data(data):\n",
    "      min_vals = np.min(data, axis=0)\n",
    "      max_vals = np.max(data, axis=0)\n",
    "      scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "      return scaled_data\n",
    "\n",
    "    data = scale_data(data)\n",
    "\n",
    "    mask = np.ones(data.shape)\n",
    "\n",
    "    n_cols = data.shape[1]\n",
    "    n_miss_cols = int(n_cols * 0.5)  # 选择50%的列作为缺失列\n",
    "    miss_cols = np.random.choice(n_cols, size=n_miss_cols, replace=False)  # 随机选择缺失列的索引\n",
    "\n",
    "    obs_cols = [col for col in range(data.shape[1]) if col not in miss_cols]\n",
    "    for miss_col in miss_cols:\n",
    "      print(miss_col)\n",
    "      missvar_bounds = np.quantile(data[:, miss_col], up_percentile)\n",
    "      print(data[:, miss_col])\n",
    "      print(missvar_bounds)\n",
    "      temp = data[:, miss_col] > missvar_bounds\n",
    "      print(temp)\n",
    "\n",
    "\n",
    "      obsvar_bounds = np.quantile(data[temp, :][:,obs_cols], obs_percentile)\n",
    "      print(obsvar_bounds)\n",
    "      \n",
    "        # 初始化与原始数据维度相同的mask，所有元素均为1\n",
    "      #mask[temp, miss_col] = (data[temp, :][:,obs_cols][:, 0] <= obsvar_bounds).astype(int)\n",
    "      temp2 = data[:, miss_col]> obsvar_bounds\n",
    "      #temp2 = data[:, miss_col] > missvar_bounds  # 根据缺失值情况将对应位置的值设为0\n",
    "      print(temp2)\n",
    "      print()\n",
    "      # print(mask)\n",
    "      # print()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar_single(data, up_percentile = 0.5, obs_percentile = 0.5):\n",
    "    \n",
    "    def scale_data(data):\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "        return scaled_data\n",
    "\n",
    "    data = scale_data(data)\n",
    "\n",
    "    mask = np.ones(data.shape)\n",
    "\n",
    "    n_cols = data.shape[1]\n",
    "    n_miss_cols = int(n_cols * 0.5)  # 选择50%的列作为缺失列\n",
    "    miss_cols = np.random.choice(n_cols, size=n_miss_cols, replace=False)  # 随机选择缺失列的索引\n",
    "\n",
    "    obs_cols = [col for col in range(data.shape[1]) if col not in miss_cols]\n",
    "    \n",
    "    for miss_col in miss_cols:\n",
    "        missvar_bounds = np.quantile(data[:, miss_col], up_percentile)\n",
    "        temp = data[:, miss_col] > missvar_bounds\n",
    "        \n",
    "        obsvar_bounds = np.quantile(data[temp][:, obs_cols], obs_percentile)\n",
    "        temp2 = data[:, miss_col] > obsvar_bounds\n",
    "\n",
    "        merged_temp = np.logical_or(temp, temp2).astype(int)\n",
    "        mask[:, miss_col] = merged_temp\n",
    "    print(\"Missing Rate\",1 - np.count_nonzero(mask) / mask.size)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23111111111111116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffuse_mnar_single(dat,0.75,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse_mnar_single(data, target_miss_rate, up_percentile, obs_percentile):\n",
    "    \n",
    "    def scale_data(data):\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "        return scaled_data\n",
    "    \n",
    "    def compute_miss_rate(mask):\n",
    "        return 1 - np.count_nonzero(mask) / mask.size\n",
    "    \n",
    "    data = scale_data(data)\n",
    "\n",
    "    mask = np.ones(data.shape)\n",
    "\n",
    "    n_cols = data.shape[1]\n",
    "    n_miss_cols = int(n_cols * 0.5)  # 选择50%的列作为缺失列\n",
    "    miss_cols = np.random.choice(n_cols, size=n_miss_cols, replace=False)  # 随机选择缺失列的索引\n",
    "\n",
    "    obs_cols = [col for col in range(data.shape[1]) if col not in miss_cols]\n",
    "    \n",
    "    while True:\n",
    "        for miss_col in miss_cols:\n",
    "            missvar_bounds = np.quantile(data[:, miss_col], up_percentile)\n",
    "            temp = data[:, miss_col] > missvar_bounds\n",
    "            obsvar_bounds = np.quantile(data[temp][:, obs_cols], obs_percentile)\n",
    "            temp2 = data[:, miss_col] > obsvar_bounds\n",
    "\n",
    "            merged_temp = np.logical_or(temp, temp2).astype(int)\n",
    "            mask[:, miss_col] = merged_temp\n",
    "        \n",
    "        miss_rate = compute_miss_rate(mask)\n",
    "\n",
    "        print(\"Missing Rate\",miss_rate)\n",
    "        \n",
    "        if miss_rate <= target_miss_rate:\n",
    "            #up_percentile += 0.05\n",
    "            obs_percentile += 0.05\n",
    "            print(up_percentile, obs_percentile)\n",
    "        else:\n",
    "            #print(up_percentile,obs_percentile)\n",
    "            return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[10040,10001,10002],[1,2,1],[10,20,50],[1,1,2]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Rate 0.15888888888888886\n",
      "0.5 0.55\n",
      "Missing Rate 0.16666666666666663\n",
      "0.5 0.6000000000000001\n",
      "Missing Rate 0.16666666666666663\n",
      "0.5 0.6500000000000001\n",
      "Missing Rate 0.16666666666666663\n",
      "0.5 0.7000000000000002\n",
      "Missing Rate 0.16666666666666663\n",
      "0.5 0.7500000000000002\n",
      "Missing Rate 0.16666666666666663\n",
      "0.5 0.8000000000000003\n",
      "Missing Rate 0.16666666666666663\n",
      "0.5 0.8500000000000003\n",
      "Missing Rate 0.16666666666666663\n",
      "0.5 0.9000000000000004\n",
      "Missing Rate 0.16666666666666663\n",
      "0.5 0.9500000000000004\n",
      "Missing Rate 0.16666666666666663\n",
      "0.5 1.0000000000000004\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Quantiles must be in the range [0, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mask \u001b[39m=\u001b[39m diffuse_mnar_single(dat,\u001b[39m0.2\u001b[39;49m,\u001b[39m0.5\u001b[39;49m,\u001b[39m0.5\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m mask\n",
      "Cell \u001b[1;32mIn[126], line 26\u001b[0m, in \u001b[0;36mdiffuse_mnar_single\u001b[1;34m(data, target_miss_rate, up_percentile, obs_percentile)\u001b[0m\n\u001b[0;32m     24\u001b[0m missvar_bounds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mquantile(data[:, miss_col], up_percentile)\n\u001b[0;32m     25\u001b[0m temp \u001b[39m=\u001b[39m data[:, miss_col] \u001b[39m>\u001b[39m missvar_bounds\n\u001b[1;32m---> 26\u001b[0m obsvar_bounds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mquantile(data[temp][:, obs_cols], obs_percentile)\n\u001b[0;32m     27\u001b[0m temp2 \u001b[39m=\u001b[39m data[:, miss_col] \u001b[39m>\u001b[39m obsvar_bounds\n\u001b[0;32m     29\u001b[0m merged_temp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlogical_or(temp, temp2)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mquantile\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32me:\\ANACONDA\\envs\\py3.10\\lib\\site-packages\\numpy\\lib\\function_base.py:4411\u001b[0m, in \u001b[0;36mquantile\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[0;32m   4409\u001b[0m q \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(q)\n\u001b[0;32m   4410\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _quantile_is_valid(q):\n\u001b[1;32m-> 4411\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mQuantiles must be in the range [0, 1]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4412\u001b[0m \u001b[39mreturn\u001b[39;00m _quantile_unchecked(\n\u001b[0;32m   4413\u001b[0m     a, q, axis, out, overwrite_input, method, keepdims)\n",
      "\u001b[1;31mValueError\u001b[0m: Quantiles must be in the range [0, 1]"
     ]
    }
   ],
   "source": [
    "mask = diffuse_mnar_single(dat,0.2,0.5,0.5)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033333333333333326\n",
      "0.1 0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0.   0.25 1.  ]\n",
      "0.39999999999999997\n",
      "[False False  True]\n",
      "0.5\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "\n",
      "0\n",
      "[1.         0.         0.02564103]\n",
      "0.22051282051282045\n",
      "[ True False False]\n",
      "0.0\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
