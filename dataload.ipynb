{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data_loaders import *\n",
    "\n",
    "from missing_process.utils_generation import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_NA(X, p_miss, mecha=\"OTselfmask\", p_obs=0.2, q=0.25):\n",
    "    \"\"\"\n",
    "    Generate missing values for specifics missing-data mechanism and proportion of missing values. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
    "        Data for which missing values will be simulated.\n",
    "        If a numpy array is provided, it will be converted to a pytorch tensor.\n",
    "    p_miss : float\n",
    "        Proportion of missing values to generate for variables which will have missing values.\n",
    "    mecha : str, \n",
    "            Indicates the missing-data mechanism to be used. \"MCAR\" by default, \"MAR\", \"MNAR\" or \"MNARsmask\"\n",
    "    opt: str, \n",
    "         For mecha = \"MNAR\", it indicates how the missing-data mechanism is generated: using a logistic regression (\"logistic\"), quantile censorship (\"quantile\") or logistic regression for generating a self-masked MNAR mechanism (\"selfmasked\").\n",
    "    p_obs : float\n",
    "            If mecha = \"MAR\", or mecha = \"MNAR\" with opt = \"logistic\" or \"quanti\", proportion of variables with *no* missing values that will be used for the logistic masking model.\n",
    "    q : float\n",
    "        If mecha = \"MNAR\" and opt = \"quanti\", quantile level at which the cuts should occur.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionnary containing:\n",
    "    'X_init': the initial data matrix.\n",
    "    'X_incomp': the data with the generated missing values.\n",
    "    'mask': a matrix indexing the generated missing values.s\n",
    "    \"\"\"\n",
    "    \n",
    "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "    if not to_torch:\n",
    "        X = X.astype(np.float32)\n",
    "        X = torch.from_numpy(X)\n",
    "    \n",
    "    if mecha == \"MAR\":\n",
    "        print(\"MAR\")\n",
    "        mask = MAR_mask(X, p_miss, p_obs).double()\n",
    "\n",
    "    elif mecha == \"OTlogistic\":\n",
    "        print(\"OTlogistic\")\n",
    "        mask = MNAR_mask_logistic(X, p_miss, p_obs).double()\n",
    "\n",
    "    elif mecha == \"OTquantile\":\n",
    "        print(\"OTquantile\")\n",
    "        mask = MNAR_mask_quantiles(X, p_miss, q, 1-p_obs).double()\n",
    "\n",
    "    elif mecha == \"OTselfmask\":\n",
    "        print(\"OTselfmask\")\n",
    "        mask = MNAR_self_mask_logistic(X, p_miss).double()\n",
    "\n",
    "    else:\n",
    "        print(\"MCAR\")\n",
    "        mask = (torch.rand(X.shape) < p_miss).double()\n",
    "    \n",
    "    X_nas = X.clone()\n",
    "    X_nas[mask.bool()] = np.nan\n",
    "    \n",
    "    return {'X_init': X.double(), 'X_incomp': X_nas.double(), 'mask': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=scale(dataset_loader(\"wine_quality_white\")[\"data\"])\n",
    "\n",
    "\n",
    "to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "if not to_torch:\n",
    "    X = X.astype(np.float32)\n",
    "    X = torch.from_numpy(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4898 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  ...,  True,  True, False],\n",
       "        [False, False,  True,  ..., False, False,  True],\n",
       "        [False, False, False,  ...,  True, False,  True],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False,  True, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False,  True, False]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_logisitc = MNAR_mask_logistic(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 1] [ 0  3  5  6  7  8  9 10]\n",
      "torch.Size([4898, 8]) torch.Size([4898, 8])\n",
      "torch.Size([4898, 11])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True,  ...,  True, False,  True],\n",
       "        [False, False,  True,  ...,  True,  True,  True],\n",
       "        [ True, False,  True,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True, False, False,  ...,  True, False,  True],\n",
       "        [False, False,  True,  ...,  True,  True,  True],\n",
       "        [False, False,  True,  ...,  True, False,  True]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MNAR_mask_logistic(X, p, p_params =0.3, exclude_inputs=True):\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "\n",
    "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "    if not to_torch:\n",
    "        X = torch.from_numpy(X)\n",
    "\n",
    "    mask = torch.zeros(n, d).bool() if to_torch else np.zeros((n, d)).astype(bool)\n",
    "    # control SPlit\n",
    "    d_params = max(int(p_params * d), 1) if exclude_inputs else d ## number of variables used as inputs (at least 1)\n",
    "    d_na = d - d_params if exclude_inputs else d ## number of variables masked with the logistic model\n",
    "    idxs_params = np.random.choice(d, d_params, replace=False) if exclude_inputs else np.arange(d) # 任选三个parameter obs\n",
    "\n",
    "    idxs_nas = np.array([i for i in range(d) if i not in idxs_params]) if exclude_inputs else np.arange(d) # 剩下的 feature miss\n",
    "\n",
    "    coeffs = pick_coeffs(X, idxs_params, idxs_nas)\n",
    "    intercepts = fit_intercepts(X[:, idxs_params], coeffs, p)\n",
    "\n",
    "\n",
    "    ps = torch.sigmoid(X[:, idxs_params].mm(coeffs) + intercepts)\n",
    "    \n",
    "    ber = torch.rand(n, d_na)\n",
    "    print(idxs_params,idxs_nas)\n",
    "    print(ber.shape,ps.shape)\n",
    "    mask[:, idxs_nas] = ber < ps\n",
    "\n",
    "    print(mask.shape)\n",
    "\n",
    "    if exclude_inputs:\n",
    "        mask[:, idxs_params] = torch.rand(n, d_params) < p\n",
    "\n",
    "    return mask\n",
    "def pick_coeffs(X, idxs_obs=None, idxs_nas=None, self_mask=False):\n",
    "    n, d = X.shape\n",
    "    if self_mask:\n",
    "        coeffs = torch.randn(d)\n",
    "        Wx = X * coeffs\n",
    "        coeffs /= torch.std(Wx, 0)\n",
    "    else:\n",
    "        d_obs = len(idxs_obs)\n",
    "        d_na = len(idxs_nas)\n",
    "        coeffs = torch.randn(d_obs, d_na) #dimension\n",
    "        Wx = X[:, idxs_obs].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "    return coeffs\n",
    "\n",
    "MNAR_mask_logistic(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNAR_self_mask_logistic(X, p):\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "    if not to_torch:\n",
    "        X = torch.from_numpy(X)\n",
    "\n",
    "    ### Variables will have NA proportions that depend on those observed variables, through a logistic model\n",
    "    ### The parameters of this logistic model are random.\n",
    "\n",
    "    ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "    coeffs = pick_coeffs(X, self_mask=True)\n",
    "    ### Pick the intercepts to have a desired amount of missing values\n",
    "    intercepts = fit_intercepts(X, coeffs, p, self_mask=True)\n",
    "\n",
    "    ps = torch.sigmoid(X * coeffs + intercepts)\n",
    "\n",
    "    ber = torch.rand(n, d) if to_torch else np.random.rand(n, d)\n",
    "    mask = ber < ps if to_torch else ber < ps.numpy()\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = pick_coeffs(X, self_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_intercepts(X, coeffs, p, self_mask=False):\n",
    "    if self_mask:\n",
    "        d = len(coeffs)\n",
    "        intercepts = torch.zeros(d)\n",
    "        for j in range(d):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X * coeffs[j] + x).mean().item() - p\n",
    "            intercepts[j] = optimize.bisect(f, -50, 50)\n",
    "    else:\n",
    "        d_obs, d_na = coeffs.shape\n",
    "        intercepts = torch.zeros(d_na)\n",
    "        for j in range(d_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X.mv(coeffs[:, j]) + x).mean().item() - p\n",
    "            intercepts[j] = optimize.bisect(f, -50, 50)\n",
    "    return intercepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = len(coeffs)\n",
    "intercepts = torch.zeros(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.4000,  0.7000,  0.0000,  ...,  3.5100,  0.5600,  9.4000],\n",
       "        [ 7.8000,  0.8800,  0.0000,  ...,  3.2000,  0.6800,  9.8000],\n",
       "        [ 7.8000,  0.7600,  0.0400,  ...,  3.2600,  0.6500,  9.8000],\n",
       "        ...,\n",
       "        [ 6.3000,  0.5100,  0.1300,  ...,  3.4200,  0.7500, 11.0000],\n",
       "        [ 5.9000,  0.6450,  0.1200,  ...,  3.5700,  0.7100, 10.2000],\n",
       "        [ 6.0000,  0.3100,  0.4700,  ...,  3.3900,  0.6600, 11.0000]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "f(a) and f(b) must have different signs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m intercepts \u001b[39m=\u001b[39m fit_intercepts(X, coeffs, \u001b[39m0.5\u001b[39;49m, self_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32me:\\Uni\\Deakin\\OneDrive - Deakin University\\MNAR\\deep\\MNAR\\missing_process\\utils_generation.py:406\u001b[0m, in \u001b[0;36mfit_intercepts\u001b[1;34m(X, coeffs, p, self_mask)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x):\n\u001b[0;32m    405\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(X \u001b[39m*\u001b[39m coeffs[j] \u001b[39m+\u001b[39m x)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem() \u001b[39m-\u001b[39m p\n\u001b[1;32m--> 406\u001b[0m         intercepts[j] \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mbisect(f, \u001b[39m-\u001b[39;49m\u001b[39m50\u001b[39;49m, \u001b[39m50\u001b[39;49m)\n\u001b[0;32m    407\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     d_obs, d_na \u001b[39m=\u001b[39m coeffs\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32me:\\ANACONDA\\envs\\py3.10\\lib\\site-packages\\scipy\\optimize\\_zeros_py.py:557\u001b[0m, in \u001b[0;36mbisect\u001b[1;34m(f, a, b, args, xtol, rtol, maxiter, full_output, disp)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[39mif\u001b[39;00m rtol \u001b[39m<\u001b[39m _rtol:\n\u001b[0;32m    556\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mrtol too small (\u001b[39m\u001b[39m%g\u001b[39;00m\u001b[39m < \u001b[39m\u001b[39m%g\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (rtol, _rtol))\n\u001b[1;32m--> 557\u001b[0m r \u001b[39m=\u001b[39m _zeros\u001b[39m.\u001b[39;49m_bisect(f, a, b, xtol, rtol, maxiter, args, full_output, disp)\n\u001b[0;32m    558\u001b[0m \u001b[39mreturn\u001b[39;00m results_c(full_output, r)\n",
      "\u001b[1;31mValueError\u001b[0m: f(a) and f(b) must have different signs"
     ]
    }
   ],
   "source": [
    "intercepts = fit_intercepts(X, coeffs, 0.5, self_mask=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
