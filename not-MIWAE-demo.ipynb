{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not-MIWAE: Deep Generative Modelling with Missing not at Random Data\n",
    "This notebook illustrates how to fit a *deep latent variable model* to data affected by a missing process which depends on the missing data itself, i.e. *missing not at random*.\n",
    "\n",
    "We fit a linear PPCA-like model to a relatively small UCI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('./')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams['font.size'] = 15.0\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['savefig.format'] = 'pdf'\n",
    "plt.rcParams['lines.linewidth'] = 2.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Here we use the white-wine dataset from the UCI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "data1 = np.array(pd.read_csv(url, low_memory=False, sep=';'))\n",
    "# ---- drop the classification attribute\n",
    "data = data1[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
       "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
       "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
       "       ...,\n",
       "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
       "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
       "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1\n",
    "# All numerical and the last column is the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, D = data.shape\n",
    "n_latent = D - 1\n",
    "n_hidden = 128\n",
    "n_samples = 20\n",
    "max_iter = 30000\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- standardize data\n",
    "data = data - np.mean(data, axis=0)\n",
    "data = data / np.std(data, axis=0)\n",
    "\n",
    "# ---- random permutation \n",
    "p = np.random.permutation(N)\n",
    "data = data[p, :]\n",
    "\n",
    "# ---- we use the full dataset for training here, but you can make a train-val split\n",
    "Xtrain = data.copy()\n",
    "Xval = Xtrain.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce missing \n",
    "Here we denote\n",
    "- Xnan: data matrix with np.nan as the missing entries\n",
    "- Xz: data matrix with 0 as the missing entries\n",
    "- S: missing mask \n",
    "\n",
    "The missing process depends on the missing data itself:\n",
    "- in half the features, set the feature value to missing when it is higher than the feature mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- introduce missing process\n",
    "Xnan = Xtrain.copy()\n",
    "Xz = Xtrain.copy()\n",
    "\n",
    "# Selecte half of them, if the number is less than mean then we seen them as missing\n",
    "# (Using np.nan or 0 to represent)\n",
    "mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "ix_larger_than_mean = Xnan[:, :int(D / 2)] > mean\n",
    "\n",
    "Xnan[:, :int(D / 2)][ix_larger_than_mean] = np.nan\n",
    "Xz[:, :int(D / 2)][ix_larger_than_mean] = 0\n",
    "\n",
    "# Mask to indicate if we have missing value\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "The model we are building has a Gaussian prior and a Gaussian observation model,\n",
    "\n",
    "$$ p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} | \\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "$$ p(\\mathbf{x} | \\mathbf{z}) = \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_{\\theta}(\\mathbf{z}), \\sigma^2\\mathbf{I})$$\n",
    "\n",
    "$$ p(\\mathbf{x}) = \\int p(\\mathbf{x} | \\mathbf{z})p(\\mathbf{z}) d\\mathbf{z}$$\n",
    "\n",
    "where $\\mathbf{\\mu}_{\\theta}(\\mathbf{z}): \\mathbb{R}^d \\rightarrow \\mathbb{R}^p $ in general is a deep neural net, but in this case is a linear mapping, $\\mathbf{\\mu} = \\mathbf{Wz + b}$.\n",
    "\n",
    "The variational posterior is also Gaussian\n",
    "\n",
    "$$q_{\\gamma}(\\mathbf{z} | \\mathbf{x}) = \\mathcal{N}(\\mathbf{z} | \\mu_{\\gamma}(\\mathbf{x}), \\sigma_{\\gamma}(\\mathbf{x})^2 \\mathbf{I})$$\n",
    "\n",
    "If the missing process is *missing at random*, it is ignorable and the ELBO becomes, as described in [the MIWAE paper](https://arxiv.org/abs/1812.02633)\n",
    "\n",
    "$$ E_{\\mathbf{z}_1...\\mathbf{z}_K} \\left[ \\log \\frac{1}{K}\\sum_{k=1}^K \\frac{p_{\\theta}(\\mathbf{x^o} | \\mathbf{z}_k)p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z}_k | \\mathbf{x^o})} \\right] $$\n",
    "\n",
    "When the missing process is MNAR it is non-ignorable and we need to include the missing model. In this example we include the missing model as a logistic regression in each feature dimension\n",
    "\n",
    "$$ p_{\\phi}(\\mathbf{s} | \\mathbf{x^o, x^m}) = \\text{Bern}(\\mathbf{s} | \\pi_{\\phi}(\\mathbf{x^o, x^m}))$$\n",
    "\n",
    "$$ \\pi_{\\phi, j}(x_j) = \\frac{1}{1 + e^{-\\text{logits}_j}} $$\n",
    "\n",
    "$$ \\text{logits}_j = W_j (x_j - b_j) $$\n",
    "\n",
    "The ELBO in the MNAR case becomes\n",
    "\n",
    "$$ E_{(\\mathbf{z}_1, \\mathbf{x}_1^m)...(\\mathbf{z}_K, \\mathbf{x}_K^m)} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\phi}(\\mathbf{s} | \\mathbf{x}^o, \\mathbf{x}_k^m) p_{\\theta}(\\mathbf{x}^o | \\mathbf{z}_k) p(\\mathbf{z}_k)}{q_{\\gamma}(\\mathbf{z} | \\mathbf{x}^o)} \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "Let's first define the inputs of the model\n",
    "- x_pl: data input\n",
    "- s_pl: mask input\n",
    "- n_pl: number of importance samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating graph...\")\n",
    "tf.reset_default_graph()\n",
    "# ---- input\n",
    "with tf.variable_scope('input'):\n",
    "    x_pl = tf.placeholder(tf.float32, [None, D], 'x_pl')\n",
    "    s_pl = tf.placeholder(tf.float32, [None, D], 's_pl')\n",
    "    n_pl = tf.placeholder(tf.int32, shape=(), name='n_pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the noise variance is learned as a shared parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- parameters\n",
    "with tf.variable_scope('data_process'):\n",
    "    logstd = tf.get_variable('logstd', shape=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder / inference network consists of two hidden layers with 128 units and tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = keras.layers.Dense(units=n_hidden, activation=tf.nn.tanh, name='l_enc1')(x_pl)\n",
    "x = keras.layers.Dense(units=n_hidden, activation=tf.nn.tanh, name='l_enc2')(x)\n",
    "\n",
    "q_mu = keras.layers.Dense(units=n_latent, activation=None, name='q_mu')(x)\n",
    "\n",
    "q_logstd = keras.layers.Dense(units=n_latent, activation=lambda x: tf.clip_by_value(x, -10, 10),\n",
    "                           name='q_logstd')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_z = tfp.distributions.Normal(loc=q_mu, scale=tf.exp(q_logstd))\n",
    "\n",
    "# ---- sample the latent value\n",
    "l_z = q_z.sample(n_pl)                    # shape [n_samples, batch_size, dl]\n",
    "l_z = tf.transpose(l_z, perm=[1, 0, 2])   # shape [batch_size, n_samples, dl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = keras.layers.Dense(units=D, activation=None, name='mu')(l_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation model / likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_x_given_z = tfp.distributions.Normal(loc=mu, scale=tf.exp(logstd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing model\n",
    "- first mix observed data and samples of missing data\n",
    "- feed through missing model\n",
    "- find likelihood of missing model parameters\n",
    "\n",
    "We have to expand the dimensions of x_pl and s_pl, since mu has size [batch, n_samples, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out_mixed = mu * tf.expand_dims(1 - s_pl, axis=1) + tf.expand_dims(x_pl * s_pl, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.get_variable('W', shape=[1, 1, D])\n",
    "W = -tf.nn.softplus(W)\n",
    "b = tf.get_variable('b', shape=[1, 1, D])\n",
    "\n",
    "logits = W * (l_out_mixed - b)\n",
    "\n",
    "p_s_given_x = tfp.distributions.Bernoulli(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- evaluate the observed data in p(x|z)\n",
    "log_p_x_given_z = tf.reduce_sum(tf.expand_dims(s_pl, axis=1) * \n",
    "                                p_x_given_z.log_prob(tf.expand_dims(x_pl, axis=1)), axis=-1)  # sum over d-dimension\n",
    "\n",
    "# --- evaluate the z-samples in q(z|x)\n",
    "q_z2 = tfp.distributions.Normal(loc=tf.expand_dims(q_z.loc, axis=1), scale=tf.expand_dims(q_z.scale, axis=1))\n",
    "log_q_z_given_x = tf.reduce_sum(q_z2.log_prob(l_z), axis=-1)\n",
    "\n",
    "# ---- evaluate the z-samples in the prior p(z)\n",
    "prior = tfp.distributions.Normal(loc=0.0, scale=1.0)\n",
    "log_p_z = tf.reduce_sum(prior.log_prob(l_z), axis=-1)\n",
    "\n",
    "# ---- evaluate the mask in p(s|x)\n",
    "log_p_s_given_x = tf.reduce_sum(p_s_given_x.log_prob(tf.expand_dims(s_pl, axis=1)), axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses for the MIWAE and not-MIWAE respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lpxz = log_p_x_given_z\n",
    "lpz = log_p_z\n",
    "lqzx = log_q_z_given_x\n",
    "lpsx = log_p_s_given_x\n",
    "\n",
    "# ---- MIWAE\n",
    "# ---- importance weights\n",
    "l_w = lpxz + lpz - lqzx\n",
    "\n",
    "# ---- sum over samples\n",
    "log_sum_w = tf.reduce_logsumexp(l_w, axis=1)\n",
    "\n",
    "# ---- average over samples\n",
    "log_avg_weight = log_sum_w - tf.log(tf.cast(n_pl, tf.float32))\n",
    "\n",
    "# ---- average over minibatch to get the average llh\n",
    "MIWAE = tf.reduce_mean(log_avg_weight, axis=-1)\n",
    "\n",
    "\n",
    "# ---- not-MIWAE\n",
    "# ---- importance weights\n",
    "l_w = lpxz + lpsx + lpz - lqzx\n",
    "\n",
    "# ---- sum over samples\n",
    "log_sum_w = tf.reduce_logsumexp(l_w, axis=1)\n",
    "\n",
    "# ---- average over samples\n",
    "log_avg_weight = log_sum_w - tf.log(tf.cast(n_pl, tf.float32))\n",
    "\n",
    "# ---- average over minibatch to get the average llh\n",
    "notMIWAE = tf.reduce_mean(log_avg_weight, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- training stuff\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "global_step = tf.Variable(initial_value=0, trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose wether you want to train the MIWAE or the notMIWAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = -notMIWAE\n",
    "# loss = -MIWAE\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "train_op = optimizer.minimize(loss, global_step=global_step, var_list=tvars)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30000 updates, 0.25 s, 33.18 train_loss, 32.03 val_loss\n",
      "100/30000 updates, 0.18 s, 30.17 train_loss, 30.32 val_loss\n",
      "200/30000 updates, 0.13 s, 29.65 train_loss, 29.13 val_loss\n",
      "300/30000 updates, 0.13 s, 29.02 train_loss, 27.92 val_loss\n",
      "400/30000 updates, 0.12 s, 26.61 train_loss, 26.77 val_loss\n",
      "500/30000 updates, 0.12 s, 25.84 train_loss, 25.70 val_loss\n",
      "600/30000 updates, 0.13 s, 24.26 train_loss, 24.63 val_loss\n",
      "700/30000 updates, 0.13 s, 22.86 train_loss, 23.63 val_loss\n",
      "800/30000 updates, 0.13 s, 22.96 train_loss, 22.66 val_loss\n",
      "900/30000 updates, 0.12 s, 22.50 train_loss, 21.78 val_loss\n",
      "1000/30000 updates, 0.12 s, 21.30 train_loss, 20.96 val_loss\n",
      "1100/30000 updates, 0.13 s, 20.46 train_loss, 20.20 val_loss\n",
      "1200/30000 updates, 0.13 s, 20.07 train_loss, 19.49 val_loss\n",
      "1300/30000 updates, 0.12 s, 18.76 train_loss, 18.86 val_loss\n",
      "1400/30000 updates, 0.12 s, 19.51 train_loss, 18.28 val_loss\n",
      "1500/30000 updates, 0.12 s, 17.89 train_loss, 17.72 val_loss\n",
      "1600/30000 updates, 0.12 s, 16.92 train_loss, 17.15 val_loss\n",
      "1700/30000 updates, 0.12 s, 16.84 train_loss, 16.64 val_loss\n",
      "1800/30000 updates, 0.12 s, 16.24 train_loss, 16.20 val_loss\n",
      "1900/30000 updates, 0.12 s, 15.39 train_loss, 15.77 val_loss\n",
      "2000/30000 updates, 0.12 s, 15.09 train_loss, 15.41 val_loss\n",
      "2100/30000 updates, 0.12 s, 15.66 train_loss, 15.07 val_loss\n",
      "2200/30000 updates, 0.12 s, 14.16 train_loss, 14.81 val_loss\n",
      "2300/30000 updates, 0.12 s, 14.22 train_loss, 14.55 val_loss\n",
      "2400/30000 updates, 0.12 s, 13.84 train_loss, 14.31 val_loss\n",
      "2500/30000 updates, 0.12 s, 15.34 train_loss, 14.10 val_loss\n",
      "2600/30000 updates, 0.12 s, 15.47 train_loss, 13.91 val_loss\n",
      "2700/30000 updates, 0.12 s, 14.10 train_loss, 13.77 val_loss\n",
      "2800/30000 updates, 0.12 s, 13.36 train_loss, 13.62 val_loss\n",
      "2900/30000 updates, 0.12 s, 13.43 train_loss, 13.45 val_loss\n",
      "3000/30000 updates, 0.12 s, 15.13 train_loss, 13.32 val_loss\n",
      "3100/30000 updates, 0.12 s, 12.39 train_loss, 13.17 val_loss\n",
      "3200/30000 updates, 0.12 s, 13.57 train_loss, 13.05 val_loss\n",
      "3300/30000 updates, 0.12 s, 13.16 train_loss, 12.94 val_loss\n",
      "3400/30000 updates, 0.12 s, 12.51 train_loss, 12.88 val_loss\n",
      "3500/30000 updates, 0.12 s, 12.18 train_loss, 12.80 val_loss\n",
      "3600/30000 updates, 0.12 s, 12.54 train_loss, 12.72 val_loss\n",
      "3700/30000 updates, 0.12 s, 13.44 train_loss, 12.69 val_loss\n",
      "3800/30000 updates, 0.12 s, 12.50 train_loss, 12.61 val_loss\n",
      "3900/30000 updates, 0.12 s, 12.21 train_loss, 12.60 val_loss\n",
      "4000/30000 updates, 0.12 s, 13.02 train_loss, 12.54 val_loss\n",
      "4100/30000 updates, 0.12 s, 12.30 train_loss, 12.48 val_loss\n",
      "4200/30000 updates, 0.12 s, 12.29 train_loss, 12.45 val_loss\n",
      "4300/30000 updates, 0.12 s, 11.72 train_loss, 12.43 val_loss\n",
      "4400/30000 updates, 0.12 s, 12.53 train_loss, 12.37 val_loss\n",
      "4500/30000 updates, 0.12 s, 11.03 train_loss, 12.34 val_loss\n",
      "4600/30000 updates, 0.12 s, 12.16 train_loss, 12.41 val_loss\n",
      "4700/30000 updates, 0.12 s, 12.70 train_loss, 12.35 val_loss\n",
      "4800/30000 updates, 0.12 s, 13.02 train_loss, 12.26 val_loss\n",
      "4900/30000 updates, 0.12 s, 12.30 train_loss, 12.24 val_loss\n",
      "5000/30000 updates, 0.12 s, 12.26 train_loss, 12.24 val_loss\n",
      "5100/30000 updates, 0.12 s, 11.62 train_loss, 12.23 val_loss\n",
      "5200/30000 updates, 0.12 s, 11.90 train_loss, 12.19 val_loss\n",
      "5300/30000 updates, 0.12 s, 12.38 train_loss, 12.16 val_loss\n",
      "5400/30000 updates, 0.12 s, 11.81 train_loss, 12.17 val_loss\n",
      "5500/30000 updates, 0.12 s, 12.07 train_loss, 12.13 val_loss\n",
      "5600/30000 updates, 0.12 s, 11.94 train_loss, 12.14 val_loss\n",
      "5700/30000 updates, 0.12 s, 11.66 train_loss, 12.09 val_loss\n",
      "5800/30000 updates, 0.13 s, 11.61 train_loss, 12.07 val_loss\n",
      "5900/30000 updates, 0.12 s, 11.82 train_loss, 12.04 val_loss\n",
      "6000/30000 updates, 0.12 s, 14.33 train_loss, 12.05 val_loss\n",
      "6100/30000 updates, 0.12 s, 12.65 train_loss, 12.02 val_loss\n",
      "6200/30000 updates, 0.12 s, 13.63 train_loss, 12.02 val_loss\n",
      "6300/30000 updates, 0.12 s, 11.62 train_loss, 12.08 val_loss\n",
      "6400/30000 updates, 0.12 s, 10.86 train_loss, 11.99 val_loss\n",
      "6500/30000 updates, 0.12 s, 12.31 train_loss, 11.97 val_loss\n",
      "6600/30000 updates, 0.12 s, 11.62 train_loss, 11.95 val_loss\n",
      "6700/30000 updates, 0.12 s, 11.03 train_loss, 11.98 val_loss\n",
      "6800/30000 updates, 0.12 s, 12.26 train_loss, 11.90 val_loss\n",
      "6900/30000 updates, 0.12 s, 11.62 train_loss, 11.94 val_loss\n",
      "7000/30000 updates, 0.12 s, 12.13 train_loss, 11.89 val_loss\n",
      "7100/30000 updates, 0.12 s, 12.34 train_loss, 11.91 val_loss\n",
      "7200/30000 updates, 0.12 s, 10.55 train_loss, 11.91 val_loss\n",
      "7300/30000 updates, 0.12 s, 12.29 train_loss, 11.89 val_loss\n",
      "7400/30000 updates, 0.12 s, 11.66 train_loss, 11.90 val_loss\n",
      "7500/30000 updates, 0.12 s, 13.18 train_loss, 11.86 val_loss\n",
      "7600/30000 updates, 0.12 s, 11.25 train_loss, 11.87 val_loss\n",
      "7700/30000 updates, 0.12 s, 10.43 train_loss, 11.84 val_loss\n",
      "7800/30000 updates, 0.12 s, 10.65 train_loss, 11.85 val_loss\n",
      "7900/30000 updates, 0.12 s, 12.02 train_loss, 11.85 val_loss\n",
      "8000/30000 updates, 0.12 s, 12.77 train_loss, 11.86 val_loss\n",
      "8100/30000 updates, 0.12 s, 10.81 train_loss, 11.82 val_loss\n",
      "8200/30000 updates, 0.12 s, 10.96 train_loss, 11.80 val_loss\n",
      "8300/30000 updates, 0.12 s, 10.68 train_loss, 11.82 val_loss\n",
      "8400/30000 updates, 0.12 s, 12.38 train_loss, 11.79 val_loss\n",
      "8500/30000 updates, 0.12 s, 11.56 train_loss, 11.79 val_loss\n",
      "8600/30000 updates, 0.12 s, 13.11 train_loss, 11.78 val_loss\n",
      "8700/30000 updates, 0.12 s, 13.46 train_loss, 11.75 val_loss\n",
      "8800/30000 updates, 0.12 s, 12.21 train_loss, 11.75 val_loss\n",
      "8900/30000 updates, 0.12 s, 10.45 train_loss, 11.78 val_loss\n",
      "9000/30000 updates, 0.12 s, 10.85 train_loss, 11.73 val_loss\n",
      "9100/30000 updates, 0.12 s, 12.58 train_loss, 11.71 val_loss\n",
      "9200/30000 updates, 0.12 s, 12.15 train_loss, 11.72 val_loss\n",
      "9300/30000 updates, 0.12 s, 11.55 train_loss, 11.72 val_loss\n",
      "9400/30000 updates, 0.12 s, 11.55 train_loss, 11.71 val_loss\n",
      "9500/30000 updates, 0.12 s, 12.17 train_loss, 11.68 val_loss\n",
      "9600/30000 updates, 0.12 s, 11.72 train_loss, 11.69 val_loss\n",
      "9700/30000 updates, 0.12 s, 11.71 train_loss, 11.73 val_loss\n",
      "9800/30000 updates, 0.12 s, 10.63 train_loss, 11.71 val_loss\n",
      "9900/30000 updates, 0.12 s, 11.80 train_loss, 11.71 val_loss\n",
      "10000/30000 updates, 0.12 s, 12.49 train_loss, 11.70 val_loss\n",
      "10100/30000 updates, 0.12 s, 10.81 train_loss, 11.68 val_loss\n",
      "10200/30000 updates, 0.12 s, 10.85 train_loss, 11.67 val_loss\n",
      "10300/30000 updates, 0.12 s, 11.46 train_loss, 11.67 val_loss\n",
      "10400/30000 updates, 0.12 s, 11.11 train_loss, 11.69 val_loss\n",
      "10500/30000 updates, 0.12 s, 32.99 train_loss, 11.66 val_loss\n",
      "10600/30000 updates, 0.12 s, 11.83 train_loss, 11.66 val_loss\n",
      "10700/30000 updates, 0.12 s, 12.75 train_loss, 11.64 val_loss\n",
      "10800/30000 updates, 0.12 s, 12.91 train_loss, 11.67 val_loss\n",
      "10900/30000 updates, 0.12 s, 10.95 train_loss, 11.64 val_loss\n",
      "11000/30000 updates, 0.12 s, 12.26 train_loss, 11.66 val_loss\n",
      "11100/30000 updates, 0.12 s, 10.49 train_loss, 11.63 val_loss\n",
      "11200/30000 updates, 0.12 s, 24.81 train_loss, 11.64 val_loss\n",
      "11300/30000 updates, 0.12 s, 11.38 train_loss, 11.68 val_loss\n",
      "11400/30000 updates, 0.12 s, 10.96 train_loss, 11.60 val_loss\n",
      "11500/30000 updates, 0.12 s, 12.06 train_loss, 11.59 val_loss\n",
      "11600/30000 updates, 0.12 s, 11.44 train_loss, 11.63 val_loss\n",
      "11700/30000 updates, 0.12 s, 12.87 train_loss, 11.64 val_loss\n",
      "11800/30000 updates, 0.12 s, 10.99 train_loss, 11.63 val_loss\n",
      "11900/30000 updates, 0.12 s, 11.65 train_loss, 11.68 val_loss\n",
      "12000/30000 updates, 0.12 s, 11.25 train_loss, 11.67 val_loss\n",
      "12100/30000 updates, 0.12 s, 10.87 train_loss, 11.60 val_loss\n",
      "12200/30000 updates, 0.12 s, 11.27 train_loss, 11.59 val_loss\n",
      "12300/30000 updates, 0.12 s, 11.87 train_loss, 11.61 val_loss\n",
      "12400/30000 updates, 0.12 s, 11.50 train_loss, 11.61 val_loss\n",
      "12500/30000 updates, 0.12 s, 11.81 train_loss, 11.62 val_loss\n",
      "12600/30000 updates, 0.12 s, 11.48 train_loss, 11.55 val_loss\n",
      "12700/30000 updates, 0.12 s, 11.22 train_loss, 11.56 val_loss\n",
      "12800/30000 updates, 0.12 s, 11.25 train_loss, 11.58 val_loss\n",
      "12900/30000 updates, 0.12 s, 11.97 train_loss, 11.58 val_loss\n",
      "13000/30000 updates, 0.12 s, 11.48 train_loss, 11.58 val_loss\n",
      "13100/30000 updates, 0.12 s, 11.42 train_loss, 11.57 val_loss\n",
      "13200/30000 updates, 0.12 s, 12.24 train_loss, 11.58 val_loss\n",
      "13300/30000 updates, 0.12 s, 11.39 train_loss, 11.53 val_loss\n",
      "13400/30000 updates, 0.12 s, 10.60 train_loss, 11.56 val_loss\n",
      "13500/30000 updates, 0.12 s, 12.57 train_loss, 11.58 val_loss\n",
      "13600/30000 updates, 0.12 s, 12.23 train_loss, 11.57 val_loss\n",
      "13700/30000 updates, 0.12 s, 11.29 train_loss, 11.53 val_loss\n",
      "13800/30000 updates, 0.12 s, 10.49 train_loss, 11.52 val_loss\n",
      "13900/30000 updates, 0.12 s, 11.70 train_loss, 11.55 val_loss\n",
      "14000/30000 updates, 0.12 s, 10.98 train_loss, 11.54 val_loss\n",
      "14100/30000 updates, 0.12 s, 11.23 train_loss, 11.55 val_loss\n",
      "14200/30000 updates, 0.12 s, 10.97 train_loss, 11.50 val_loss\n",
      "14300/30000 updates, 0.12 s, 11.28 train_loss, 11.56 val_loss\n",
      "14400/30000 updates, 0.12 s, 11.01 train_loss, 11.54 val_loss\n",
      "14500/30000 updates, 0.12 s, 11.41 train_loss, 11.54 val_loss\n",
      "14600/30000 updates, 0.12 s, 10.84 train_loss, 11.55 val_loss\n",
      "14700/30000 updates, 0.12 s, 12.33 train_loss, 11.53 val_loss\n",
      "14800/30000 updates, 0.12 s, 12.37 train_loss, 11.56 val_loss\n",
      "14900/30000 updates, 0.12 s, 11.72 train_loss, 11.55 val_loss\n",
      "15000/30000 updates, 0.12 s, 11.82 train_loss, 11.56 val_loss\n",
      "15100/30000 updates, 0.12 s, 11.38 train_loss, 11.56 val_loss\n",
      "15200/30000 updates, 0.12 s, 10.98 train_loss, 11.50 val_loss\n",
      "15300/30000 updates, 0.12 s, 12.49 train_loss, 11.48 val_loss\n",
      "15400/30000 updates, 0.12 s, 12.69 train_loss, 11.54 val_loss\n",
      "15500/30000 updates, 0.12 s, 11.48 train_loss, 11.57 val_loss\n",
      "15600/30000 updates, 0.12 s, 11.40 train_loss, 11.51 val_loss\n",
      "15700/30000 updates, 0.12 s, 11.18 train_loss, 11.50 val_loss\n",
      "15800/30000 updates, 0.12 s, 10.68 train_loss, 11.50 val_loss\n",
      "15900/30000 updates, 0.12 s, 11.75 train_loss, 11.58 val_loss\n",
      "16000/30000 updates, 0.12 s, 11.01 train_loss, 11.51 val_loss\n",
      "16100/30000 updates, 0.12 s, 11.00 train_loss, 11.56 val_loss\n",
      "16200/30000 updates, 0.12 s, 11.70 train_loss, 11.51 val_loss\n",
      "16300/30000 updates, 0.12 s, 11.11 train_loss, 11.57 val_loss\n",
      "16400/30000 updates, 0.12 s, 11.35 train_loss, 11.57 val_loss\n",
      "16500/30000 updates, 0.12 s, 11.44 train_loss, 11.55 val_loss\n",
      "16600/30000 updates, 0.12 s, 11.76 train_loss, 11.64 val_loss\n",
      "16700/30000 updates, 0.12 s, 11.80 train_loss, 11.50 val_loss\n",
      "16800/30000 updates, 0.12 s, 10.31 train_loss, 11.47 val_loss\n",
      "16900/30000 updates, 0.13 s, 11.67 train_loss, 11.50 val_loss\n",
      "17000/30000 updates, 0.13 s, 11.68 train_loss, 11.50 val_loss\n",
      "17100/30000 updates, 0.12 s, 11.34 train_loss, 11.54 val_loss\n",
      "17200/30000 updates, 0.12 s, 13.07 train_loss, 11.50 val_loss\n",
      "17300/30000 updates, 0.12 s, 11.62 train_loss, 11.49 val_loss\n",
      "17400/30000 updates, 0.12 s, 11.88 train_loss, 11.50 val_loss\n",
      "17500/30000 updates, 0.12 s, 11.60 train_loss, 11.49 val_loss\n",
      "17600/30000 updates, 0.12 s, 11.69 train_loss, 11.48 val_loss\n",
      "17700/30000 updates, 0.12 s, 11.76 train_loss, 11.50 val_loss\n",
      "17800/30000 updates, 0.12 s, 11.62 train_loss, 11.48 val_loss\n",
      "17900/30000 updates, 0.12 s, 11.21 train_loss, 11.57 val_loss\n",
      "18000/30000 updates, 0.12 s, 13.64 train_loss, 11.51 val_loss\n",
      "18100/30000 updates, 0.12 s, 10.73 train_loss, 11.52 val_loss\n",
      "18200/30000 updates, 0.12 s, 11.19 train_loss, 11.46 val_loss\n",
      "18300/30000 updates, 0.12 s, 10.62 train_loss, 11.49 val_loss\n",
      "18400/30000 updates, 0.12 s, 11.03 train_loss, 11.46 val_loss\n",
      "18500/30000 updates, 0.12 s, 11.32 train_loss, 11.49 val_loss\n",
      "18600/30000 updates, 0.12 s, 11.39 train_loss, 11.46 val_loss\n",
      "18700/30000 updates, 0.12 s, 11.70 train_loss, 11.47 val_loss\n",
      "18800/30000 updates, 0.12 s, 10.41 train_loss, 11.61 val_loss\n",
      "18900/30000 updates, 0.12 s, 12.01 train_loss, 11.50 val_loss\n",
      "19000/30000 updates, 0.12 s, 12.16 train_loss, 11.47 val_loss\n",
      "19100/30000 updates, 0.12 s, 11.45 train_loss, 11.47 val_loss\n",
      "19200/30000 updates, 0.12 s, 10.88 train_loss, 11.48 val_loss\n",
      "19300/30000 updates, 0.12 s, 10.49 train_loss, 11.49 val_loss\n",
      "19400/30000 updates, 0.12 s, 11.88 train_loss, 11.44 val_loss\n",
      "19500/30000 updates, 0.12 s, 11.56 train_loss, 11.44 val_loss\n",
      "19600/30000 updates, 0.12 s, 11.19 train_loss, 11.58 val_loss\n",
      "19700/30000 updates, 0.12 s, 10.89 train_loss, 11.45 val_loss\n",
      "19800/30000 updates, 0.12 s, 10.57 train_loss, 11.58 val_loss\n",
      "19900/30000 updates, 0.12 s, 9.99 train_loss, 11.50 val_loss\n",
      "20000/30000 updates, 0.12 s, 10.89 train_loss, 11.47 val_loss\n",
      "20100/30000 updates, 0.12 s, 11.67 train_loss, 11.45 val_loss\n",
      "20200/30000 updates, 0.12 s, 11.04 train_loss, 11.44 val_loss\n",
      "20300/30000 updates, 0.12 s, 12.24 train_loss, 11.45 val_loss\n",
      "20400/30000 updates, 0.12 s, 10.75 train_loss, 11.46 val_loss\n",
      "20500/30000 updates, 0.12 s, 11.91 train_loss, 11.43 val_loss\n",
      "20600/30000 updates, 0.12 s, 10.78 train_loss, 11.46 val_loss\n",
      "20700/30000 updates, 0.12 s, 12.25 train_loss, 11.44 val_loss\n",
      "20800/30000 updates, 0.12 s, 10.91 train_loss, 11.47 val_loss\n",
      "20900/30000 updates, 0.12 s, 12.65 train_loss, 11.43 val_loss\n",
      "21000/30000 updates, 0.12 s, 11.42 train_loss, 11.48 val_loss\n",
      "21100/30000 updates, 0.12 s, 11.17 train_loss, 11.46 val_loss\n",
      "21200/30000 updates, 0.12 s, 11.21 train_loss, 11.45 val_loss\n",
      "21300/30000 updates, 0.12 s, 10.87 train_loss, 11.46 val_loss\n",
      "21400/30000 updates, 0.12 s, 11.76 train_loss, 11.42 val_loss\n",
      "21500/30000 updates, 0.12 s, 10.76 train_loss, 11.50 val_loss\n",
      "21600/30000 updates, 0.12 s, 13.50 train_loss, 11.43 val_loss\n",
      "21700/30000 updates, 0.12 s, 11.70 train_loss, 11.45 val_loss\n",
      "21800/30000 updates, 0.12 s, 10.93 train_loss, 11.45 val_loss\n",
      "21900/30000 updates, 0.12 s, 12.54 train_loss, 11.42 val_loss\n",
      "22000/30000 updates, 0.12 s, 12.00 train_loss, 11.42 val_loss\n",
      "22100/30000 updates, 0.12 s, 11.16 train_loss, 11.49 val_loss\n",
      "22200/30000 updates, 0.12 s, 11.66 train_loss, 11.44 val_loss\n",
      "22300/30000 updates, 0.12 s, 11.08 train_loss, 11.48 val_loss\n",
      "22400/30000 updates, 0.12 s, 12.54 train_loss, 11.46 val_loss\n",
      "22500/30000 updates, 0.12 s, 11.24 train_loss, 11.44 val_loss\n",
      "22600/30000 updates, 0.12 s, 10.98 train_loss, 11.47 val_loss\n",
      "22700/30000 updates, 0.12 s, 12.40 train_loss, 11.41 val_loss\n",
      "22800/30000 updates, 0.12 s, 11.27 train_loss, 11.44 val_loss\n",
      "22900/30000 updates, 0.12 s, 11.32 train_loss, 11.41 val_loss\n",
      "23000/30000 updates, 0.12 s, 10.68 train_loss, 11.47 val_loss\n",
      "23100/30000 updates, 0.12 s, 11.81 train_loss, 11.48 val_loss\n",
      "23200/30000 updates, 0.12 s, 11.51 train_loss, 11.45 val_loss\n",
      "23300/30000 updates, 0.12 s, 11.27 train_loss, 11.45 val_loss\n",
      "23400/30000 updates, 0.12 s, 10.53 train_loss, 11.45 val_loss\n",
      "23500/30000 updates, 0.12 s, 10.88 train_loss, 11.51 val_loss\n",
      "23600/30000 updates, 0.12 s, 11.17 train_loss, 11.44 val_loss\n",
      "23700/30000 updates, 0.12 s, 10.56 train_loss, 11.45 val_loss\n",
      "23800/30000 updates, 0.12 s, 11.70 train_loss, 11.43 val_loss\n",
      "23900/30000 updates, 0.12 s, 11.76 train_loss, 11.42 val_loss\n",
      "24000/30000 updates, 0.12 s, 11.04 train_loss, 11.43 val_loss\n",
      "24100/30000 updates, 0.12 s, 11.75 train_loss, 11.49 val_loss\n",
      "24200/30000 updates, 0.12 s, 12.46 train_loss, 11.40 val_loss\n",
      "24300/30000 updates, 0.12 s, 11.69 train_loss, 11.50 val_loss\n",
      "24400/30000 updates, 0.12 s, 11.88 train_loss, 11.41 val_loss\n",
      "24500/30000 updates, 0.12 s, 11.49 train_loss, 11.45 val_loss\n",
      "24600/30000 updates, 0.12 s, 11.07 train_loss, 11.46 val_loss\n",
      "24700/30000 updates, 0.12 s, 11.38 train_loss, 11.48 val_loss\n",
      "24800/30000 updates, 0.12 s, 10.89 train_loss, 11.42 val_loss\n",
      "24900/30000 updates, 0.12 s, 10.81 train_loss, 11.41 val_loss\n",
      "25000/30000 updates, 0.12 s, 11.42 train_loss, 11.45 val_loss\n",
      "25100/30000 updates, 0.12 s, 11.56 train_loss, 11.44 val_loss\n",
      "25200/30000 updates, 0.12 s, 11.44 train_loss, 11.40 val_loss\n",
      "25300/30000 updates, 0.12 s, 12.90 train_loss, 11.54 val_loss\n",
      "25400/30000 updates, 0.12 s, 10.91 train_loss, 11.39 val_loss\n",
      "25500/30000 updates, 0.12 s, 11.07 train_loss, 11.45 val_loss\n",
      "25600/30000 updates, 0.12 s, 11.57 train_loss, 11.45 val_loss\n",
      "25700/30000 updates, 0.12 s, 11.46 train_loss, 11.39 val_loss\n",
      "25800/30000 updates, 0.12 s, 10.97 train_loss, 11.39 val_loss\n",
      "25900/30000 updates, 0.12 s, 10.93 train_loss, 11.42 val_loss\n",
      "26000/30000 updates, 0.12 s, 11.84 train_loss, 11.39 val_loss\n",
      "26100/30000 updates, 0.12 s, 12.31 train_loss, 11.39 val_loss\n",
      "26200/30000 updates, 0.12 s, 11.83 train_loss, 11.39 val_loss\n",
      "26300/30000 updates, 0.12 s, 10.81 train_loss, 11.39 val_loss\n",
      "26400/30000 updates, 0.12 s, 11.99 train_loss, 11.46 val_loss\n",
      "26500/30000 updates, 0.12 s, 10.73 train_loss, 11.42 val_loss\n",
      "26600/30000 updates, 0.12 s, 11.37 train_loss, 11.43 val_loss\n",
      "26700/30000 updates, 0.12 s, 10.43 train_loss, 11.42 val_loss\n",
      "26800/30000 updates, 0.12 s, 11.96 train_loss, 11.57 val_loss\n",
      "26900/30000 updates, 0.12 s, 11.35 train_loss, 11.43 val_loss\n",
      "27000/30000 updates, 0.12 s, 11.15 train_loss, 11.39 val_loss\n",
      "27100/30000 updates, 0.12 s, 10.69 train_loss, 11.39 val_loss\n",
      "27200/30000 updates, 0.12 s, 11.86 train_loss, 11.41 val_loss\n",
      "27300/30000 updates, 0.12 s, 12.18 train_loss, 11.42 val_loss\n",
      "27400/30000 updates, 0.12 s, 10.63 train_loss, 11.42 val_loss\n",
      "27500/30000 updates, 0.12 s, 11.29 train_loss, 11.41 val_loss\n",
      "27600/30000 updates, 0.12 s, 12.07 train_loss, 11.41 val_loss\n",
      "27700/30000 updates, 0.12 s, 11.95 train_loss, 11.37 val_loss\n",
      "27800/30000 updates, 0.12 s, 10.69 train_loss, 11.37 val_loss\n",
      "27900/30000 updates, 0.12 s, 10.65 train_loss, 11.40 val_loss\n",
      "28000/30000 updates, 0.12 s, 11.32 train_loss, 11.42 val_loss\n",
      "28100/30000 updates, 0.12 s, 11.89 train_loss, 11.39 val_loss\n",
      "28200/30000 updates, 0.12 s, 10.40 train_loss, 11.39 val_loss\n",
      "28300/30000 updates, 0.12 s, 11.02 train_loss, 11.43 val_loss\n",
      "28400/30000 updates, 0.12 s, 11.80 train_loss, 11.62 val_loss\n",
      "28500/30000 updates, 0.12 s, 11.58 train_loss, 11.62 val_loss\n",
      "28600/30000 updates, 0.12 s, 10.77 train_loss, 11.44 val_loss\n",
      "28700/30000 updates, 0.12 s, 10.62 train_loss, 11.40 val_loss\n",
      "28800/30000 updates, 0.12 s, 11.36 train_loss, 11.40 val_loss\n",
      "28900/30000 updates, 0.12 s, 10.73 train_loss, 11.39 val_loss\n",
      "29000/30000 updates, 0.12 s, 11.51 train_loss, 11.64 val_loss\n",
      "29100/30000 updates, 0.12 s, 11.76 train_loss, 11.41 val_loss\n",
      "29200/30000 updates, 0.12 s, 11.38 train_loss, 11.40 val_loss\n",
      "29300/30000 updates, 0.12 s, 15.21 train_loss, 11.38 val_loss\n",
      "29400/30000 updates, 0.12 s, 10.79 train_loss, 11.37 val_loss\n",
      "29500/30000 updates, 0.12 s, 11.80 train_loss, 11.38 val_loss\n",
      "29600/30000 updates, 0.12 s, 11.19 train_loss, 11.41 val_loss\n",
      "29700/30000 updates, 0.12 s, 11.30 train_loss, 11.42 val_loss\n",
      "29800/30000 updates, 0.12 s, 11.39 train_loss, 11.40 val_loss\n",
      "29900/30000 updates, 0.12 s, 11.08 train_loss, 11.39 val_loss\n"
     ]
    }
   ],
   "source": [
    "batch_pointer = 0\n",
    "\n",
    "start = time.time()\n",
    "best = float(\"inf\")\n",
    "\n",
    "\n",
    "for i in range(max_iter):\n",
    "    x_batch = Xz[batch_pointer: batch_pointer + batch_size, :]\n",
    "    s_batch = S[batch_pointer: batch_pointer + batch_size, :]\n",
    "\n",
    "    _, _loss, _step = sess.run([train_op, loss, global_step], {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples})\n",
    "\n",
    "    batch_pointer += batch_size\n",
    "    \n",
    "    if batch_pointer > N - batch_size:\n",
    "        batch_pointer = 0\n",
    "\n",
    "        p = np.random.permutation(N)\n",
    "        Xz = Xz[p, :]\n",
    "        S = S[p, :]\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        took = time.time() - start\n",
    "        start = time.time()\n",
    "        \n",
    "        # --- change the following batch if you want a true validation set\n",
    "        x_batch = Xz  \n",
    "        s_batch = S\n",
    "        \n",
    "        val_loss, _step = sess.run([loss, global_step], {x_pl: x_batch, s_pl: s_batch, n_pl: n_samples})\n",
    "\n",
    "        print(\"{0}/{1} updates, {2:.2f} s, {3:.2f} train_loss, {4:.2f} val_loss\".format(i, max_iter, took, _loss, val_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single imputation RMSE\n",
    "The *self-normalized importance sampling* approach for the MIWAE is described in this [paper](https://arxiv.org/pdf/1812.02633.pdf). This needs to be modified slightly in the MNAR case to account for the missing model, as described in the not-MIWAE paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imputationRMSE(sess, Xorg, Xnan, L):\n",
    "\n",
    "    N = len(Xorg)\n",
    "    \n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan)] = 0\n",
    "    S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1)[:, None])\n",
    "        return e_x / e_x.sum(axis=1)[:, None]\n",
    "\n",
    "    def imp(xz, s, L):\n",
    "        _mu, _log_p_x_given_z, _log_p_z, _log_q_z_given_x = sess.run(\n",
    "            [mu, log_p_x_given_z, log_p_z, log_q_z_given_x],\n",
    "            {x_pl: xz, s_pl: s, n_pl: L})\n",
    "\n",
    "        wl = softmax(_log_p_x_given_z + _log_p_z - _log_q_z_given_x)\n",
    "\n",
    "        xm = np.sum((_mu.T * wl.T).T, axis=1)\n",
    "        xmix = xz + xm * (1 - s)\n",
    "\n",
    "        return _mu, wl, xm, xmix\n",
    "\n",
    "    XM = np.zeros_like(Xorg)\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        xz = Xz[i, :][None, :]\n",
    "        s = S[i, :][None, :]\n",
    "\n",
    "        _mu, wl, xm, xmix = imp(xz, s, L)\n",
    "\n",
    "        XM[i, :] = xm\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('{0} / {1}'.format(i, N))\n",
    "\n",
    "    return np.sqrt(np.sum((Xorg - XM) ** 2 * (1 - S)) / np.sum(1 - S)), XM\n",
    "\n",
    "\n",
    "def not_imputationRMSE(sess, Xorg, Xnan, L):\n",
    "\n",
    "    N = len(Xorg)\n",
    "    \n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan)] = 0\n",
    "    S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1)[:, None])\n",
    "        return e_x / e_x.sum(axis=1)[:, None]\n",
    "\n",
    "    def imp(xz, s, L):\n",
    "        _mu, _log_p_x_given_z, _log_p_z, _log_q_z_given_x, _log_p_s_given_x  = sess.run(\n",
    "            [mu, log_p_x_given_z, log_p_z, log_q_z_given_x, log_p_s_given_x],\n",
    "            {x_pl: xz, s_pl: s, n_pl: L})\n",
    "\n",
    "        wl = softmax(_log_p_x_given_z + _log_p_s_given_x + _log_p_z - _log_q_z_given_x)\n",
    "\n",
    "        xm = np.sum((_mu.T * wl.T).T, axis=1)\n",
    "        xmix = xz + xm * (1 - s)\n",
    "\n",
    "        return _mu, wl, xm, xmix\n",
    "\n",
    "    XM = np.zeros_like(Xorg)\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        xz = Xz[i, :][None, :]\n",
    "        s = S[i, :][None, :]\n",
    "\n",
    "        _mu, wl, xm, xmix = imp(xz, s, L)\n",
    "\n",
    "        XM[i, :] = xm\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('{0} / {1}'.format(i, N))\n",
    "\n",
    "    return np.sqrt(np.sum((Xorg - XM) ** 2 * (1 - S)) / np.sum(1 - S)), XM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the single imputation RMSE using 10k importance samples\n",
    "If you used the MIWAE loss use the imputationRMSE \n",
    "\n",
    "If you used the notMIWAE loss use the not_imputationRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4898\n",
      "100 / 4898\n",
      "200 / 4898\n",
      "300 / 4898\n",
      "400 / 4898\n",
      "500 / 4898\n",
      "600 / 4898\n",
      "700 / 4898\n",
      "800 / 4898\n",
      "900 / 4898\n",
      "1000 / 4898\n",
      "1100 / 4898\n",
      "1200 / 4898\n",
      "1300 / 4898\n",
      "1400 / 4898\n",
      "1500 / 4898\n",
      "1600 / 4898\n",
      "1700 / 4898\n",
      "1800 / 4898\n",
      "1900 / 4898\n",
      "2000 / 4898\n",
      "2100 / 4898\n",
      "2200 / 4898\n",
      "2300 / 4898\n",
      "2400 / 4898\n",
      "2500 / 4898\n",
      "2600 / 4898\n",
      "2700 / 4898\n",
      "2800 / 4898\n",
      "2900 / 4898\n",
      "3000 / 4898\n",
      "3100 / 4898\n",
      "3200 / 4898\n",
      "3300 / 4898\n",
      "3400 / 4898\n",
      "3500 / 4898\n",
      "3600 / 4898\n",
      "3700 / 4898\n",
      "3800 / 4898\n",
      "3900 / 4898\n",
      "4000 / 4898\n",
      "4100 / 4898\n",
      "4200 / 4898\n",
      "4300 / 4898\n",
      "4400 / 4898\n",
      "4500 / 4898\n",
      "4600 / 4898\n",
      "4700 / 4898\n",
      "4800 / 4898\n",
      "imputation RMSE:  1.033732313264911\n"
     ]
    }
   ],
   "source": [
    "# ---- S has been permuted during training, so just reinstantiate it\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "\n",
    "rmse, imputations = not_imputationRMSE(sess, Xtrain, Xnan, 10000)\n",
    "# rmse, imputations = imputationRMSE(sess, Xtrain, Xnan, 10000)\n",
    "\n",
    "print(\"imputation RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to missForest and MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ANACONDA\\envs\\py3.10\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "estimator = RandomForestRegressor(n_estimators=100)\n",
    "imp = IterativeImputer(estimator=estimator)\n",
    "imp.fit(Xnan)\n",
    "Xrec = imp.transform(Xnan)\n",
    "rmse_mf = np.sqrt(np.sum((Xtrain - Xrec) ** 2 * (1 - S)) / np.sum(1 - S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missForst imputation RMSE:  1.6231673917045206\n"
     ]
    }
   ],
   "source": [
    "print(\"missForst imputation RMSE: \", rmse_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = IterativeImputer(max_iter=100)\n",
    "imp.fit(Xnan)\n",
    "Xrec = imp.transform(Xnan)\n",
    "RMSE_iter = np.sqrt(np.sum((Xtrain - Xrec) ** 2 * (1 - S)) / np.sum(1 - S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICE, imputation RMSE 1.4102763723316905\n"
     ]
    }
   ],
   "source": [
    "print(\"MICE, imputation RMSE\", RMSE_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the learned missing model\n",
    "There is a separate missing process in each feature dimesion, inspect each of them, plot as function of feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAG6CAYAAAAVhXJkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABpj0lEQVR4nO3deXxU1d0/8M+dfSaTSSZ7AgmEAGEP4C7Ijgt9UNSqjyKCP5diy6P+RKtYFeWh4lIff1qppT6urVZt0UpdEJBFBVGUJayyJCyB7NskM5NZz++PJHfmZrJMdoZ83q9XXp17zp07Z+DV+OHcc79HEkIIEBEREVGrVL09ACIiIqJIwNBEREREFAaGJiIiIqIwMDQRERERhYGhiYiIiCgMDE1EREREYWBoIiIiIgoDQ1MXEkLAZrOBpa+IiIjOPQxNXaimpgYxMTGoqanp7aEQERFRF2NoIiIiIgoDQxMRERFRGBiaiIiIiMLA0EREREQUBoYmIiIiojAwNBERERGFgaGJiIiIKAwMTURERERhYGgiIiIiCkNEhqa6ujr87ne/g06nw4IFCzp8Hb/fj5dffhk5OTkwmUxISEjAtddei927d3fZWImIiOjcEHGhadOmTRgzZgxeffVVeDyeDl/H7/fjxhtvxOLFi7Fw4UKUlpZi+/btqKmpwUUXXYR169Z14aiJiIgo0kVUaPr73/+Oa6+9Fvfffz9eeOGFTl1r1apVWL16Ne6//37cc889iIqKwuDBg/GPf/wDUVFRmDdvHveQIyIiIllEhabMzEwcOHAAv/71ryFJUqeu9Yc//AEAcPfddyvarVYrbrzxRpSUlOCtt97q1GcQERHRuSOiQtPFF1+MtLS0Tl9n3759yMvLQ2JiIoYMGRLSP3HiRADAmjVrOv1ZREREdG7Q9PYAesOePXsA1M9cNWfQoEGK83rT3x5+DHVFQ3t7GGcp0SXnSPI5wbOXAhBCeax4LQAR/F4R1C4a2ut/JOEPup6/4dgfeC38AHyA8EGCD0L4IAkvAC+E8AHwQhIeAG4I4QaEG0Jyw486CFEHn+SGXyXgU0vwqiV4VRI8GtT/r1YFl1aCS6uCW6NCnU6Cw6CGQ6+Gw6iGXa+GR6eGBAkSVIAESFBBJamgggqSpIJaUkMtaaBRaaBVaet/1FqYtEaYNEaYdSZE641INMWif0w80qLjEWuIhVVvRYw+ptOzwkREZ4s+GZqKiooAAHFxcc32W61WAEBpaSn8fj9UquYn5FwuF1wul3xss9m6eKSA1+GHy9i/y69L5xDhh8ZbB73Xjmh3DXR1Nug8NdC5bTDUVcJYVwaDsxwGV3lQyAuo0wIVZqAyGqg0SyiPBs7ESyiIl3A6AXAYGkKPr/1DU8OAOH0q0s39kZ0wEMPjh2B88nhkRGcwTBFRxOmTocnhcAAAdDpds/16vV5xrtlsbva8FStW4Kmnnur6ARK1h6SCV2uCV2tCnTGx5dP8PhjrymCuPQ1zbQHMtQWIri2A3l2NtEogrRKAYuasXmUUkJciYf+A+p/jSYBQhRd4fKhDqSsfpa587Cz/Rm6P1cXj4rQLcEHKBZiSPgVJpqQOfHEiop7VJ0OTyWQCALjd7mb7g2ePGs9tzpIlS/DAAw/IxzabDenp6V00yobPTzHBd3hXl16zr2juP+sipLet//hLTc6RGg6bvl+CCG6TJNQvGZQCfZIEQN1wrAIkFQTUgKSGgBpCqv8BNBAqbZjfMnxCpYbDlAyHKRklSePldn1dBeIqDyGu8hCslT9D56lVvM9qB847JnDesfo/vVq9hP3pGmwdrsGObD982vZPQVW5y7H2+FqsPb4WT3+/ArMyr8K8EfMwPH54574kEVE36pOhKSUlBQBQUVHRbH9lZSUAICkpqcVbc0D9jFTwrFR3uPF3j3Xr9akX+P2AzwV4G3+cgMcJeByApw5w2yHqKuB32uBz2OF12OG118Bjr4XXbofHWQeP3Qm30wOX3wS3iEKd34w6vwVOfwwc/lg4/LFw+mPqQ1kbXIY4FKZeisLUSwEA0bWnkFz0A1KKfwgJUABgdglcdNSDi456IH1rhf+KX6Bi2gwUmA04VV2OM7ZylDgqUWwvRbHjDDyqUqi0FVDpKiCpQmur+YQX/877N/6d929ckHIB5o+Yj0n9J/H2HRGddfpkaBo7diwAID8/v9n+vLw8AMCYMWN6akjUl6hUgMoIaI0tntI4J6UG0PxNZABeN1BbBNjOANUFQEU+UH4YKDsMlB2F32VHrS8eNl+y/FPpTUeZdyBsvpQWP7vGnI6awek4NuR69Iuxo7/rIMy7v4S/8EzIuaKyEtL7f0P8+39DxsSJSH7kYegHTwv0C4EiWx0OFdYgt6ACa4/swlFbLtSmfKhN+VBpHIrr7SjagR1FOzBzwEwsn7AcJm3LM71ERD2tT4amkSNHYtCgQcjLy8PRo0cxePBgRf/WrVsBAFdffXVvDI8oPBodEJtR/9OUEFBVn4Ll9E+wFPwIFPwIFH4EeOsAAC6/CeXeASj2DEWBawzOuEfCC33TS6CgKgoFOB/Rl07E+RNjkFJzAM7vv0fNpk0QDmXgsX/7LfKu/R7xd96BhIULodLrIUkSUmOMSI0xYuqwJNw3YxiKqq/F+gNF+HzfGfxwcjt0cd9CYz6iuNb6E+uRX52Pl6a+hAxLM9+PiKgXSEKIcJ7bPuu89dZbuP322zF//vwWi1Du3bsXCxcuxOTJk/H0008r+v785z/jnnvuwUMPPYTnnntObq+qqkJWVhY0Gg2OHj2K6OjosMdks9kQExOD6upqWCyWDn0vom7jdQMntgI/fwEc/gKoOhnoEloUubNxyj0Oh52TUOtPaPYSiRnRmHD9YKT008L26aeo/OBDuA4eDDlPOyADqU8+iahLLml1SDtPVmL5pwewu/gQtHHfQhuzC5IUWCMVrYvGc5Oew8R+Ezv4pYmIus45HZoWLVqElStXAgDKysoQHx8v9/n9ftxwww1Ys2YNXnnlFdx6660oKirCPffcg82bN+PTTz/F5Zdf3q4xMTRRxBACKD0E5H4A7HwHcJTLXX6hQoF7DA66rkBe3YXw+0PX9Q0cHY8JNwxBTKIRdfv2ofSPf4T9629CzrPecjOSf/c7SOqW11YJIfD53iI8s/YgTjsOw9j/r1BpA+U7JEi4/7z78X9G/Z9Ofmkios6JuNDU2uLQN998EwsWLJCPN2zYgBtvvBGTJ0/GRx99FPJen8+HV155Ba+//jqOHDkCk8mEyy67DEuXLsW4cePaPTaGJopInjrgwL+AH/4CnP5J0VXnj8Zu/zzsqZoBr1f5/x+tQY0ZC0Zg0NhECCFQs3Ytip5+Gr7SMsV5ltmzkbbiaUia1lcDuLw+/PafuViz92cY+r8Ljem4ov/piU9jdtbsDn9NIqLOirjQdDZjaKKId+I7YO0jQOFuRXOtLw4/mP4bB0+mhRRZH3/FAFx0zSCoVBJ8NhtKXnwRVe9/oKioHn3Vlej33HOQtK2XUvD5BX77z1ys3nkc+uRPoYvbLveZtWb88+p/op+5X6e/JhFRRzA0dSGGJjon+P3A7r8BG54CHMpZo/LkX+LrmrtxJs+uaO8/zIrL7xgJY3T9s341Gzfi9H33Q3gCJQaiZ85AvxdegNRCUdnAxws8+vFevL/jFHQJ66FP/EruG580Hm9c8QbUqrZLKRARdbWI2rCXiHqASgWMvw34r5+Ai+5RdMUX/xPX6BZi3ARlKYCCQ5X48OkdqCyqD1PR06ah/59WKgJSzfoNKLj3PviDisc2//ESnr52NOZdPADusmnwOQbIfTtLduLN/W929hsSEXUIQxMRNc8YC1z1DHDTu4AusJWQqioPl568Dlf+wg2tPjDjU1vpwpqXd6O2sj4UmS+7DOmr/gzJYAics3kzip9e0eZHq1QSll0zEr8Y3R/OMzdC+ALha+WulThQfqALviARUfswNBFR64b/B3DnV0DcoECbx46sXbfihrk+WFMCs061FS78+4+74XLU35aLuuQSpP9lFaSg7YiqPvgAtQ210FojSRKemD0CJikJdcWBmmle4cXDXz8Mp9fZBV+OiCh8DE1E1LakYcBdG4HBMwJtfg+sG+bhunlaxKVFyc0VZ+z47E+58Lrr6y1FXXgh0l/5o+JyhY89Dl9t6BYtTSVbDLhvxhB4q8+DxzZabj9uO46Xd77cyS9FRNQ+DE1EFB6jFbjlQ2DMfwba3LUwfHQTZt8aC7M1UFG88Gg11r2+H36fHwAQdemlsN5yi9zvLSxEybOBorKtWXBpJrISzagrmgO/J/CAxYc/f4iquqrOfScionZgaCKi8KnUwDWvAINnBtrspTD/+0bMviMd+qhALab8PWX4+v3D8nHS4geg7d9fPq76xz9Q+23bt+l0GhWeunoU4IuCqzhQp8ntd+OTY5908gsREYWPoYmI2ketBW58G+h3XqCt8jjiNtyC/7hrMDS6wK+V/d+cQd7uUgCAKioKqb//veJShY8/Dl9NTZsfOXFIAmaNToG3ZgT8nhi5/R+H/wFWTSGinsLQRETtp4sCbvkHED8k0Fa0FymHluPKu0cjuPj+lr//HFgYftGFobfpnns+rI987BcjYNTq4Km6QG47YTuBH4p+6Nx3ISIKE0MTEXVMVDww7yMgOjXQtvtdDND9iLEzM+QmR7UbW1cflY+bu03n3L+/zY9LizVi4eQseKougBCBX10f/vxhJ78IEVF4GJqIqONiM4Ab3gIQNLX07/tw4XQrYpKMctPBrYU4dbACQPO36Srefjusj7vpgnTAFwNvzXC5bePJjShzlrXyLiKirsHQRESdk3ExcPGvA8c1hdBsfAzT5g1TnLbpb4fgcTWUIbjoQkRNmCD32T7/Ap7i4jY/KiXGgEuz4uGpukhu8wovPj7ycSe/BBFR2xiaiKjzpj0GxGUFjve8hzTxA0ZNCmyuW1Neh+2fHJOP4xbMD5zv9aLy3ffC+qhrxvaDzz4Yfnec3PbPw/+Ez+/r+PiJiMLA0EREnaczAXP+hKa36S65Kl5Rvyl3UwGK8qoBAFETJ0I3OBC0qj74AH6Ho82PunJUCnQaDTyVgdmmM/Yz2HZmW+e/BxFRKxiaiKhrZFwMXPKbwHFNIXSbH8eUuUG36QSw7aP6ReGSJCHuttvkLl91Nao/abvuksWgxczhyfBUnwchAnvffXiYC8KJqHsxNBFR15n6u5DbdAPiTmLIBclyU+HRapw5WgUAiLn6aqitVrmv4u13IPz+Nj/mmrFpED4zvLZRctvXBV+jyF7U+e9ARNQChiYi6jrybbogW57DBb8YqLhzt3PtCQCAymCA9eab5Xb38eOo3bKlzY+Zkp2EGKNWcYvOL/xYm7+2U8MnImoNQxMRda2Mi4HsWYHjnz+HVRxF1rhEuenEvnKUnqqvBG695WZIWq3cV/H2O21+hE6jwqzRqfA5M+H3xMrtO4p3dH78REQtYGgioq43+WHl8eZncd6VAxVNO7+sn23SJCTAMjuwp5xj+3bUHTzY5kdcO64fAAk+e6bctqt4F5+iI6Juw9BERF0vbSww9KrA8c+fIVGbh4wRgTIBx34qQVVJ/dNycfPnK95e8c5f2/yI8wdY0S/WCJ9jkNxW46nB4crDrbyLiKjjGJqIqHtMaTLbtOU5nHfVAPlQCGDXupMAAEP2UJguuVjuq9mwAcLjafXyKpWEq8emwevIVLT/VPxTJwdORNQ8hiYi6h5p45SzTYc+RWrUCaQMigk0fVeI2koXACBm9tVyu7+mBo6fdrb5EXPG9oPwxMPviZbbfiz+sQsGT0QUiqGJiLpPk9km6evnFbNNfp/A7q/qZ5vMkycBUuARu9pNG9u8fHZKNDLiohS36H4q/glCiM6OnIgoBEMTEXWftHHA0CsDx4c+xYD404jvZ5ab9n99GnV2DzTx8TCOHSu312zcFFb4GZ8RC1/QLboqVxWOVR1r5R1ERB3D0ERE3avJk3TSd3/EeVcGZpu8bj+O7SwBAJinTZXbPadOwX2s7fAzfoBVEZoArmsiou7B0ERE3avfeGDwzMDxgTXIGq6FMTpQm+nwD8UAgOipUxVvrdm4qc3Lj8+wwu9Ogt8bJbdxXRMRdQeGJiLqfuffHnjtc0F18CMMHp8kN505UoWaijrosrKgzciQ22s3tR2ahqVEw6jVwOcYKLdxXRMRdQeGJiLqfkMuB6ICFcGx610MvShFccqRHcWQJAnRU6fIbc7du+EtL2/10hq1CjnpMYpbdKXOUpysOdkVIycikjE0EVH3U2uBMTcFjs/sRHLUaVgSDHLT4R31t+jMU6cFzhMCtZvb3otufIZV8QQdwHVNRNT1GJqIqGeMu1VxKO1+F0MuSJaPywtqUX6mFqbzxkNlscjttZvDXNfkSoHwBULYj0Vc10REXYuhiYh6RtJwIG184Dj3Aww9L15xypEfiiFptTBfdpncVvvtVvhdrlYvPS4jFoAqZF0TEVFXYmgiop4zbm7gtb0UcbVbkZAeqNl0eEcxhBCK0gPC6YTj++9bvWy8WY+B8SZ4g27RnbGfwZnaM103diLq8xiaiKjnjLoeUOsDx7uUt+hqyutQlGern2nSaALtG9uuDl6/ron1moio+zA0EVHPMVqB4bMDx0e+xJARakAKavqhCGqLBabzz5fbajdtbrOEwPgBVvjr0iD8OrmN9ZqIqCsxNBFRzwq+Ref3Ivrkx0gbHCs3Hd1ZAp/Pj+igW3Te4mLUHTjQ6mXHZ1gBqOFzBKqNczE4EXUlhiYi6lmZkwFL/8DxrncxNOgWnbPGg4KDlTBPmaJ4m2PHjlYvm50SjSidWnGL7mTNSdjcti4ZNhERQxMR9SyVGhh7c+C49CCy+pdApQ7cozu8owja9HRoEgMFMetyc1u9rFolISc9Fj6Xsmjm8erjXTJsIiKGJiLqeTk3Kw4NJ9chY2Sg/MDJfRWAAIxjc+Q25+49bV62vl5TkqItvzq/k4MlIqrH0EREPS8+C0jIDhwfXouBowOhqc7uQempGhjGjJHbPGfOwFtW1uplxw+IhfBYIfxquS2vOq/rxk1EfRpDExH1jqFXBF6f2Yn0DL+i+9TBChjH5CjanG3cohuXXr8Y3O9OkNs400REXYWhiYh6x9ArFYeWii2wJBrl41MHK2EcNRJQBX5NtXWLzhqlw6DEKPjdgbVQDE1E1FUYmoiod6RfBBhiAseH1yJ9eJx8WHisCj6tAfohQ+S2tmaagIZ1Te7AuqZTNafg8Xm6ZsxE1KcxNBFR71BrgMEzA8fHNiF9aLR86PcKnDlSBWPQuqa6vXshfL5WL5vTPwZ+V2CmySd8OFVzquvGTUR9FkMTEfWe4Ft07lr0Nx6CFFQd/NTBChhzAqHJb7fDdexYq5fMSjQrbs8BvEVHRF2DoYmIes/g6YAU+DWkP/klkgZa5OOCgxUw5igXg7dVr2lQolkx0wTwCToi6hoMTUTUe0xx9WubGv38BdKHW+XD8tN2eOL7QRUVJbc597QempItehg1Jvg9gfVSnGkioq7A0EREvSu49EDVCaSn2hXdpw9XwzB6tHzc1mJwSZKQmRClKHLJ0EREXYGhiYh6V5PSA8l1m6DVB4pTnjpYqVgM7jpyBH67Mlg1FVJ2wJYPIUQXDZiI+iqGJiLqXYnDgNgM+VB9dB36ZQdu0Z06VAFD8Lomvx/OfftbveSgJjNNdo8dJY6SrhszEfVJDE1E1LskSTnbdPI7pGcZ5ENHtRt1KUMVb3Hmtl7kclBzT9DZeIuOiDqHoYmIel/wuibhQ7pxn6L7TKEf2rQ0+di5p/XQ1HRNEwDkVfEJOiLqnIgLTXV1dVi2bBmys7NhMBiQmpqKefPmIT+//f+K3LNnD2677TYMHToUJpMJqampuOyyy/DXv/4VvjYK6BFRFxowEdAGnpCLLfkcZqtePj51sALGsYFbdHV7cltdo5SZGAXhM0P4AjNWXAxORJ0VUaHJ6XRi+vTpeP7557F8+XJUVFRg7dq12Lt3L8aNG4c9bfzrM9gnn3yC8847D5s3b8Yf//hHlJSU4IcffsCYMWNw22234brrruvGb0JECloDMGiKfCgd/xr9hwXWNZ05XAXtqMBicG9pKbxFRS1ezmLQIsFsCFkMTkTUGREVmp588kls27YNzzzzDG644QaYTCbk5ORg9erVqK2txdy5c8OeIXrkkUfg8/nwwgsv4IorroDZbEZ6ejpeeeUVjBw5EmvWrMG6deu6+RsRkSzzssDr2mJkZHjlQ6/Hj5qkEYrT26rX1HQxeH4VQxMRdU7EhCaHw4GVK1dCp9Nh/vz5ir6srCzMmDED+/fvx+effx7W9U6cOAEAGDlypKJdkiSMGFH/y3nXrl1dMHIiCkvGJYrD/nrluqYKfxyg0cjHba1ralp2oMRZglp3bRcMlIj6qogJTRs3boTdbkdOTg7MZnNI/8SJEwEAa9asCet65513HgBg/37lo8tCCBw4cAAAkJqa2pkhE1F7pIwGdIENe40lW2FJNMrHJQUOGIYNk4/bKnJZvxhc+QTdcdvxrhkrEfVJEROaGtcrZWZmNts/aNAgxXltWbVqFbKysrB48WKsW7cOdrsdp06dwqJFi7B//35kZGRwXRNRT1KpgfQLA8cntyE5aB+64vxqGEYH1jXVHTzY+mLwhCj43E2eoOMedETUCRETmooaFn3GxcU122+1WhXntWXEiBH44YcfcNVVV2HWrFkwm83IyMjA3/72N9x6663Yvn17szNawVwuF2w2m+KHiDphQNAtuoo8JKdJ8qGzxgNvRmCmSTgc8BYWtnipQYlmCHcchAhUF+cTdETUGRETmhwOBwBAp9M126/X6xXntWX79u0YN24cNmzYgM8++wzV1dU4evQoHnzwQXi93rAC0IoVKxATEyP/pKenh/ltiKhZGZcqDpN1RxXH1aZ+imPXsZZnjjLiTFCrNPC74+U2hiYi6oyICU0mkwkA4Ha7m+13uVyK81pjs9lw3XXXoaCgAP/6179wxRVXwGKxICsrC48//jhcLhfGjRuHH3/8sdXrLFmyBNXV1fLPqVOn2vmtiEih33mAOvAPo0THVqg0gdmmCne04nTXMWWoCqbTqJBuNSrWNfH2HBF1RsSEppSUFABARUVFs/2VlZUAwlu8/cUXX6CwsBCjRo3C6KDd0xvNnTsXTqcTv/3tb1u9jl6vh8ViUfwQUSdoDfXBqYH69FYk9A8EpdJCN9RBt+jdx461ernMhCj4g9Y1nbKdgsfv6cIBE1FfEjGhaezYsQDQYuXvvLz6f0GOCdoNvSWN12gpYKU1bNfw008/tXeYRNRZwaUHivYhOT1QGbz0VA20WYPlY9fR1kPToESzYqbJK7woqCnourESUZ8SMaFp6tSpiIqKQm5uLux2e0j/1q1bAQBXX311m9dKSEgAAJw5c6bZ/sZ2rVbb0eESUUcNCF7XJJBsDjzc4fP44RwQ+IeR69ixNp+g8/MJOiLqIhETmkwmExYtWgSXy4W3335b0ZeXl4cNGzZg5MiRmDVrlty+d+9eTJgwAY8++qji/CuvvBI6nQ779+/Hvn3KAnoA8N577wEAZs6c2Q3fhIhalX4hgMA6pmShnPGtsQTKjvhtNnhLS1u81KAEZYFLgIvBiajjIiY0AcDSpUtxySWX4JFHHsHq1avhdDqRm5uL66+/HiaTCe+++y7U6sDjxatWrcK2bduwYsUKlJeXy+39+/fH888/DyEErr32Wqxfvx41NTXIz8/H4sWL8dFHH6Ffv3545plneuNrEvVthhggZZR8GFO2CYaowKxvJeIVp7vzWp45GpRoBvx6+D2B9YanavjABhF1TESFJqPRiI0bN2Lx4sVYsmQJrFYrZs6ciZEjR2LXrl3IyclRnD9nzhxYrVbMmTMnpL7Tvffei02bNskb9MbFxWHUqFFYt24dHn74YezZswcDBgzoya9HRI2CSg9IhTuRPCBKPi6rVitObW1dU7JFD5NODeGJlduK7cVdN04i6lM0bZ9ydjEYDFi6dCmWLl3a5rkzZsxo8Wk7AJg8eTImT57clcMjoq4w4BLgh1X1r31uJMfV4ERDV3W5Gz5rEtSVJQBaLzsgSRIyE6JwzBuDxqhVZA+vAC4RUVMRNdNERH1E0yKXmkOKY+fgwHYr7jaeoMtMiILwxMjHRQ6GJiLqGIYmIjr7RCcDcVnyYZJzs6K7JjFbfu1qo1bToIQo+INuz9k9dtS4a7pkmETUtzA0EdHZKWgfOkPhN4hNMsrHVbpAjTVfRQW8DcVtmzMo0QzhjVG08RYdEXUEQxMRnZ2Cb9G5a5Cc4pMPK5xGBFdnaq0yeGZCFPwehiYi6jyGJiI6OwXNNAFAsum0/NrlApyGhMBxK+uaMhOjILyxijauayKijmBoIqKzkzUTMMTKh8lSrqK7JnGo/Lq1dU0WgxZxhjgIEfh1x7IDRNQRDE1EdHaSJCBtnHwYX/sN1JrAryx7WmCzbXcrZQcAoH9sFERQgUveniOijmBoIqKzV9pY+aW6bB8S003ycbU5Q37tOtb6fnIpMQb4g27R8fYcEXUEQxMRnb1SxwZeCx+SE93yoc1vgWjYo85bXAxfTctlBFJjjIpaTbw9R0QdwdBERGevoJkmAIjXF8ivfUIFhzGwGW9rT9ClxRoUtZoK7UUQQrR4PhFRcxiaiOjsFTtAsRg8wbtX0W0395Nft7YYPDXGCOENrGly+epQ7aruunESUZ/A0EREZ68mi8Gttd9CkgLdtZagdU2tlB1IjTEoNu0FgGIHb9ERUfswNBHR2S3oFp2mbC9ikwOVwR2Jg+XXrrxWQlOsEX5WBSeiTmJoIqKzW5PF4PFxHvmw1pgiv25t496kaD3QtMAlQxMRtRNDExGd3ZouBjcGbqs5hAletQEA4Dl9Gn6Ho9lLaNUqJJjiIPwauY1lB4iovRiaiOjs1nQxuDik6K6NSpNfu/LyW7xMWoxJsXEvZ5qIqL0Ymojo7Na0MrjzO0W33RwITa1VBq8vO8DQREQdx9BERGe/oFt05srt0BnV8nGtub/82nX8eIuXSLEYOdNERJ3C0EREZ7+gxeASfIhPCHTZrQPl155TgeKXTTWdaSp2FLPAJRG1C0MTEZ39mi4Gj6qQX9caktEYfTynTrV4ifqtVGLlY4/fg4q6ihbPJyJqiqGJiM5+TRaDx6sC5QW8kg51hjgAgLug5Zmm1FhDaK0mPkFHRO3A0EREZ78mi8ET3D8qumuj6rdT8ZWXw2+3N3uJ+qrgLHBJRB3H0EREkSHoFl1czdeKrtqgPejcBaebfXtStAGSz6poY2giovZgaCKiyBC0GFwn2WGJDWxCZw+q1eQpaH5dk1olISkqFsKvlduK7dx/jojCx9BERJGh6WJwS438uvH2HAC421wMzrIDRNQxDE1EFBmaLgbXBsKRw5QEn6p+Bqm1sgP1G/cGrlHs4EwTEYWPoYmIIkPTyuDePUF9KthN9Zv3uk+dbPESaU0Wg3OmiYjag6GJiCJHao78MsG5TdHVuBi81ZmmGGOTApcl8Pl9XTxIIjpXMTQRUeRIHim/tEinoQms6Ya9YV2Tp6AAwu9v9u2pMQaIoNtzPuFFeV15twyViM49DE1EFDmSRsgvVZIfcVavfNw40yQ8HnhLSpp9e2qscqYJ4C06IgofQxMRRY6EoYBKIx/Gm0rl17VRaW1up5IWY1Bs2gswNBFR+BiaiChyaHRA/BD5MHg7FY8uGm6dBQDgbmFdU7xZD5UvVtHGJ+iIKFwMTUQUWZIDt+gSPLsUXbUNRS5bK3CZbLZC+PRyG2eaiChcDE1EFFmC1jXFu3YouuwN65pammkCgLQmG/cyNBFRuBiaiCiyBD1BZ1DVwBgVtJ2KKRlAy2uagMaq4LHycZGDoYmIwsPQRESRJWimCQDiLA75tUMucNlaaDIonqDjTBMRhYuhiYgiS2wGoIsOHOoC5QXsphQIAL7ycvjt9mbfntrkCboyRxm8fm+z5xIRBWNoIqLIIklA0nD50IrAE3RebRQ8WjMAwF1wutm3N63V5IcfZc6ybhosEZ1LGJqIKPIEPUFnrdup6Grcg66lJ+iazjQBYGgiorAwNBFR5Ala12T1/6zocjQsBm9pXVNqjBHCa1a0MTQRUTgYmogo8gSFJrOqHBqtkI8bF4O3tHFvfJQOGsGZJiJqP4YmIoo8QWUHJEnAGl0nHzeWHXC3cHtOpZKQZI6DEIFSBQxNRBQOhiYiijymOMCcIh/G6gNP0DmiWp9pAoC0mCgIX5R8zNBEROFgaCKiyBS0GDxOHJVf1xni4VNp4SkogPD7m31r/ca9gbIF5c7y7hsnEZ0zGJqIKDIFrWuKdecquhymZAi3G96SkqbvAlBfdoChiYjai6GJiCJT0Lomq5Sv6GprO5X6sgOBJ+iKHaXdMEAiOtcwNBFRZAqeadIUQpJCn6BraePeFIsB/uCZpjrONBFR2xiaiCgyJWYDUv2vMLXkhSUq8ASdo40Cl4nReghfYKbJ5XPC4XE0ey4RUSOGJiKKTFojEJclH1r1wXvQNRa4bH6mKcmiXAgO8Ak6ImobQxMRRa7g7VREYA86pykJAlKLa5oSzDqGJiJqN4YmIopcSUGLwb0H5Nd+lRZOQzzcBc3PNOk1apg1VkUbQxMRtYWhiYgiV/BMk0YZkBymZPjKyuB3Opt9a6IpQXHM0EREbYm40FRXV4dly5YhOzsbBoMBqampmDdvHvLz89t+czOOHTuGhQsXIisrCwaDAXFxcRg7diwWLVqE06dPd/HoiahLKZ6gaxKaGiuDFxY2/1ZzLIRfLR/zCToiaktEhSan04np06fj+eefx/Lly1FRUYG1a9di7969GDduHPbs2dOu661ZswajRo2C0WjEl19+ierqanz33XfIyMjAypUrceTIkW76JkTUJawDAY0RAGBQ2WHUu+QuuVbT6TPNvjU52gjhY4FLIgpfRIWmJ598Etu2bcMzzzyDG264ASaTCTk5OVi9ejVqa2sxd+5c+Hy+sK51+PBh3Hjjjbjrrrvw4osvYvDgwdDr9cjOzsbf/vY3jBgxAiaTqZu/ERF1ikoNJAyRD636wC02uexAYfOhKSlaryhwydtzRNSWiAlNDocDK1euhE6nw/z58xV9WVlZmDFjBvbv34/PP/88rOs9+uijcLvdWLJkSUifxWLB/v37ceGFF3bJ2ImoGyVmyy+DK4PbTSkQADxnmg9NidF6xRN0JawKTkRtiJjQtHHjRtjtduTk5MBsNof0T5w4EUD9Lbe2VFVVYc2aNRg2bBhSU1O7fKxE1IOCQ5P/Z/m1VxsFj9YMbwtrmhKj9Yqq4KUOzjQRUesiJjQ1rlfKzMxstn/QoEGK81qzY8cOeDweDBgwADt27MDs2bNhtVphMBgwbNgwPPHEE7Db7W1ex+VywWazKX6IqIclBIUmjbIuk92U0uKapsQmt+cqXRXwC3/3jJGIzgkRE5qKiooAAHFxcc32W61WxXmtaVzgvW/fPkybNg1XXHEFfv75Z5w+fRo333wz/vu//xuTJk1qMzitWLECMTEx8k96enp7vhIRdYXEYfJLq0b5xKvDlNLy03PRBsVCcJ/wwubiP3yIqGURE5ocjvp9oXQ6XbP9er1ecV5rqqurAQAFBQW49957sWjRIiQlJSE+Ph5Lly7FnDlzsHPnTvz+979v9TpLlixBdXW1/HOqherDRNSN4jIBlQYAYFaVQ6MOPAziMCXDU1wM0cwDIkkWfUhVcJYdIKLWRExoanySze12N9vvcrkU54Xr1ltvDWlbsGABAOC9995r9b16vR4Wi0XxQ0Q9TK0F4gcDACRJwGqslLvspmTA64W3NHSRd7ReA41Q/n+WT9ARUWsiJjSlpNQ/PlxRUdFsf2Vl/S/KcBZ2B9/iy8jICOlvXDd14sSJFkMaEZ1FEobKL2PVJ+XXcoHLZp6gkyQJccZ4RRtDExG1JmJC09ixYwGgxcrfeXl5AIAxY8a0ea0RIwJVhNsKRZIkhTlCIuo1Qeua4nyH5Nd1hnj4VNoWF4MnGbmVChGFL2JC09SpUxEVFYXc3NxmF2hv3boVAHD11Ve3ea0LL7wQsbGxAJoPYcePHwcADB48GFqttuODJqKeEVR2IEatXAzuNCa2uBg8JToGwhdYJ8mq4ETUmogJTSaTCYsWLYLL5cLbb7+t6MvLy8OGDRswcuRIzJo1S27fu3cvJkyYgEcffVRxvl6vx69//WsAwFtvvRXyWY3Xv/POO7v4WxBRtwgKTbEa5aySw5gEz5nm95FMsugVT9BxpomIWqPpiou43W4cPHgQpaWlqKqqQmxsLBITEzF8+PAWn3briKVLl+Lrr7/GI488guTkZMyaNQtHjhzB/PnzYTKZ8O6770KtDmzAuWrVKmzbtg3btm3D4sWLER8fWL/w2GOPYcuWLXj11VcxfPhw3HrrrfB4PPjTn/6Ejz76CLNmzcLixYu7bOxE1I3iBwOQAAjEqJVlRxympBZnmhLNevjPREOlq59hKnWyKjgRtazDoam0tBRvvfUWPvvsM/zwww/y02vB9Ho9LrzwQvzHf/wH5s+fj8TExE4N1mg0YuPGjXj22WexZMkSzJ07FzExMZg5cyZWr14tF7hsNGfOHLz33nuYPHlySH0no9GIr776Ci+88AJWrlyJBx54AGq1GqNGjcKf/vQn3H333YoARkRnMa2xfvPeynzoVHWI0tthd0UBqJ9p8p7Z3uzbmpYdKLZzpomIWiYJIUR73nD06FE8/vjj+Pjjj+VF1AkJCcjOzkZcXBwsFguqq6tRWVmJQ4cOoby8/l9wOp0O1113HZYtW4bBgwd3/Tc5C9hsNsTExKC6uprlB4h62ns3AYfXAgD+VfMCTtvr/xEVU30MF/z8Zwz96ceQBzs2HSrBwi8egy7uOwCARWvF1lu+7tlxE1HEaNdM06JFi/Daa6/B5/Nh6tSpuOWWWzBlypQWtzYB6tcbbdq0Ce+99x4+/PBDrF69GnfffTf++Mc/dnrwRESyxGw5NMWKYziN+tDkMCbB73DAb7NBHROjfEuTTXtrPFXw+r3QqLpk5QIRnWPatRD8jTfewD333IOTJ09i/fr1uP3221sNTED9nnB33HEHvvrqK5w4cQILFy7EG2+80alBExGFCNqDLlZdIL/26KLh0ZiaXdeU1CQ0CQhU1lWGnEdEBLQzNOXl5eH//b//h7S0tA59WL9+/fDSSy/h2LFjHXo/EVGLgmo1xaqbe4IutFZTXJQOwmdWtPEJOiJqSbtCU2NV7kaHDx/u0Ic2vQ4RUaclDJFfhpQdMCU1W+BSo1bBolU+JMLQREQt6VSdpgkTJuD777/vqrEQEXWcwQJY+gEAotXFkCS/3NVagcsEVgUnojB1KjTZ7XZMnz4dn376aZvnbtu2rTMfRUTUtoY96NSSDxZdYG2Sw9T87TkASDYrQ1N5HauCE1HzOhWavvrqKxiNRlx33XV4/fXXmz0nNzcXs2fPxqRJkzrzUUREbQte1ySdkl87jEnwFDYfmlKio+D3muRjzjQRUUs6FZouueQSbNu2DRkZGbj77rvx1FNPyX3Hjh3DLbfcgvHjx+Ozzz5Dv379Oj1YIqJWJQ6VX8aqTsqvnaZkuFuYaWpadoD7zxFRSzpdjGTIkCH47rvvMHv2bCxbtgwnT56ERqPBW2+9BY/Hg379+mHJkiW46667umK8REQtC55pCloM7lPr4az2wO92Q9Vka6ek6Mb954oBAEV2bqVCRM3rkgpuiYmJ+OijjzB27Fh5A9yUlBQ88sgjuPvuu6HX67viY4iIWhdUqymmadkBUxK8hYXQDRigaE+yGCC8gbIDpQ6GJiJqXqduzwFATU0Nli1bhlGjRqGsrAySJEEIgfHjx+Ouu+5iYCKinhMVD5jqF3aHlB0wNr9xb/3tuUBoquBCcCJqQadC09NPP43MzEw89dRTcLlceOihh1BQUICbbroJn332GaZNmybvPUdE1CMabtGZVRVQSx65uaVaTUnReviD1jQ5fXbUeeu6f5xEFHE6FZoee+wx1NTU4Fe/+hWOHj2KZ599FikpKfj73/+OBx54ANu3b8ell16K/Pz8rhovEVHrGhaDS5JArCYws+RsoSp404XgAMsOEFHzOhWabrnlFhw8eBB/+tOfkJqaquj7wx/+gBdffBFHjx7FpZdeih9//LFTAyUiCkvwHnSqwB50Ld2eM+k0MKhiFW0sO0BEzelUaPrb3/6GQYMGtdh/33334f3330dlZSWmTZvWmY8iIgpPcNmBoHVNTmMCXGearwoep49XHDM0EVFzOr0QvC033HAD1q1bB42mSx7UIyJqXfBMU1BoEioNakrtzb/F1KQqOGs1EVEzuj00AcCkSZOwdevWnvgoIurrLGmArv5puKZlB2w2QPj9IW9JNcdBiMCvQ4YmImpOj4QmABg+fHhPfRQR9WWSBCQMAdBM2QFdHHzNPNGbZDEqyg7w9hwRNadd98zeeeedLh/A2LFjMWbMmC6/LhH1YQnZwJldMEg10MEON6IABDbu1SQmKk5PijZAnDEDWhsAoNhR0uNDJqKzX7tC04IFCyBJUoc+SAiheG/j8dKlSxmaiKhrNcw0SRIQI51GqahfHN74BJ0xJ0dxelKTsgPFds40EVGodoWmN998s8sHMHbs2C6/JhH1cYmBxeBW7RmUuhtCkym52QKXTauCc00TETWnXaFp/vz53TUOIqKukxAoO2DVnQbc9a9d+ljUnT4ccnqSRVkVvMpdGTI7TkTUYwvBiYh6TNwgQFX/b0LFE3SSClWFNSGnJ5qVM00efx0cXke3D5OIIkunQtOBAwewadMm2O3N1z4hIuoVam19cELoE3TVlZ6Q060mHVT+Jlup8BYdETXRqdD0/PPPY8aMGThw4ICivbi4GE8//TSWL1+OPXv2dGqAREQd0nCLLkZdpGiucYT+2lOpJFh0cYo2lh0goqY6FZq+++47DB48GBdccIHc5nK5cMkll+Dxxx/HE088gfPOOw/PP/98pwdKRNQuDaFJp6qD0V8hN9dKMfDVhs6OxxuVW6lw014iaqpToamwsBBDhw5VtL3//vs4fvw4zj//fLz44ovIysrCI488worgRNSzgp6gsyCw55zTlARvUegedClRyq1UONNERE11KjS5XC6YzWZF2+rVq6FWq/HBBx/gvvvuw4YNG6DRaPDSSy91aqBERO3SUKsJUK5rchjrC1w2lWKOhxBq+ZhrmoioqU6Fpn79+uH48ePyscPhwFdffYVLL70UAwcOBACkp6fjsssu40wTEfWsoLIDcbrT8muPLhr2E0UhpydZDBDeKPm4jKGJiJroVGiaMmUKduzYgdzcXAD126w4nU5cddVVivNSUlJQVsapbiLqQfpoIDoNAGA1nFZ0VZ6qDDm9aVXwwlpupUJESp0KTQ899BC0Wi2mTZuGa6+9Fg888ADUajVuuukmxXnl5eWwWCydGigRUbsl1s82WZuUHagqCa3BlBSth/AFlhuUOPgPPSJS6lRoGjZsGD7++GMYDAZ88skncLlceOqpp5CZmSmf4/f7sWPHDvTv37/TgyUiapeE+sXg0epiSMInN1fb/CGnJlkMiqrgFXx6joiaaNc2Ks258sorcfLkSRw5cgQxMTFISUlR9K9btw4VFRUhs09ERN2uYTG4WvLB5C2FXVv/+6m2ThdyatP952zcSoWImuiSbVRUKhWys7NDAlNj3+23347rrruuKz6KiCh8wWUH/IHF37UqC4TXqzy1yVYqXuGG3cPdDogooNMzTW25/PLLcfnll3f3xxARhUoIhKYY1RkUYiwAwGlMgqekBLq0NLlfp1HBpI5F8I27MmcZzDplWRUi6rvaNdO0f//+LvnQrroOEVGrzEmAPgYAEKsNPEHnU+thO3o65PRYvXIrFVYFJ6Jg7QpNY8aMwc033yyXGGivXbt24cYbb0ROTk6H3k9E1C6SJD9BF9ek7EB5XmnI6YnGRMUxq4ITUbB2haalS5fis88+w7hx4zB27Fg8++yz2L59O1wuV7Pn19XV4bvvvsOKFSswevRonH/++Vi7di2WLl3aJYMnImpTQ5HLeFOBornqjC3k1FSzMjSxKjgRBWvXmqYnnngCCxcuxO9//3u88847WLJkCSRJgkajQXp6OqxWK6Kjo1FTU4OKigqcOnUKPp8PQgjExMTgvvvuw5IlS5CYmNj2hxERdYWG0BStrYDK54JfrQcAVJeF/mMv1WKFKFZDUtWXJ+DtOSIK1u6F4ElJSXjppZfwzDPP4MMPP8Snn36Kb7/9Fnl5eSHnpqSk4LLLLsMvfvEL3HjjjTAYDF0yaCKisDU8QSdJAlHuEtQY0wEANnvoRHtStAHijBmSqhoAUFQbeguPiPquDj89ZzQaMX/+fMyfPx8AUFpaipKSElRXVyMmJgZJSUmcUSKi3he0B120twg1qA9NtV5jyKn1+89FA9r60FTI0EREQbqs5EBiYiJDEhGdfWIHAGod4HPDgkI0bqjiUMfA5/VBrVHLpyY1KXBZyq1UiChIlxS3rKurw759+7Bx40bs2bMHdXV1XXFZIqLOU2uA+MEAgFh1YA86oVKj+qRyzVJitB7+oP3nqlwVPTNGIooInQpNQggsW7YMycnJyMnJwcyZMzF+/HjEx8djzpw52LZtW1eNk4io4xKHAQCsuiZlB35WbuRbP9MU2H+uxlu/lQoREdDJ0PTUU0/hySefRE1NDUaPHo1rr70WV1xxBcxmM9asWYPLLrsMd911F9xud1eNl4io/RpCU5xRWXag4oRyJsms10AjLPKxT3hQ46np/vERUUTo1JqmN998EyqVCh9++GHI3nLr16/HE088gddffx0nT57EF198AZWqS+4GEhG1T8MTdGazDdqSWni09bfgqoqVe8tJkgSLzorg1nJnOSw6C4iIOpViioqKMGnSpGY34505cya2bduG+fPnY8OGDfjzn//cmY8iIuq4hpkmtc4Pk7NYbrZV+UJOjdPHK45ZFZyIGnUqNCUlJSEhIaHFfkmS8Oc//xmJiYn43//93858FBFRx8UNAlQaSBIQ5QmEppo6bcipSVHK32kscElEjToVmiZPnowtW7a0+rScXq/HZZddhkOHDnXmo4iIOk6jA+KyAAAWf6Hc7BRGeNzK2aZ+0cmKY26lQkSNOhWaHnvsMTgcDtxzzz2tntdY8JKIqNc0rGuKQaGiubrEqThOjY6F8AeWe5bYeXuOiOp1KjTNnz8fQ4cOxTvvvIOpU6di+/btIeds2bIFmzdvbnbdExFRj0kaDgCI1SrLDFSdVm7cm2QxQATVajpTU9L9YyOiiNCp0LRjxw7s2rULQghs2bIFEyZMwIABA3DddddhwYIFmDJlCqZPn47Zs2fjD3/4Q5cMuK6uDsuWLUN2djYMBgNSU1Mxb9485Ofnd+q6NpsNGRkZkCQJb731VpeMlYjOIg0zTbF6ZWgqz1duldK0VlORnVupEFG9TpUcKCkpwa5du+SfnTt34ujRozh16pTivN27d2PevHkYP348xo0bh/HjxyM5ObmFq7bM6XRixowZyM3NxRtvvIFf/OIXOHLkCObPn49x48Zhy5YtyMnJ6dB3eeCBB0LGTUTnkIYn6IxRTugdFXAZ4gAAlQXVitOSog2KrVS4pomIGnUqNCUkJGDmzJmYOXOm3FZbW4s9e/bIIWrXrl04cOAA8vPz8dFHH0GSJABASkoKTp8+3dKlm/Xkk09i27ZteOWVV3DDDTcAAHJycrB69WpkZ2dj7ty52LNnD9RqdRtXUlq3bh3++te/4vzzz8ePP/7YrvcSUYSIHwxIKmhNPpjKS+TQVF2mfJAlMVoPf9BMU7W7skeHSURnry7bsLeR2WzGhAkTMGHCBLnN4/Fg3759cojauXMn9u7d267rOhwOrFy5EjqdDvPnz1f0ZWVlYcaMGfjyyy/x+eefY/bs2WFf12az4c4778Sjjz6K/Px8hiaic5VGD8QNgtZ3FCZHCSqt9TNPtiYFv+OjdJCC1jTVNmyl0vgPPiLqu3qkRLdWq8W4ceNwxx134JVXXsG2bdtgs9nafmOQjRs3wm63IycnB2azOaR/4sSJAIA1a9a067qLFy+G1WrFo48+2q73EVEEShwGSQWYvUVyk9unQV2tRz5WqSSYNLHysR9e2Nzt+31FROemXtvXpL3/atuzZw8AIDMzs9n+QYMGKc4Lx/r16/H222/jzTffhFYbWuSOiM4xDYvBLUJZdqCy2KE4jtHFKY65romIgF4MTe1VVFT/L8O4uLhm+61Wq+K8ttTU1ODOO+/EQw89hPHjx3doTC6XCzabTfFDRGexhsXgsSrlxr2VRco96BKMrApORKEiJjQ5HPX/EtTpdM326/V6xXltefDBB2E2m/HEE090eEwrVqxATEyM/JOent7haxFRD2gITTG6Eqh8brm5olAZmlKiEhXHnGkiIiCCQpPJZAIAuN3uZvtdLpfivNZs2LABr7/+Ot544w05bHXEkiVLUF1dLf+wZAHRWS5hCAAJ2igvTM5A0crKU8qyA/2ilaGp1MFaTUTUDU/PdZeUlBQAQEVFRbP9lZX1jwWnpqa2ep3G23L3338/Lrrook6NSa/Xdyp0EVEP0xoB60Boo87AVFqEWnN/AEBlYa3itDRLLEShFpKqfoH46RqGJiKKoJmmsWPHAkCLlb/z8vIAAGPGjGn1Oj/99BNOnDiBF154AZIkKX7efvttAMDtt98ut7E6ONE5JnEYdFE+RDkC6x9rbD54gzbuTbIYFQUuz9RyKxUiiqDQNHXqVERFRSE3Nxd2uz2kf+vWrQCAq6++utXrTJkyBUKIZn8a6z+9+eabctuCBQu6/LsQUS9KzIa2SWgCJFSVBNZDJln0iv3nuGkvEQERFJpMJhMWLVoEl8slzwg1ysvLw4YNGzBy5EjMmjVLbt+7dy8mTJjAGkxEFJA4DCqNgNmr3IOusjAQmhLNyqrgFXXNLwsgor4lYkITACxduhSXXHIJHnnkEaxevRpOpxO5ubm4/vrrYTKZ8O677yq2UFm1ahW2bduGFStWoLycT78QEQIb92oKAeGXm4PLDiRG6xW352wehiYiirDQZDQasXHjRixevBhLliyB1WrFzJkzMXLkSOzatStks945c+bAarVizpw5LdZ3AtDimqaBAwd259chot7QEJoMpjoYnYHbbpVFgZkmg1YNnRQjHzt8VfAHBSwi6psi5um5RgaDAUuXLsXSpUvbPHfGjBktPm0XTAjRFUMjokigiwJiM6AzVyLKUQSnKQlAaK0mi8aKxnK1Aj7YXDbEGmJ7dqxEdFaJqJkmIqIukTisvlaTo1huqi52wO8P/AMqVt9kKxVWBSfq8xiaiKjvScyG1uyDKegJOp9PoKbcGTjFpNxKpdTJWk1EfR1DExH1Pc3UagKUT9ClRaco+kocrNVE1NcxNBFR35M0AhqjDyanMjRVBD1Bl2FRhqYCW2GPDI2Izl4MTUTU9yQOg6RWwaSvhc4V2Hcu+Am61Bgz/EFlB05UMzQR9XUMTUTU9+hMQFwWdFFexbqmysKmtZos8vGZWuWsFBH1PQxNRNQ3pYyC1qxc11RV7JBLkKTGGCE8gVpNXNNERAxNRNQ3JY9sCE2BsgMuhxcOmxsAkBpjgD9opqnKzf3niPo6hiYi6puSR4XcngMC65oMWjUMUqBWk8NXBY/f06NDJKKzC0MTEfVNjTNN9qZlBwLrmmJ1wbWaBMocnG0i6ssYmoiob4pJhy7OBJ27GmpvoKhlZXHgCbokU7LiLcVBt/KIqO9haCKivkmSoEofAbXWr1jXFDzT1C+aoYmIAhiaiKjPklJGQRul3E4luFZTZmw/xfmnbSw7QNSXMTQRUd+VPBI6s3IxuL3KBbfTCwAYGBcH4dPJfflVZ3p8iER09mBoIqK+K7l+pinKrrzt1riuKS3WCL83UKupoIYzTUR9GUMTEfVdScNDClwCQGXDHnSpMQYIb3CBS65pIurLGJqIqO/Sm6FLSYChrgyS3ys3VxbWzzQlWwyKrVQqXSw5QNSXMTQRUZ+mzRwKlfDD5CyV2yoanqDTqlUwqgIFLu2+CnmbFSLqexiaiKhP02aPAwBE2U/LbeWna+XXsdpAgUs/PKhyVfXY2Ijo7MLQRER9mqr/GGiMPphrA0/G1ZTXwdXwBF2SKUlxPjfuJeq7GJqIqG9LHgVtlBfm2gJFc+NsU1p0qqKdBS6J+i6GJiLq22IHQGeRYA66PQcA5QX1oWlgjDI0nahmrSaivoqhiYj6NpUK2uR46F1V0HgCW6iUNcw0ZcWnQIjAr8q8CoYmor6KoYmI+jxtxgBIgGK2qXGmqV+sSVF2oKCmsKeHR0RnCYYmIurzdINHAADMtcon6IRfoF+sEcITCE3FXAhO1GcxNBFRn6cdcREAZWjyuv2oLnUiwawHfIGq4JWu0pD3E1HfwNBERH2eZvglkFQiZDF4WUEtVCpJUeCy1lve08MjorMEQxMR9XmSyQpttIQoeyEg/HJ7Y9mBWF2i3OaFAw6Po8fHSES9j6GJiAiALskCtd8DU9CapbKGxeAJxkTFuSxwSdQ3MTQREQHQZQ4A0OQJuoaZpn5mFrgkIoYmIiIAgH7YGADKxeCN26kMjFWGprwK5donIuobGJqIiADoxk0CgNDK4KdrMTi+n6Itr4oFLon6IoYmIiIA+uGNM01N9qArqEWG1QK/N0puO2Ur6tGxEdHZgaGJiAiAOjYW6ihtw3Yqgafjyk7XIi3WqKgKXmznmiaivoihiYiogb5/UsN2KoHZpvKCWlhNWkhBBS4rWOCSqE9iaCIiaqAbPBQAYK4NrFkqP10LCMCkipfbar0VPT42Iup9DE1ERA30o84DoFzX1LidSowuEJrcogpev7fHx0dEvYuhiYiogW7IMADNP0GXYEwONEgCZc6ynhwaEZ0FGJqIiBroB2UCQMh2KmUFtehnTlacW1jLJ+iI+hqGJiKiBprUVEh6bf12Kk7ldioZMcoCl0fLWauJqK9haCIiaiBJEvQD0gEoK4OXn67FkPj+inOPVbEqOFFfw9BERBREN3Q4gNDtVAZExUL4tXLbqerCHh8bEfUuhiYioiD6wYMBhC4G19X6IDyBWk0scEnU9zA0EREF0WUOAgBE15xUtNecdkDlD4Smcha4JOpzGJqIiILos+pDk95tg6GuXG4vzrfBqEqQj21ezjQR9TUMTUREQXQZGYC6/lejxZYvtxflVcOq6Scfu1EBp9fZ4+Mjot7D0EREFETS6aBLzwAAxNiOy+0OmxuZ2oGKc0/alLfwiOjcxtBERNSELisLgHKmCQCG+gcojvcWH+uxMRFR72NoIiJqQj+ocTF4ASS/R25PqYtVnJdbfLQnh0VEvYyhiYioCV1DaFIJL6KDNu9FuRd+T7R8eLTqeA+PjIh6E0MTEVETjU/QAcpbdNVn7FC7U+TjQvupHh0XEfWuiAtNdXV1WLZsGbKzs2EwGJCamop58+YhPz+/7TcH2bFjB/7rv/4Lo0aNgslkgsFgwODBg3HPPfe0+1pEdG5pnGkCgJjqwO8Dv1cgwz1MPq7ycP85or4kokKT0+nE9OnT8fzzz2P58uWoqKjA2rVrsXfvXowbNw579uwJ6zqfffYZLrzwQvz73//G73//e5w+fRonT57EI488gnfffRdjxozBtm3buvnbENHZSm02Q5OUBACwBD1BBwCZriz5tVeyodZd25NDI6JeFFGh6cknn8S2bdvwzDPP4IYbboDJZEJOTg5Wr16N2tpazJ07Fz6fr83rOJ31tVU++OADXHPNNbBarUhKSsKdd96J5557DrW1tbjjjju6++sQ0VlM13CLzuCqgN5TJben1SUrzjtaebwHR0VEvSliQpPD4cDKlSuh0+kwf/58RV9WVhZmzJiB/fv34/PPP2/zWnFxcZg5cyYuuuiikL6rr74aAHDo0CGcPMkaLER9lb5hOxUJytkmS02U4rydZ4704KiIqDdFTGjauHEj7HY7cnJyYDabQ/onTpwIAFizZk2b15o2bRrWrVvXbF9MTEyz7UTUt+iCF4MHPyVXK2BwWeTDfaUsO0DUV0RMaGpcr5SZmdls/6CGhZvhrmtqyaFDh+TrZWRkdOpaRBS59FmBtUtNi1wmVQ+XXx+vPtFjYyKi3hUxoamoqAhA/a215litVsV5HfXOO+8AAJ544ok2z3W5XLDZbIofIjo3GIYFnpKz1JyEJALrJdNqs+XXJc7TPTouIuo9EROaHA4HAECn0zXbr9frFed1RG5uLl599VVcf/31IeummrNixQrExMTIP+np6R3+bCI6u6hjY6FtmG1W+92wuAJFLtMcgVnoGn9hj4+NiHpHxIQmk8kEAHC73c32u1wuxXntVVpaihtuuAEXXnihPNvUliVLlqC6ulr+OXWKhe6IziXGUaPk19FB65ri7XGQRP2vT79kR1VdVQ+PjIh6Q8SEppSU+iq8FRUVzfZXVlYCAFJTU9t97crKSlx55ZVITEzEZ599Fnbw0uv1sFgsih8iOncYxoyWX0cHlRZQ+9WIcwR+1+zhHnREfULEhKaxY8cCQIvVuvPy8gAAY8aMadd1S0tLMXXqVFitVqxbt45PzxGRzDg6EJpimhS5TK4ZKL/eVciyA0R9QcSEpqlTpyIqKgq5ubmw2+0h/Vu3bgUQqLMUjjNnzmDy5Mno168fPv30U8UM02uvvYb9+/d3fuBEFLEMw4cDqvpfk0ZnCbT+QPXvpKDQdKiMWy8R9QURE5pMJhMWLVoEl8uFt99+W9GXl5eHDRs2YOTIkZg1a5bcvnfvXkyYMAGPPvpoyPVOnjyJSZMmYfjw4fj4449hMBgU/b///e+xY8eO7vkyRBQRVCYT9EOGAKgvcml1BMJRmm0oIOpfn6phIVyivkDT2wNoj6VLl+Lrr7/GI488guTkZMyaNQtHjhzB/PnzYTKZ8O6770KtVsvnr1q1Ctu2bcO2bduwePFixMfHA6gPWVOnTsXp06cxduxY3HbbbSGfVVpa2mPfi4jOXobRo+D6+WcAgKXsCErM9bfsLO5YxNQlotpYirKgJ+uI6NwVUaHJaDRi48aNePbZZ7FkyRLMnTsXMTExmDlzJlavXi0XuGw0Z84cvPfee5g8ebKivtOaNWvkLVJWr17do9+BiCKLcdRoVP+z/vdEXOlBYGCgL71qGKqNpXCKYgghIElS7wySiHqEJIQQvT2Ic4XNZkNMTAyqq6v5JB3ROcK5fz+OX/9LAPV347ZPXg6nVF9M90Tsfnwx/C8AgHXXb0SqObG3hklEPSBi1jQREfUGw9ChkBqK6koAkpz75L4022Co/PVLAnYUHO6N4RFRD2JoIiJqhaTV1j9F18Badkh+rfXrkVLTsO9lEWs1EZ3rGJqIiNpgCKrXFH3mCAC/fJxeVb9H3ZFKlh0gOtcxNBERtcE4OrCdirbOjgQpEJAaQ9OZWm6jRHSuY2giImqDYbRyp4Fk1175dYKjP4zuaFR6zvT0sIiohzE0ERG1QTdwAFRms3wcX3lA0d+/OhtuqRh+v7/pW4noHMLQRETUBkmlgmFU4Badsfg0tJJTPk6vGg6oPPi5/HRvDI+IeghDExFRGILXNXmqgH7awC26/tXZgJDww6mfe2FkRNRTGJqIiMIQ/AQd/ECKJxCaTJ5oxDvSsOMMN/kmOpcxNBERhcEYHJoAJNQcUhynVw3HgYq9IKJzF0MTEVEYNCkpUCckBI6dWljURfJxetUwlHkOgztTEZ27GJqIiMIgSZJitslx2od03S75OKUmE2rJjr1FrNdEdK5iaCIiClPUJZfIr71VtUgNWtekFhr0rxqGzw5v742hEVEPYGgiIgqTefIkxbHF5YQabvl4SNl52FG4q+nbiOgcwdBERBQm3YAB0A0cKB+7ClUYaNghHw+sHIWiau5BR3SuYmgiImqH4NkmZ14JBqu3yMdqoUG/KiuqnM7m3kpEEY6hiYioHaImBd2i8/sRL3mhUtXITUPLxuGzQzt7YWRE1N0YmoiI2sF0wQWQTCb52FGdjAzjVvk4rWYwth3c3QsjI6LuxtBERNQOKp1O8RSdfe9JjLf8pDjHl+du+jYiOgcwNBERtZM56Badr6oKluRsuHXFclt6WRq8Pn9vDI2IuhFDExFROzUtPWCvTIIx+lv5OLYuEZt/ONLTwyKibsbQRETUTtqUFOizs+Xj2h25GNyvWHHO7q8P9vSwiKibMTQREXVA8C0614GDuGDklSiKPia3aU9p4OMtOqJzCkMTEVEHmKdMVhyLEguqrD/IxxqvCaf2l/f0sIioGzE0ERF1gDEnByqLRT6u/WYr1Ell8EleuW3PutzeGBoRdROGJiKiDpA0GpgnTpCP7Vu3YkjmVJyw7pfbCo75YStjdXCicwVDExFRB5knB27R+e12THel4UByoNAlhBo7/7WnF0ZGRN2BoYmIqIOiLrsMUKsDx59vQUn0CRSbj8ttB3+qQW1lXS+Mjoi6GkMTEVEHaeLiED1jhnxcu3EjBruz8FP/dXKbX6ixa/UPzb2diCIMQxMRUSdY594SOPD7cfNhDU7G7kdp1Cm5ef9PTtirONtEFOkYmoiIOsF0wQXQDxkiHw/bdhRaL7CzX2C2ySe02P2Pb3pjeETUhRiaiIg6QZIkWG+9VT4WVdX4xdE05MftRYWxUG7ft8sLp42zTUSRjKGJiKiTYmb/B1TR0fLx1bleAH7sDFrb5PXrsefv65p5NxFFCoYmIqJOUplMiL3uOvnYnHcGQ85ocCx+F6oMgT3pcvdoUGez98YQiagLMDQREXUB6y03A5IkH9+Ua4WQBHb2Wy+3efwGbP/zv3phdETUFRiaiIi6gG7AAERNukw+Hr2vFDF2gaMJP6HaUCq3789LxamNW3pjiETUSQxNRERdJG7uXPm15PXh8t06+FV+bBn0vuK8jR+Vwl1R2vTtRHSWY2giIuoiURMnQjsgQz6e/aMPFrvAmZij2Jf8tdxe643D1j/+AxCiN4ZJRB3E0ERE1EUklQpxt86Tjw0ON+Zv8AMAtg/4N/zaMrnvQOEwnPzk/ZBrENHZi6GJiKgLWf/zJuizs+Xjyw4IjD3mh1ftxueZnyjO3bReA9epgz09RCLqIIYmIqIuJGm1SP3vZYon6e780g+9W6AgPhfuuJ/l9lpfPL5+5TOImuLmLkVEZxmGJiKiLmYcMwbWeYEq4UnVwI3f1N+me7PfZ4jSVcp9h6vH47vnXoWoLQu5DhGdXRiaiIi6QeK990GTmiof/2KHwKBCAWE+gX+lHoIEn9y3q3QSdv7hBcBR0RtDJaIwMTQREXUDtTkKKUufkI9VAvjVFz6ofQJHkz9C7QgHAL/cv71oJvb9z+8BZ1XPD5aIwsLQRETUTaKnTIFl1lXycWYx8JtP/VBJLrypeg/ZU6IV528puAo/v/QEYOetOqKzEUMTEVE3Sl6yBKqYGPl44gGBhZ/7oTXvxzNVP+KC6TFBZ6vw1fHZyH3mMYj8b3t+sETUKoYmIqJupElMRL//eQGSViu3Td0rcNdaP4rU72FPqgFjJ5jlPgE1vim9EV++tBnuDS8Afl9zlyWiXsDQRETUzcwTJqDfyy8BQcFpxm6BOzZV4n9+fAEVo/th5AVRivccq7sUH36cjLI/3wnYCnt6yETUDEkI1vHvKjabDTExMaiurobFYunt4RDRWca2fj1O3/9/AV9g9mjTaAmvXXgefjtpGUbVCnz7z2PwC7Xcr4YbF8f+A6NnDIb6skWAPrq5SxNRD2Bo6kIMTUTUFtvnn6PgwQch+QO/essswCtTBmLaf7yIawekYe0r36OmRq14X7SqBBfGf4qhV02E6sLbAY2+p4dO1OcxNHUhhiYiCkf1mjU4/fAjkJr8+t0wIhru2/6AX8+4GBv/9yccP+QMeW+c5gQuSlqPgZPGQTV+LmBJ66lhE/V5Ebemqa6uDsuWLUN2djYMBgNSU1Mxb9485Ofnt/tafr8fL7/8MnJycmAymZCQkIBrr70Wu3fv7vqBExE1iLn6agx46y34UhIU7TMO1OCSZffgld8shOXiWEy6aTAMBr/inArvAHxx5k688+FAfPfUClT85S7gwCeAp64nvwJRnxRRM01OpxMzZsxAbm4u3njjDfziF7/AkSNHMH/+fBw/fhxbtmxBTk5OWNfy+/248cYb8cknn+Dll1/GbbfdhsLCQixcuBDffPMN/v3vf+Pyyy9v1/g400RE7eG323Hg6cegXr02pM+jAvZnj8CwO++D25OCPRtOwONVN3MVIEl7BIOMP6F/pgaJOaOhGjIdSBii2P+OiDovokLTww8/jOeeew6vvPIKfvOb38jtx44dQ3Z2NoYNG4Y9e/ZArW7+F0uwV199Fb/+9a/x4IMP4vnnn5fbKysrkZWVBa1Wi6NHjyI6OvxFlwxNRNQRxzb/G6ceXYLkiubLC1RGR8E26lJ4B87AqWIT/P6WbxLoJDvSdPvRL/oUEtIMSBicBsPAUUDqWCA6uZu+AVHfEDGhyeFwICkpCR6PB+Xl5TCbzYr+K6+8El9++SXWrFmD2bNnt3m9rKws5OXl4fDhwxgyZIiib+HChVi1ahVefvll/Nd//VfYY2RoIqKOKq0owKcvPoCMDXuRVtnyeW5tFPIHTUZl4gVwaJLCunaUqhzxmuOI1Vcg2iJgidcjOjkW5tRkGBKTIVlSgegUwGjl7BRRKyImNH366aeYPXs2LrjgAvzwww8h/cuXL8fjjz+OO++8E6+99lqr19q3bx9Gjx6NxMRElJSUhPT/7W9/w7x58zBjxgysX78+7DEyNBFRZ52oOo4P33kKqeu+x7ijotWFp7VRaShMuQilCTmoMyZ26PMk+GCUbDCqqmBU10Cv80Cv80OvB3QGFXRGDTR6LbQGLbQGHTRGPTR6PdR6HdR6PdR6A9R6PVR6HVRaPVRaLdQ6HSStHpJaA0mjBVRaQK0FJDWgiriltEQyTW8PIFx79uwBAGRmZjbbP2jQIMV5PXUtIqKuNCB2IB66902cXHASf/r3s1Bt24qR+W6MOCVg8CjPNdvPYMixjzHk2Mdw6uNQaR2KSms2KmOHwq2PDevzBNRwCCscPivgA+Bu74i9DT/2Fj7AB0n4Afgb/lfI/wshIEHUv244BhDUhoZ2yMcSgv+dH/Q65N//ofMBUjNt4emmuYWImLI4G3lx59v/p1c+OWJCU1FREQAgLi6u2X6r1ao4ryuuVVpaCr/fD1UL/zJyuVxwuVzysc1ma/OziYjCkWHJwDNzV8Jzswd7SvbgX/s3oPD7jUg/ehoDSgXSSwWSqgKPQBtdFTAWbUda0XYIAG6dBbVRaag190dtVD/Yo1JRZ4iHV2vq2S8iqSGk+nWmzAjUFSS/p+2TuknEhCaHwwEA0Ol0zfbr9XrFeV1xrcZzm66farRixQo89dRTbX4eEVFHaVVanJ9yPs5POR+Y/giqXdU4XHEEPxUexicFP8Nx7CBMhYWIttkRW+NCfK2AtUbA4rDB7LTBWnlIcYvPozGiTh+HOkM8XPoYuHXR8Gij4dZFw62NhldjlH98GmOvfW+is1HEhCaTqf5fR25383PHjTM+jed1xbXaut6SJUvwwAMPyMc2mw3p6eltfj4RUUfF6GNwQer5uCD1fGC8ss8v/Kh2VeO0rQSFtVXId9SiymGDo7IcddVV8DvsEA4nUOeEqKuD5HZD5SmBynsaktcHtccHye+D5PND5ReQfAJqnxoQGgAaqHxqqIQaEtQA1JCgggQ1JKFC/ZyXGpAajyVAUgFC1bC4vKGttR8JTdoaNfc6tE20uoa9aee5tOD9XPou4fABuKJXPjliQlNKSgoAoKKiotn+ysr6x01SU1O77FpJSUkt3poD6mekgmeliIh6k0pSwWqwwmqwYlR4D9YRUTtEzGMMY8eOBYAWK3/n5eUBAMaMGdOj1yIiIqK+IWJC09SpUxEVFYXc3FzY7aFPaWzduhUAcPXVV7d5rZEjR2LQoEEoLS3F0aNHO3UtIiIi6hsiJjSZTCYsWrQILpcLb7/9tqIvLy8PGzZswMiRIzFr1iy5fe/evZgwYQIeffTRkOs99NBDAIC//OUvivaqqip8+OGHSEpKwoIFC7r+ixAREVFEipjQBABLly7FJZdcgkceeQSrV6+G0+lEbm4urr/+ephMJrz77ruKLVRWrVqFbdu2YcWKFSgvL1dc6+6778Z1112HF198EatWrYLdbsexY8dw4403oqamBn/961/btYUKERERndsiKjQZjUZs3LgRixcvxpIlS2C1WjFz5kyMHDkSu3btCtmsd86cObBarZgzZ05ITSaVSoUPP/wQf/jDH7By5UokJCTgwgsvhMlkwvfff9/uzXqJiIjo3BYx26hEAm6jQkREdO6KqJkmIiIiot7C0EREREQUBoYmIiIiojAwNBERERGFgaGJiIiIKAwMTURERERhYGgiIiIiCoOmtwdwLmkseWWz2Xp5JERERNRe0dHRkCSpxX6Gpi5UU1MDAEhPT+/lkRAREVF7tVWcmhXBu5Df78eZM2faTKrtZbPZkJ6ejlOnTrHSeITi32Hk499h5OPfYWTrib8/zjT1IJVKhf79+3fb9S0WC/+PHuH4dxj5+HcY+fh3GNl68++PC8GJiIiIwsDQRERERBQGhqYIoNfrsXTpUuj1+t4eCnUQ/w4jH/8OIx//DiPb2fD3x4XgRERERGHgTBMRERFRGBiaiIiIiMLA0EREREQUBoamCFReXo4XXngBU6dORXx8PLRaLZKSknDVVVdhzZo1vT08aqfvv/8eY8aMgSRJOH78eG8Ph4LU1dVh2bJlyM7OhsFgQGpqKubNm4f8/PzeHhq1Q11dHX73u99Bp9NhwYIFvT0caoeNGzfijjvuwNChQ2EwGGAymTBixAg89NBDKC0t7fHxMDRFoGHDhuGhhx7CpEmTsHPnTlRVVeGTTz5BeXk5rrnmGjzwwAO9PUQKQ21tLe677z7MmDEDR44c6e3hUBNOpxPTp0/H888/j+XLl6OiogJr167F3r17MW7cOOzZs6e3h0hh2LRpE8aMGYNXX30VHo+nt4dD7bBy5UpMnz4dP/74I/785z+jtLQUR44cwYIFC/DSSy9h9OjROHz4cM8OSlDEiYqKEgsXLgxpLy4uFhaLRQAQ3333XS+MjNojOztbzJw5Uxw7dkwMGDBAABD5+fm9PSxq8Nvf/lYAEK+88oqi/ejRo0KtVouRI0cKr9fbS6OjcLz33nsiJiZGrFy5UrzxxhsCgJg/f35vD4vC9PzzzwudTidOnjwZ0vfwww8LAOLKK6/s0TFxpikCjR8/HvPmzQtpT0pKwkUXXQQAWLduXU8Pi9rp6aefxrp16zBo0KDeHgo14XA4sHLlSuh0OsyfP1/Rl5WVhRkzZmD//v34/PPPe2mEFI7MzEwcOHAAv/71r7t0P1DqGSkpKbj55puRnp4e0nf11VcDADZs2ACfz9djY2JoikBff/01Lr300mb7YmJieng01FHXXXddbw+BWrBx40bY7Xbk5OTAbDaH9E+cOBEAuIbwLHfxxRcjLS2tt4dBHXTrrbfirbfearav8b91kiRB9GC5SYamc8yhQ4cAAFOmTOndgRBFsMb1SpmZmc32N84Ocl0TUe9o/G/dxIkTodFoeuxzGZrOIbt27cK+ffswZcoUTJo0qbeHQxSxioqKAABxcXHN9lutVsV5RNSz3nnnHUiShCeeeKJHP7fn4hnJrr32Whw8eLBd73nnnXdw4YUXttjv9/uxaNEiWK1WvP32250dIrWhO/4O6ezhcDgAADqdrtn+xr2vGs8jop6zdu1arFmzBosXL+7xuyoMTb0gPz8fP//8c7ve09Yv58WLF2P37t1Yv349MjIyOjM8CkN3/B3S2cNkMgEA3G53s/0ul0txHhH1jCNHjmD+/Pm4/vrr8eyzz/b45zM09YLdu3d36fWeeuopvP766/j0009bXCBOXaur/w7p7JKSkgIAqKioaLa/srISAJCamtpjYyLq606cOIGZM2di4sSJeO+996BWq3t8DAxNEe7RRx/Fq6++ivXr18vlBoioc8aOHQsALVb+zsvLAwCMGTOmp4ZE1KcdPXoU06dPx6RJk/DWW2/1SmACuBA8ot1///34y1/+go0bNyoC07Zt2/DBBx/04siIItvUqVMRFRWF3Nxc2O32kP6tW7cCCNSKIaLuc+DAAUyaNAmXX3453n77bUVg+sMf/oBTp0712FgYmiKQEAILFy7E+++/j82bN2PcuHGK/nXr1uHVV1/tpdERRT6TyYRFixbB5XKFPFiRl5eHDRs2YOTIkZg1a1YvjZCob9izZw+mTJmC66+/Hn/5y1+gUiljy0MPPYRjx4712Hh4ey7CCCGwYMEC+Ums5cuXh5yzb98+JCQk9MLoiM4dS5cuxddff41HHnkEycnJmDVrlrwI1WQy4d133+21WwREfcGPP/6Iyy+/HC6XC6Wlpbj55pt7e0iQRE+W0qROq6qqkmvEtGby5MnYvHlz9w+IOuzJJ5/EU0891WzfgAEDcPz48Z4dEIWoq6vDs88+i3fffRcnT55ETEwMZs6ciWXLlnH7mwjR2vYpb775JhYsWNBzg6F2uf/++/HSSy+1ed6mTZt6rPQAQxMRERFRGLimiYiIiCgMDE1EREREYWBoIiIiIgoDQxMRERFRGBiaiIiIiMLA0EREREQUBoYmIiIiojAwNBERERGFgaGJiIiIKAwMTUTUqq1bt+KKK65AcnIyDAYDMjMzcdttt/XIZ1dVVeHJJ5/EW2+91SOfF6mefPJJSJIk/3ALHqLuwW1UiKhFR48eRU5ODkaOHIkPPvgAaWlpWL16NebOnYue+NVx/PhxZGZmci/FME2ZMgVbtmxBfn4+Bg4c2NvDITrncKaJiFr05ZdfwuFwYN68ecjMzIRer8ctt9yCgwcP9vbQiIh6HEMTEbWotLQUABAdHa1oHzZsWG8Mh4ioVzE0EVGIzZs3Q5IkPPXUUwCA22+/XV4v8+STT8rneTwevPjiixg3bhxMJhMsFgsuvfRSvP322yHXFELggw8+wE033YQhQ4bAYDDAarVixowZ+Pzzz0POHzhwIDIzMwEAW7ZsUazZ2bx5M/71r3+FtDVqusYn2JVXXqnoczgcePDBB5GRkQGNRhPyHUtLS/F//+//RVZWFvR6PRISEjB79mxs27YtrD/LpmNpev0pU6Yo+hrXb+Xm5uKhhx7C+PHjYbVaYTQaMXLkSDz55JOoq6sL67MBYOzYsfK1p0yZouhraUzB1qxZg2nTpiE2NhZGoxEjRozAU089BbvdHvYYiM4ZgoioBUuXLhUAxJtvvhnS53K5xPTp04VKpRIvvPCCqK6uFqWlpeLxxx8XAMRvfvMbxflOp1MAENOnTxd79+4VTqdTHDt2TNx9990CgHj99ddDPiM/P18AEJMnT25xjPPnzxcAxKZNm0L6BgwYIFr6NdfYd/XVV4tVq1aJsrIyceDAAZGUlCSWLl0qhBAiLy9PpKeni/j4ePHpp5+Kuro6cfToUTF79myhVqvF+++/3+K4ghUXFwutViusVqtwOp0h/R9//LFISkoSLpdLbrvppptEbGys+Oijj4TNZhNlZWXi3XffFRaLRUyePFn4fL6Q60yePFkAEPn5+Yr21v4c33zzTQFA/s7BHnvsMQFAzJs3TxQUFAi73S7+/ve/i6ioKHHBBRcIh8MR1vcnOlcwNBFRi1oLTY3/QV2wYEFI3xVXXCEAiPXr18ttLpdLjB07VhQXFyvO9fv9IicnRyQkJAiv16vo64nQ9MQTTyjaX3zxRfGPf/xDCCHExIkTBQDx1ltvKc6x2+0iLi5OWCwWUV5e3uLYgv3yl78UAMQ777wT0nfllVeKhx9+WNH28MMPi9deey3k3P/5n/8RAMS//vWvkL6uDE3r168XAERWVlbI38uKFSsEAPG73/2uhW9LdG7i7Tkiajefz4eVK1cCAH71q1+F9N96660AgL/85S9ym06nw65du5CUlKQ4V5IkjBkzBmVlZb2ywPzmm29WHN9///345S9/iZ07d+Lbb7+F2WzG3LlzFeeYTCZce+21sNlseP/998P6nMY/p+A/E6D+CcH169fjrrvuUrQ/88wzuPPOO0Ouk5OTAwD45ptvwvrcjnr55ZcBAHfeeSfUarWir/Hv97XXXuvWMRCdbRiaiKjdfv75Z1RWVkKlUsn/EQ+WkZEBAPjhhx8U7QcPHsTtt9+OoUOHwmg0yutp/vrXvwIAKioqun/wLYy1qe3btwMARo0aBY1G0+L7mn7HlkyfPh1ZWVn49ttvFeHwtddew7Rp05CVlaU4326344UXXsB5552H+Ph4+c9q+vTpALr/z6rx+48bNy6kLy0tDRqNBiUlJawJRX0KQxMRtVtlZSUAwO/3w2QyhSx0njx5MgCguLhYfs8333yDcePG4bPPPsNzzz2HoqIiiPolApg/f758vZ5mMpmabW/8jtu3bw/5fpIkYenSpQCU37E1kiTJs0mNs01erxdvvPFGyGydy+XCZZddhgcffBDTpk3D7t274fP5IITApk2bAHT/n1Xj92+6cF6SJKjVani9XgDhf3+icwFDExG1m9VqBQBotVr5P+bN/TidTvk9y5cvh8vlwu9+9zvMmTMHMTExXTKWpk/HBXM4HB2+buN3nDZtWovfTwiBL774Iuxr3n777dBqtXjnnXfgcrnwySefAACuueYaxXkfffQRdu3ahbFjx+L5559Heno6VKqO/7ruyJ9R4/f/+uuvW/3+F110UYfHRRRpGJqIqN2ys7NhtVrh8XhQUFDQ7Dk7d+6Ub/EAQH5+PgBg6NChIecGh6tgrf3HvpHRaASAkEfgXS4XysrK2nx/Sy6++GIAgXE3Z8OGDTh8+HDY10xKSsI111yDiooK/POf/8SqVatwxx13hNz+68ifVWta+jMC0OLfX1vf//jx4/jyyy97ZXaQqLcwNBFRu6nVavzmN78BALz++ush/eXl5Zg2bRq+/PJLua1xDdCePXsU57rdbnz//ffNfk5cXBwAZVB44YUX8Mtf/lI+zs7OBoCQReQfffRRp7Z6GT9+PCZOnIj8/Hz5lliwr776CjNnzkRhYWG7rtt4K2758uXYuHFjyAJwIPBntXfv3pBQ0pEF4ImJibBarThy5Ah8Pp/cLoTAxx9/3Ox77r33XgDAG2+8EdInhMBtt92GZ555plMzYEQRp6ce0yOiyBNOnSadTieeeeYZcfz4cWG328XXX38txo8fLy644AJRXV0tn//5558LSZKExWIRH374obDZbCIvL0/cdNNNQpKkFssGjBgxQkRHR4v8/Hxx5swZMWLECHHnnXfK/QUFBcJkMon+/fuLbdu2iZqaGvHll1+KK664QqSmprZZcqA1eXl5IiMjQyQnJ4sPP/xQlJaWioqKCvH3v/9dxMfHi7vvvju8P8ggfr9fZGVlCQDiqquuavYcu90un7Nw4UJx6tQpUVVVJV5//XVhNBoFADF//vyQ97VUckAIIe69914BQDzwwAOitLRUnDx5UvzqV78SV155ZVh1mvbv3y8cDoc4cOCA+M///E8RHx8vcnNz2/39iSIZQxMRhdi0aZMAEPIzYMAAxXlut1u89NJL4rzzzhMmk0lYLBYxevRosXz5ckVgarR582Yxbdo0ER8fL/R6vRgxYoRYsWKFuOWWW1r8jB9//FFMmDBBREdHi/j4eHHDDTeIkpKSkPFedNFFQq/Xi/j4eLFgwQJRVlYmByMA4qKLLhJCBOo6Nf1pLmgIIURpaal48MEHxeDBg4VOpxMJCQli4sSJ4p133hF+v79Df77PPPNMi7WWGhUXF4uFCxeKzMxModVqRUpKipg7d6743//9X8W4N23aJIfbpj/BnE6nuPfee0VycrLQ6/UiJydH/OMf/5DrNDX+fPHFF4r3ffrpp2LmzJkiNjZWGAwGkZWVJe655x6Rl5fXoe9OFMkkIXpgq3IiIiKiCMeb0URERERhYGgiIiIiCgNDExEREVEYGJqIiIiIwsDQRERERBQGhiYiIiKiMDA0EREREYWBoYmIiIgoDAxNRERERGFgaCIiIiIKA0MTERERURgYmoiIiIjC8P8BTh9+dsBlOKYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_W, _b = sess.run([W, b])\n",
    "x = np.linspace(-2, 2, 100)\n",
    "\n",
    "def sigmoid(logits):\n",
    "    return 1 / (1 + np.exp(-logits))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for d in range(D // 2):\n",
    "    logits = _W[0][0][d] * (x - _b[0][0][d])\n",
    "    psx = sigmoid(logits)\n",
    "    plt.plot(x, psx)\n",
    "    \n",
    "plt.xlabel('feature value')\n",
    "plt.ylabel('$p(s|x)$')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should illustrate that the probability of the feature value being observed when it is below the feature mean should be close to 1, while the probability of being observed above the feature mean should be close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
